"""
MultiLLM Orchestrator - çµ±æ‹¬AI
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®å¯¾è©±ã‚’ç®¡ç†ã—ã€ã‚¿ã‚¹ã‚¯ã‚’é©åˆ‡ãªWorker LLMã«æŒ¯ã‚Šåˆ†ã‘ã‚‹
"""

import asyncio
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from enum import Enum
import json
import uuid
import random
from .llm_client import ClaudeClient, TaskAnalysis, LLMMessage
from .response_formatter import ResponseFormatter, MessageProcessor
import time

logger = logging.getLogger(__name__)


class TaskType(Enum):
    """ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã®å®šç¾©"""
    CODE_IMPLEMENTATION = "code_implementation"
    UI_DESIGN = "ui_design"
    DOCUMENTATION = "documentation"
    PR_REVIEW = "pr_review"
    DATA_ANALYSIS = "data_analysis"
    IMAGE_GENERATION = "image_generation"
    GENERAL = "general"
    MEMORY_OPERATION = "memory_operation"  # OpenMemoryæ“ä½œç”¨


class TaskPriority(Enum):
    """ã‚¿ã‚¹ã‚¯å„ªå…ˆåº¦"""
    CRITICAL = 1
    HIGH = 2
    MEDIUM = 3
    LOW = 4


@dataclass
class Task:
    """ã‚¿ã‚¹ã‚¯å®šç¾©"""
    id: str
    type: TaskType
    description: str
    priority: TaskPriority
    user_id: str
    created_at: datetime
    status: str = "pending"
    assigned_worker: Optional[str] = None
    result: Optional[Dict] = None
    metadata: Dict = None


@dataclass
class LLMResponse:
    """LLMå¿œç­”ã®è¨˜éŒ²"""
    id: str
    provider: str
    model: str
    content: str
    tokens: Dict[str, int]
    metadata: Dict
    timestamp: datetime
    duration: float
    error: Optional[str] = None


@dataclass
class MCPConnection:
    """MCPæ¥ç¶šã®è¨˜éŒ²"""
    id: str
    service: str
    action: str
    request: Dict
    response: Dict
    timestamp: datetime
    duration: float
    success: bool
    error: Optional[str] = None


@dataclass
class ConversationLog:
    """ä¼šè©±ãƒ­ã‚°"""
    conversation_id: str
    messages: List[Dict]
    llm_responses: List[LLMResponse]
    mcp_connections: List[MCPConnection]
    total_tokens: int
    start_time: datetime
    end_time: Optional[datetime] = None


class MultiLLMOrchestrator:
    """
    MultiLLMã‚·ã‚¹ãƒ†ãƒ ã®çµ±æ‹¬è€…
    ã‚¿ã‚¹ã‚¯ã®åˆ†æã€æŒ¯ã‚Šåˆ†ã‘ã€é€²æ—ç®¡ç†ã‚’è¡Œã†
    """
    
    def __init__(self, config: Dict):
        self.config = config
        self.workers = {}
        self.task_queue = asyncio.Queue()
        self.active_tasks = {}
        self.memory_sync_interval = config.get('memory', {}).get('syncInterval', 300)
        # Claude APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰ï¼‰
        self.claude_client = ClaudeClient(None)  # APIã‚­ãƒ¼ãªã—ã§ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰
        
        # ä¼šè©±ãƒ­ã‚°ã®ç®¡ç†
        self.conversations = {}
        self.stream_handlers = {}  # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”¨ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼
        self.formatter = ResponseFormatter()
        self.message_processor = MessageProcessor()
        
        # ãƒªãƒˆãƒ©ã‚¤è¨­å®š
        self.max_retries = config.get('maxRetries', 3)
        self.base_delay = config.get('baseDelay', 1.0)  # åŸºæœ¬é…å»¶æ™‚é–“ï¼ˆç§’ï¼‰
        self.max_delay = config.get('maxDelay', 60.0)  # æœ€å¤§é…å»¶æ™‚é–“ï¼ˆç§’ï¼‰
        
        # ã‚¿ã‚¹ã‚¯æŒ¯ã‚Šåˆ†ã‘ãƒ«ãƒ¼ãƒ«
        self.task_routing = {
            TaskType.CODE_IMPLEMENTATION: "backend_worker",
            TaskType.UI_DESIGN: "frontend_worker",
            TaskType.DOCUMENTATION: "documentation_worker",
            TaskType.PR_REVIEW: "review_worker",
            TaskType.DATA_ANALYSIS: "analytics_worker",
            TaskType.IMAGE_GENERATION: "creative_worker",
            TaskType.GENERAL: "backend_worker",  # GENERALã‚¿ã‚¹ã‚¯ã¯backend_workerã«å‰²ã‚Šå½“ã¦
            TaskType.MEMORY_OPERATION: "mcp_worker"  # OpenMemoryæ“ä½œã¯MCP Workerã«å‰²ã‚Šå½“ã¦
        }
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®ã‚¿ã‚¹ã‚¯åˆ†é¡
        self.task_keywords = {
            TaskType.CODE_IMPLEMENTATION: [
                'ã‚³ãƒ¼ãƒ‰', 'code', 'å®Ÿè£…', 'implement', 'ãƒã‚°', 'bug', 'API', 
                'ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹', 'database', 'ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰', 'backend'
            ],
            TaskType.UI_DESIGN: [
                'UI', 'UX', 'ãƒ‡ã‚¶ã‚¤ãƒ³', 'design', 'ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰', 'frontend',
                'ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ', 'layout', 'ã‚¹ã‚¿ã‚¤ãƒ«', 'style', 'React'
            ],
            TaskType.DOCUMENTATION: [
                'ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ', 'document', 'README', 'ãƒãƒ‹ãƒ¥ã‚¢ãƒ«', 'manual',
                'ã‚¬ã‚¤ãƒ‰', 'guide', 'èª¬æ˜', 'explain'
            ],
            TaskType.PR_REVIEW: [
                'PR', 'ãƒ—ãƒ«ãƒªã‚¯', 'ãƒ¬ãƒ“ãƒ¥ãƒ¼', 'review', 'ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼',
                'ãƒãƒ¼ã‚¸', 'merge'
            ],
            TaskType.DATA_ANALYSIS: [
                'åˆ†æ', 'analyze', 'analysis', 'ãƒ‡ãƒ¼ã‚¿', 'data', 'çµ±è¨ˆ',
                'statistics', 'ãƒ¬ãƒãƒ¼ãƒˆ', 'report'
            ],
            TaskType.IMAGE_GENERATION: [
                'ç”»åƒ', 'image', 'ã‚¤ãƒ©ã‚¹ãƒˆ', 'illustration', 'å›³', 'diagram',
                'ãƒ‡ã‚¶ã‚¤ãƒ³ç”Ÿæˆ', 'generate'
            ],
            TaskType.GENERAL: [
                'ã“ã‚“ã«ã¡ã¯', 'hello', 'ãƒãƒ­ãƒ¼', 'ã‚„ã‚', 'help', 'ãƒ˜ãƒ«ãƒ—',
                'æ•™ãˆã¦', 'tell me', 'ä½•ãŒã§ãã‚‹', 'what can you do'
            ],
            TaskType.MEMORY_OPERATION: [
                'è¨˜æ†¶ã—ã¦', 'ä¿å­˜ã—ã¦', 'ãƒ¡ãƒ¢ãƒªã«ä¿å­˜', 'save memory',
                'æ€ã„å‡ºã—ã¦', 'æ¤œç´¢ã—ã¦', 'search memory', 'recall',
                'ãƒ¡ãƒ¢ãƒªã‚’å…¨éƒ¨è¦‹ã›ã¦', 'è¨˜æ†¶ã‚’è¡¨ç¤º', 'ä¸€è¦§è¡¨ç¤º', 'list memory',
                'ãƒ¡ãƒ¢ãƒªã‚’ã™ã¹ã¦å‰Šé™¤', 'å…¨å‰Šé™¤', 'delete all', 'å‰Šé™¤ã—ã¦'
            ]
        }
    
    async def initialize(self):
        """Orchestratorã®åˆæœŸåŒ–"""
        logger.info("ğŸ¯ MultiLLM Orchestrator initializing...")
        
        # Claude-4ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
        await self.claude_client.initialize()
        
        # Worker LLMsã®åˆæœŸåŒ–
        await self._initialize_workers()
        
        # ãƒ¡ãƒ¢ãƒªåŒæœŸã‚µãƒ¼ãƒ“ã‚¹ã®é–‹å§‹
        asyncio.create_task(self._memory_sync_loop())
        
        # ã‚¿ã‚¹ã‚¯å‡¦ç†ãƒ«ãƒ¼ãƒ—ã®é–‹å§‹
        asyncio.create_task(self._task_processing_loop())
        
        logger.info("âœ… Orchestrator initialized successfully")
    
    async def shutdown(self):
        """Orchestratorã®çµ‚äº†å‡¦ç†"""
        logger.info("ğŸ›‘ Shutting down MultiLLM Orchestrator...")
        await self.claude_client.shutdown()
        logger.info("âœ… Orchestrator shutdown complete")
    
    async def _initialize_workers(self):
        """Worker LLMsã®åˆæœŸåŒ–"""
        worker_configs = self.config.get('workers', {})
        
        for worker_name, worker_config in worker_configs.items():
            # ã“ã“ã§Worker LLMã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
            # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€å„Workerã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦åˆæœŸåŒ–
            self.workers[worker_name] = {
                'config': worker_config,
                'status': 'active',
                'current_task': None
            }
            logger.info(f"âœ… Initialized worker: {worker_name}")
        
        # MCPãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’è¿½åŠ 
        self.workers['mcp_worker'] = {
            'config': {'model': 'mcp-integration'},
            'status': 'active',
            'current_task': None
        }
        logger.info("âœ… Initialized worker: mcp_worker")
    
    async def process_user_request(self, request: str, user_id: str, context: Dict = None, conversation_id: str = None, stream_handler=None) -> Dict:
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‡¦ç†
        1. ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’åˆ†æ
        2. ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã«åˆ†è§£
        3. å„Workerã«æŒ¯ã‚Šåˆ†ã‘
        4. çµæœã‚’çµ±åˆã—ã¦è¿”ã™
        """
        logger.info(f"ğŸ“¥ Processing user request: {request[:100]}...")
        start_time = time.time()
        
        # ä¼šè©±IDã®ç”Ÿæˆã¾ãŸã¯å–å¾—
        if not conversation_id:
            conversation_id = f"conv_{uuid.uuid4()}"
        
        # ä¼šè©±ãƒ­ã‚°ã®åˆæœŸåŒ–ã¾ãŸã¯å–å¾—
        if conversation_id not in self.conversations:
            self.conversations[conversation_id] = ConversationLog(
                conversation_id=conversation_id,
                messages=[],
                llm_responses=[],
                mcp_connections=[],
                total_tokens=0,
                start_time=datetime.now()
            )
        
        conversation = self.conversations[conversation_id]
        
        # ã‚¹ãƒˆãƒªãƒ¼ãƒ ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®ç™»éŒ²
        if stream_handler:
            self.stream_handlers[conversation_id] = stream_handler
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¨˜éŒ²
        conversation.messages.append({
            'role': 'user',
            'content': request,
            'timestamp': datetime.now().isoformat()
        })
        
        # Claude-4ã«ã‚ˆã‚‹çŸ¥çš„ã‚¿ã‚¹ã‚¯åˆ†æ
        analysis_start = time.time()
        
        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚¤ãƒ™ãƒ³ãƒˆ: åˆ†æé–‹å§‹
        if stream_handler:
            await stream_handler(json.dumps({
                'type': 'analysis',
                'content': 'ã‚¿ã‚¹ã‚¯ã‚’åˆ†æä¸­...',
                'timestamp': datetime.now().isoformat()
            }) + '\n')
        
        task_analysis = await self.claude_client.analyze_task(request, context)
        analysis_duration = time.time() - analysis_start
        logger.info(f"ğŸ§  Task analysis: {task_analysis.task_type} - {task_analysis.reasoning}")
        
        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚¤ãƒ™ãƒ³ãƒˆ: åˆ†æå®Œäº†
        if stream_handler:
            await stream_handler(json.dumps({
                'type': 'analysis',
                'content': f'ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—: {task_analysis.task_type}',
                'details': {
                    'task_type': task_analysis.task_type,
                    'priority': task_analysis.priority,
                    'complexity': task_analysis.complexity,
                    'reasoning': task_analysis.reasoning
                },
                'timestamp': datetime.now().isoformat()
            }) + '\n')
        
        # ã‚¿ã‚¹ã‚¯åˆ†æã‚’LLMå¿œç­”ã¨ã—ã¦è¨˜éŒ²
        analysis_response = LLMResponse(
            id=str(uuid.uuid4()),
            provider='anthropic',
            model='claude-3.5-sonnet',
            content=f"Task Type: {task_analysis.task_type}\nReasoning: {task_analysis.reasoning}",
            tokens={'prompt': 0, 'completion': 0, 'total': 0},  # ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰ã§ã¯0
            metadata={'task': 'analysis'},
            timestamp=datetime.now(),
            duration=analysis_duration
        )
        conversation.llm_responses.append(analysis_response)
        
        # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã‚’è¨­å®š
        try:
            task_type = TaskType(task_analysis.task_type.lower())
        except ValueError:
            task_type = TaskType.GENERAL
        
        # ã€Œæ€ã„å‡ºã—ã¦ã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚‹å ´åˆã¯ã€ç›´æ¥ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆ
        if 'æ€ã„å‡ºã—ã¦' in request:
            # LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã§ç›´æ¥å‡¦ç†
            messages = [LLMMessage(role='user', content=request)]
            response = await self.claude_client.generate_response(
                messages=messages,
                context=context,
                stream_callback=stream_handler
            )
            
            # ä¼šè©±ãƒ­ã‚°ã«è¨˜éŒ²
            conversation.messages.append({
                'role': 'assistant',
                'content': response,
                'timestamp': datetime.now().isoformat(),
                'provider': 'claude-4.0'
            })
            
            return {
                'response': response,
                'conversation_log': asdict(conversation),
                'task_analysis': asdict(task_analysis)
            }
        
        # ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆ
        task = Task(
            id=str(uuid.uuid4()),
            type=task_type,
            description=request,
            priority=TaskPriority[task_analysis.priority] if task_analysis.priority in TaskPriority.__members__ else TaskPriority.MEDIUM,
            user_id=user_id,
            created_at=datetime.now(),
            metadata=context or {}
        )
        
        # è¤‡é›‘åº¦ã«å¿œã˜ãŸå‡¦ç†åˆ†å²
        if task_analysis.complexity == "complex":
            # è¤‡æ•°ã®ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã«åˆ†è§£ã—ã¦ä¸¦åˆ—å‡¦ç†
            subtasks = []
            for i, subtask_desc in enumerate(task_analysis.subtasks):
                subtask = Task(
                    id=f"{task.id}_sub_{i}",
                    type=task.type,
                    description=subtask_desc,
                    priority=task.priority,
                    user_id=user_id,
                    created_at=datetime.now(),
                    metadata={"parent_task": task.id, "worker": task_analysis.assigned_workers[i] if i < len(task_analysis.assigned_workers) else "backend_worker"}
                )
                subtasks.append(subtask)
            
            results = await self._process_parallel_tasks(subtasks, conversation)
            final_result = await self._integrate_results(results)
        else:
            # å˜ä¸€ã‚¿ã‚¹ã‚¯ã¨ã—ã¦å‡¦ç†
            preferred_worker = task_analysis.assigned_workers[0] if task_analysis.assigned_workers else None
            final_result = await self._process_single_task(task, preferred_worker, conversation)
        
        # ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¨˜éŒ²
        conversation.messages.append({
            'role': 'assistant',
            'content': final_result.get('result', final_result.get('summary', 'Task completed')),
            'timestamp': datetime.now().isoformat(),
            'provider': 'claude-4.0',
            'connections': [asdict(conn) for conn in conversation.mcp_connections[-5:]]  # æœ€æ–°5ä»¶ã®MCPæ¥ç¶šã‚’å«ã‚ã‚‹
        })
        
        # ä¼šè©±çµ‚äº†æ™‚åˆ»ã‚’è¨˜éŒ²
        conversation.end_time = datetime.now()
        
        # ã‚¹ãƒˆãƒªãƒ¼ãƒ ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if conversation_id in self.stream_handlers:
            del self.stream_handlers[conversation_id]
        
        return {
            'response': final_result.get('result', final_result.get('summary', 'Task completed')),
            'conversation_log': asdict(conversation),
            'task_analysis': asdict(task_analysis)
        }
    
    def _analyze_task_type(self, request: str) -> TaskType:
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆå†…å®¹ã‹ã‚‰ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š"""
        request_lower = request.lower()
        scores = {}
        
        for task_type, keywords in self.task_keywords.items():
            score = sum(1 for keyword in keywords if keyword in request_lower)
            if score > 0:
                scores[task_type] = score
        
        if scores:
            return max(scores, key=scores.get)
        
        return TaskType.GENERAL
    
    def _determine_priority(self, request: str) -> TaskPriority:
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‹ã‚‰å„ªå…ˆåº¦ã‚’åˆ¤å®š"""
        urgent_keywords = ['ç·Šæ€¥', 'urgent', 'è‡³æ€¥', 'ASAP', 'critical']
        high_keywords = ['é‡è¦', 'important', 'å„ªå…ˆ', 'priority']
        
        request_lower = request.lower()
        
        if any(keyword in request_lower for keyword in urgent_keywords):
            return TaskPriority.CRITICAL
        elif any(keyword in request_lower for keyword in high_keywords):
            return TaskPriority.HIGH
        
        return TaskPriority.MEDIUM
    
    def _is_complex_task(self, request: str) -> bool:
        """è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        # è¤‡æ•°ã®å‹•è©ã‚„æ¥ç¶šè©ã‚’å«ã‚€å ´åˆã¯è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã¨åˆ¤å®š
        complex_indicators = ['ãã—ã¦', 'ã¾ãŸ', 'ã•ã‚‰ã«', 'and', 'also', 'then']
        return any(indicator in request for indicator in complex_indicators)
    
    async def _decompose_task(self, task: Task) -> List[Task]:
        """è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã‚’ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã«åˆ†è§£"""
        # LLMã‚’ä½¿ç”¨ã—ã¦ã‚¿ã‚¹ã‚¯ã‚’åˆ†è§£
        prompt = f"""
ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ã‚’ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã«åˆ†è§£ã—ã¦ãã ã•ã„ï¼š

ã‚¿ã‚¹ã‚¯: {task.description}

ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã‚’JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
[
  {{
    "description": "ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã®èª¬æ˜",
    "type": "ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—",
    "dependencies": []
  }}
]
"""
        
        # LLMã«å•ã„åˆã‚ã›ï¼ˆå®Ÿéš›ã®å®Ÿè£…ï¼‰
        # response = await self.llm_client.generate(prompt)
        
        # ãƒ‡ãƒ¢ç”¨ã®ç°¡æ˜“å®Ÿè£…
        subtasks = []
        if "å®Ÿè£…" in task.description and "ãƒ†ã‚¹ãƒˆ" in task.description:
            subtasks.append(Task(
                id=f"{task.id}_1",
                type=TaskType.CODE_IMPLEMENTATION,
                description="å®Ÿè£…éƒ¨åˆ†",
                priority=task.priority,
                user_id=task.user_id,
                created_at=datetime.now()
            ))
            subtasks.append(Task(
                id=f"{task.id}_2",
                type=TaskType.CODE_IMPLEMENTATION,
                description="ãƒ†ã‚¹ãƒˆä½œæˆ",
                priority=task.priority,
                user_id=task.user_id,
                created_at=datetime.now()
            ))
        else:
            subtasks.append(task)
        
        return subtasks
    
    async def _process_single_task(self, task: Task, preferred_worker: str = None, conversation: ConversationLog = None) -> Dict:
        """å˜ä¸€ã‚¿ã‚¹ã‚¯ã‚’å‡¦ç†"""
        # é©åˆ‡ãªWorkerã‚’é¸æŠï¼ˆClaude-4ã®æ¨å¥¨ã‚’å„ªå…ˆï¼‰
        worker_name = preferred_worker or self.task_routing.get(task.type, "backend_worker")
        worker = self.workers.get(worker_name)
        
        if not worker:
            logger.error(f"Worker not found: {worker_name}")
            return {"error": "é©åˆ‡ãªWorkerãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}
        
        # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚¤ãƒ™ãƒ³ãƒˆ: ãƒ¯ãƒ¼ã‚«ãƒ¼å‰²ã‚Šå½“ã¦
        if conversation and conversation.conversation_id in self.stream_handlers:
            stream_handler = self.stream_handlers[conversation.conversation_id]
            await stream_handler(json.dumps({
                'type': 'worker',
                'worker': worker_name,
                'content': f'{worker_name}ã«ã‚¿ã‚¹ã‚¯ã‚’å‰²ã‚Šå½“ã¦ä¸­...',
                'timestamp': datetime.now().isoformat()
            }) + '\n')
        
        # ã‚¿ã‚¹ã‚¯ã‚’ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
        await self.task_queue.put(task)
        self.active_tasks[task.id] = task
        
        # Workerã§ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯éåŒæœŸã§å®Ÿè¡Œï¼‰
        result = await self._execute_task_on_worker(task, worker, conversation)
        
        # çµæœã‚’æ›´æ–°
        task.result = result
        task.status = "completed"
        del self.active_tasks[task.id]
        
        return result
    
    async def _process_parallel_tasks(self, tasks: List[Task], conversation: ConversationLog = None) -> List[Dict]:
        """è¤‡æ•°ã®ã‚¿ã‚¹ã‚¯ã‚’ä¸¦åˆ—å‡¦ç†"""
        tasks_coroutines = [self._process_single_task(task, conversation=conversation) for task in tasks]
        results = await asyncio.gather(*tasks_coroutines, return_exceptions=True)
        
        # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"Task {tasks[i].id} failed: {result}")
                processed_results.append({"error": str(result)})
            else:
                processed_results.append(result)
        
        return processed_results
    
    async def _integrate_results(self, results: List[Dict]) -> Dict:
        """è¤‡æ•°ã®çµæœã‚’çµ±åˆ"""
        # çµæœã‚’çµ±åˆã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯
        integrated = {
            "type": "integrated_result",
            "subtask_count": len(results),
            "results": results,
            "summary": "ã‚¿ã‚¹ã‚¯ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ"
        }
        
        # ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚‹å ´åˆã¯å ±å‘Š
        errors = [r for r in results if "error" in r]
        if errors:
            integrated["errors"] = errors
            integrated["summary"] = f"ä¸€éƒ¨ã®ã‚¿ã‚¹ã‚¯ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {len(errors)}ä»¶"
        
        return integrated
    
    async def _execute_task_on_worker(self, task: Task, worker: Dict, conversation: ConversationLog = None) -> Dict:
        """Workerã§ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œï¼ˆå®Ÿéš›ã®å®Ÿè£…ã¯Workerã‚¯ãƒ©ã‚¹ã§ï¼‰"""
        # ã‚¨ã‚¯ã‚¹ãƒãƒãƒ³ã‚·ãƒ£ãƒ«ãƒãƒƒã‚¯ã‚ªãƒ•ã§ãƒªãƒˆãƒ©ã‚¤
        last_error = None
        
        for attempt in range(self.max_retries):
            try:
                # å‡¦ç†é–‹å§‹æ™‚åˆ»
                task_start = time.time()
                
                # ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚¨ãƒ©ãƒ¼ã‚’ç™ºç”Ÿã•ã›ã‚‹ï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰
                # if random.random() < 0.3:  # 30%ã®ç¢ºç‡ã§ã‚¨ãƒ©ãƒ¼
                #     raise Exception("Simulated worker error")
                
                # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦å‡¦ç†ã‚’åˆ†å²
                if task.type == TaskType.MEMORY_OPERATION:
                    # MCPæ¥ç¶šã®è¨˜éŒ²
                    mcp_start = time.time()
                    try:
                        result = await self._generate_memory_response(task.description, conversation)
                        mcp_duration = time.time() - mcp_start
                        
                        if conversation and hasattr(self, 'last_mcp_connection'):
                            conversation.mcp_connections.append(self.last_mcp_connection)
                    except Exception as mcp_error:
                        mcp_duration = time.time() - mcp_start
                        if conversation:
                            mcp_connection = MCPConnection(
                                id=str(uuid.uuid4()),
                                service='openmemory',
                                action='operation',
                                request={'description': task.description},
                                response={},
                                timestamp=datetime.now(),
                                duration=mcp_duration,
                                success=False,
                                error=str(mcp_error)
                            )
                            conversation.mcp_connections.append(mcp_connection)
                        raise
                        
                elif task.type == TaskType.GENERAL:
                    # Claude-4ã«ã‚ˆã‚‹å®Ÿéš›ã®å¿œç­”ç”Ÿæˆ
                    llm_start = time.time()
                    messages = [LLMMessage(role="user", content=task.description)]
                    
                    # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œ
                    if conversation and conversation.conversation_id in self.stream_handlers:
                        stream_handler = self.stream_handlers[conversation.conversation_id]
                        result = await self.claude_client.generate_response(
                            messages, 
                            stream_callback=lambda chunk: asyncio.create_task(stream_handler(chunk))
                        )
                    else:
                        result = await self.claude_client.generate_response(messages)
                    
                    llm_duration = time.time() - llm_start
                    
                    # LLMå¿œç­”ã‚’è¨˜éŒ²
                    if conversation:
                        llm_response = LLMResponse(
                            id=str(uuid.uuid4()),
                            provider='anthropic',
                            model='claude-3.5-sonnet',
                            content=result,
                            tokens={'prompt': 0, 'completion': 0, 'total': 0},  # ãƒ‡ãƒ¢ãƒ¢ãƒ¼ãƒ‰ã§ã¯0
                            metadata={'task_type': task.type.value},
                            timestamp=datetime.now(),
                            duration=llm_duration
                        )
                        conversation.llm_responses.append(llm_response)
                else:
                    # ä»–ã®ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã‚‚å°†æ¥çš„ã«ã¯LLMã§å‡¦ç†
                    result = f"{task.type.value}ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚å°‚ç”¨Workerã«ã‚ˆã‚‹è©³ç´°ãªå‡¦ç†ã¯é–‹ç™ºä¸­ã§ã™ã€‚"
                
                return {
                    "task_id": task.id,
                    "worker": worker['config'].get('model', 'unknown'),
                    "result": result,
                    "timestamp": datetime.now().isoformat(),
                    "attempts": attempt + 1
                }
                
            except Exception as e:
                last_error = e
                
                if attempt < self.max_retries - 1:
                    # ã‚¨ã‚¯ã‚¹ãƒãƒãƒ³ã‚·ãƒ£ãƒ«ãƒãƒƒã‚¯ã‚ªãƒ•è¨ˆç®—
                    delay = min(
                        self.base_delay * (2 ** attempt) + random.uniform(0, 1),
                        self.max_delay
                    )
                    
                    logger.warning(
                        f"Task {task.id} failed on attempt {attempt + 1}/{self.max_retries}. "
                        f"Retrying in {delay:.1f}s... Error: {e}"
                    )
                    
                    await asyncio.sleep(delay)
                else:
                    logger.error(f"Task {task.id} failed after {self.max_retries} attempts: {e}")
        
        # ã™ã¹ã¦ã®ãƒªãƒˆãƒ©ã‚¤ãŒå¤±æ•—ã—ãŸå ´åˆ
        return {
            "task_id": task.id,
            "worker": worker['config'].get('model', 'unknown'),
            "error": str(last_error),
            "attempts": self.max_retries,
            "status": "failed"
        }
    
    async def _memory_sync_loop(self):
        """å®šæœŸçš„ãªãƒ¡ãƒ¢ãƒªåŒæœŸ"""
        while True:
            await asyncio.sleep(self.memory_sync_interval)
            await self._sync_memory()
    
    async def _sync_memory(self):
        """å…¨LLMã®è¨˜æ†¶ã‚’OpenMemoryã«åŒæœŸ"""
        logger.info("ğŸ”„ Syncing memory across all LLMs...")
        
        # å„Workerã®è¨˜æ†¶ã‚’åé›†
        memories = {}
        for worker_name, worker in self.workers.items():
            # Workerå›ºæœ‰ã®è¨˜æ†¶ã‚’å–å¾—ï¼ˆå®Ÿè£…ã¯å„Workerã‚¯ãƒ©ã‚¹ã§ï¼‰
            memories[worker_name] = {
                "last_sync": datetime.now().isoformat(),
                "status": worker['status']
            }
        
        # OpenMemoryã«ä¿å­˜ï¼ˆå®Ÿéš›ã®å®Ÿè£…ï¼‰
        # await self.memory_service.sync(memories)
        
        logger.info("âœ… Memory sync completed")
    
    async def _task_processing_loop(self):
        """ã‚¿ã‚¹ã‚¯å‡¦ç†ã®ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—"""
        while True:
            try:
                # ã‚­ãƒ¥ãƒ¼ã‹ã‚‰ã‚¿ã‚¹ã‚¯ã‚’å–å¾—
                task = await self.task_queue.get()
                logger.info(f"ğŸ“‹ Processing task: {task.id}")
                
                # ã‚¿ã‚¹ã‚¯ã®å‡¦ç†ï¼ˆå®Ÿéš›ã¯Workerã«å§”è­²ï¼‰
                # ã“ã“ã§ã¯å‡¦ç†æ¸ˆã¿ã¨ãƒãƒ¼ã‚¯
                
            except Exception as e:
                logger.error(f"Task processing error: {e}")
            
            await asyncio.sleep(0.1)  # CPUä½¿ç”¨ç‡ã‚’æŠ‘ãˆã‚‹
    
    def get_status(self) -> Dict:
        """Orchestratorã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å–å¾—"""
        return {
            "status": "active",
            "workers": {name: w['status'] for name, w in self.workers.items()},
            "active_tasks": len(self.active_tasks),
            "queued_tasks": self.task_queue.qsize(),
            "timestamp": datetime.now().isoformat()
        }
    
    async def handle_conversation_overflow(self, conversation_id: str, token_usage: float):
        """ä¼šè©±ã®ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨ç‡ãŒé–¾å€¤ã‚’è¶…ãˆãŸå ´åˆã®å‡¦ç†"""
        if token_usage > 0.8:
            logger.warning(f"âš ï¸ Conversation {conversation_id} reaching token limit: {token_usage:.1%}")
            
            # ä¼šè©±ã‚’è¦ç´„
            summary = await self._summarize_conversation(conversation_id)
            
            # OpenMemoryã«ä¿å­˜
            await self._save_to_memory(conversation_id, summary)
            
            # æ–°ã—ã„LLMã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã«åˆ‡ã‚Šæ›¿ãˆ
            await self._rotate_llm_instance(conversation_id, summary)
    
    async def _summarize_conversation(self, conversation_id: str) -> str:
        """ä¼šè©±ã‚’è¦ç´„"""
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ä¼šè©±å±¥æ­´ã‚’å–å¾—ã—ã¦è¦ç´„
        return f"Conversation {conversation_id} summary"
    
    async def _save_to_memory(self, conversation_id: str, summary: str):
        """OpenMemoryã«ä¿å­˜"""
        # å®Ÿéš›ã®å®Ÿè£…
        pass
    
    async def _rotate_llm_instance(self, conversation_id: str, context: str):
        """æ–°ã—ã„LLMã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã«åˆ‡ã‚Šæ›¿ãˆ"""
        # å®Ÿéš›ã®å®Ÿè£…
        pass
    
    async def _generate_memory_response(self, description: str, conversation: ConversationLog = None) -> str:
        """OpenMemoryæ“ä½œã®ãƒ‡ãƒ¢å¿œç­”ï¼ˆå®Ÿéš›ã«ã¯MCP WorkerãŒå‡¦ç†ï¼‰"""
        # MCPã‚µãƒ¼ãƒ“ã‚¹ã‚’ä½¿ç”¨ã—ã¦OpenMemoryã¨é€šä¿¡
        try:
            # MCPçµ±åˆã‚µãƒ¼ãƒ“ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—ã¾ãŸã¯ä½œæˆ
            if not hasattr(self, 'mcp_service'):
                import sys
                import os
                # è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
                sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
                from services.mcp_integration import MCPIntegrationService, MCPWorker
                mcp_config = {
                    'providers': {
                        'openmemory': {
                            'type': 'openmemory',
                            'url': 'http://localhost:8765',
                            'userId': 'mourigenta'
                        }
                    }
                }
                self.mcp_service = MCPIntegrationService(mcp_config)
                await self.mcp_service.initialize()
                self.mcp_worker = MCPWorker(self.mcp_service)
            
            # MCPãƒ¯ãƒ¼ã‚«ãƒ¼ã§ã‚¿ã‚¹ã‚¯ã‚’å‡¦ç†
            mcp_start = time.time()
            result = await self.mcp_worker.process_mcp_task(description, {})
            mcp_duration = time.time() - mcp_start
            
            # MCPæ¥ç¶šã‚’è¨˜éŒ²
            if conversation:
                mcp_connection = MCPConnection(
                    id=str(uuid.uuid4()),
                    service='openmemory',
                    action=self._extract_mcp_action(description),
                    request={'description': description},
                    response=result,
                    timestamp=datetime.now(),
                    duration=mcp_duration,
                    success=not result.get('error'),
                    error=result.get('error')
                )
                conversation.mcp_connections.append(mcp_connection)
                self.last_mcp_connection = mcp_connection
            
            if result.get('error'):
                return f"âŒ ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {result['error']}"
            
            # æˆåŠŸæ™‚ã®å¿œç­”ã‚’æ•´å½¢
            mcp_result = result.get('result', {})
            
            if 'save' in description or 'è¨˜æ†¶ã—ã¦' in description or 'ä¿å­˜ã—ã¦' in description:
                return f"""âœ… ãƒ¡ãƒ¢ãƒªã«ä¿å­˜ã—ã¾ã—ãŸï¼

**ä¿å­˜å†…å®¹**: {mcp_result.get('content', description)}
**ãƒ¡ãƒ¢ãƒªID**: {mcp_result.get('id', 'N/A')}
**ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—**: {mcp_result.get('timestamp', 'N/A')}

ãƒ¡ãƒ¢ãƒªã«æ­£å¸¸ã«ä¿å­˜ã•ã‚Œã¾ã—ãŸã€‚å¾Œã§ã€Œæ€ã„å‡ºã—ã¦ã€ã¨è¨€ãˆã°æ¤œç´¢ã§ãã¾ã™ã€‚"""
            
            elif 'search' in description or 'æ€ã„å‡ºã—ã¦' in description or 'æ¤œç´¢ã—ã¦' in description:
                memories = mcp_result.get('memories', [])
                if not memories:
                    return "ğŸ” è©²å½“ã™ã‚‹ãƒ¡ãƒ¢ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
                
                response = f"ğŸ” {len(memories)}ä»¶ã®ãƒ¡ãƒ¢ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸï¼š\n\n"
                for i, memory in enumerate(memories[:5], 1):  # æœ€å¤§5ä»¶è¡¨ç¤º
                    response += f"**{i}. {memory.get('content', 'N/A')}**\n"
                    response += f"   - ID: {memory.get('id', 'N/A')}\n"
                    response += f"   - ä¿å­˜æ—¥æ™‚: {memory.get('timestamp', 'N/A')}\n"
                    response += f"   - é–¢é€£åº¦: {memory.get('similarity', 0):.2f}\n\n"
                
                return response
            
            elif 'list' in description or 'ä¸€è¦§' in description or 'å…¨éƒ¨è¦‹ã›ã¦' in description:
                memories = mcp_result.get('memories', [])
                if not memories:
                    return "ğŸ“ ç¾åœ¨ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ãƒ¡ãƒ¢ãƒªã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"
                
                response = f"ğŸ“ {len(memories)}ä»¶ã®ãƒ¡ãƒ¢ãƒªãŒä¿å­˜ã•ã‚Œã¦ã„ã¾ã™ï¼š\n\n"
                for i, memory in enumerate(memories[:10], 1):  # æœ€å¤§10ä»¶è¡¨ç¤º
                    response += f"**{i}. {memory.get('content', 'N/A')[:50]}{'...' if len(memory.get('content', '')) > 50 else ''}**\n"
                    response += f"   - ID: {memory.get('id', 'N/A')}\n"
                    response += f"   - ä¿å­˜æ—¥æ™‚: {memory.get('timestamp', 'N/A')}\n\n"
                
                if len(memories) > 10:
                    response += f"\nï¼ˆä»– {len(memories) - 10} ä»¶ã®ãƒ¡ãƒ¢ãƒªãŒã‚ã‚Šã¾ã™ï¼‰"
                
                return response
            
            elif 'delete' in description or 'å‰Šé™¤' in description:
                if 'ã™ã¹ã¦' in description or 'å…¨éƒ¨' in description:
                    return f"ğŸ—‘ï¸ {mcp_result.get('message', 'ã™ã¹ã¦ã®ãƒ¡ãƒ¢ãƒªã‚’å‰Šé™¤ã—ã¾ã—ãŸ')}"
                else:
                    return f"ğŸ—‘ï¸ {mcp_result.get('message', 'ãƒ¡ãƒ¢ãƒªã‚’å‰Šé™¤ã—ã¾ã—ãŸ')}"
            
            else:
                return f"âœ… ãƒ¡ãƒ¢ãƒªæ“ä½œãŒå®Œäº†ã—ã¾ã—ãŸ: {mcp_result.get('message', 'å‡¦ç†å®Œäº†')}"
                
        except Exception as e:
            logger.error(f"Memory operation error: {e}")
            return f"âŒ ãƒ¡ãƒ¢ãƒªæ“ä½œä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}\n\nOpenMemoryã‚µãƒ¼ãƒ“ã‚¹ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
    
    def _extract_mcp_action(self, description: str) -> str:
        """èª¬æ˜æ–‡ã‹ã‚‰MCPã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æŠ½å‡º"""
        if any(word in description for word in ['è¨˜æ†¶ã—ã¦', 'ä¿å­˜ã—ã¦', 'save']):
            return 'save'
        elif any(word in description for word in ['æ€ã„å‡ºã—ã¦', 'æ¤œç´¢ã—ã¦', 'search']):
            return 'search'
        elif any(word in description for word in ['ä¸€è¦§', 'å…¨éƒ¨è¦‹ã›ã¦', 'list']):
            return 'list'
        elif any(word in description for word in ['å‰Šé™¤', 'delete']):
            return 'delete'
        else:
            return 'unknown'
    
    def get_conversation_log(self, conversation_id: str) -> Optional[Dict]:
        """ä¼šè©±ãƒ­ã‚°ã‚’å–å¾—"""
        if conversation_id in self.conversations:
            return asdict(self.conversations[conversation_id])
        return None
    
    def get_all_conversations(self) -> List[Dict]:
        """å…¨ä¼šè©±ãƒ­ã‚°ã‚’å–å¾—"""
        return [asdict(conv) for conv in self.conversations.values()]


# ä½¿ç”¨ä¾‹
async def main():
    config = {
        "workers": {
            "backend_worker": {"model": "gpt-4-turbo"},
            "frontend_worker": {"model": "claude-3-sonnet"},
            "review_worker": {"model": "gpt-4"}
        },
        "memory": {
            "syncInterval": 300
        }
    }
    
    orchestrator = MultiLLMOrchestrator(config)
    await orchestrator.initialize()
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‡¦ç†
    result = await orchestrator.process_user_request(
        "ãƒã‚°ä¿®æ­£ã¨ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰ã®ä½œæˆã‚’ãŠé¡˜ã„ã—ã¾ã™",
        user_id="user123"
    )
    
    print(json.dumps(result, indent=2, ensure_ascii=False))


if __name__ == "__main__":
    asyncio.run(main())