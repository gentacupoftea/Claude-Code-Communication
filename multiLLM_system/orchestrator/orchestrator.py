"""
MultiLLM Orchestrator - çµ±æ‹¬AI
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®å¯¾è©±ã‚’ç®¡ç†ã—ã€ã‚¿ã‚¹ã‚¯ã‚’é©åˆ‡ãªWorker LLMã«æŒ¯ã‚Šåˆ†ã‘ã‚‹
"""

import asyncio
import logging
from datetime import datetime
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from enum import Enum
import json
import uuid
import random

logger = logging.getLogger(__name__)


class TaskType(Enum):
    """ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã®å®šç¾©"""
    CODE_IMPLEMENTATION = "code_implementation"
    UI_DESIGN = "ui_design"
    DOCUMENTATION = "documentation"
    PR_REVIEW = "pr_review"
    DATA_ANALYSIS = "data_analysis"
    IMAGE_GENERATION = "image_generation"
    GENERAL = "general"


class TaskPriority(Enum):
    """ã‚¿ã‚¹ã‚¯å„ªå…ˆåº¦"""
    CRITICAL = 1
    HIGH = 2
    MEDIUM = 3
    LOW = 4


@dataclass
class Task:
    """ã‚¿ã‚¹ã‚¯å®šç¾©"""
    id: str
    type: TaskType
    description: str
    priority: TaskPriority
    user_id: str
    created_at: datetime
    status: str = "pending"
    assigned_worker: Optional[str] = None
    result: Optional[Dict] = None
    metadata: Dict = None


class MultiLLMOrchestrator:
    """
    MultiLLMã‚·ã‚¹ãƒ†ãƒ ã®çµ±æ‹¬è€…
    ã‚¿ã‚¹ã‚¯ã®åˆ†æã€æŒ¯ã‚Šåˆ†ã‘ã€é€²æ—ç®¡ç†ã‚’è¡Œã†
    """
    
    def __init__(self, config: Dict):
        self.config = config
        self.workers = {}
        self.task_queue = asyncio.Queue()
        self.active_tasks = {}
        self.memory_sync_interval = config.get('memory', {}).get('syncInterval', 300)
        self.llm_client = None  # LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆå¾Œã§æ³¨å…¥ï¼‰
        
        # ãƒªãƒˆãƒ©ã‚¤è¨­å®š
        self.max_retries = config.get('maxRetries', 3)
        self.base_delay = config.get('baseDelay', 1.0)  # åŸºæœ¬é…å»¶æ™‚é–“ï¼ˆç§’ï¼‰
        self.max_delay = config.get('maxDelay', 60.0)  # æœ€å¤§é…å»¶æ™‚é–“ï¼ˆç§’ï¼‰
        
        # ã‚¿ã‚¹ã‚¯æŒ¯ã‚Šåˆ†ã‘ãƒ«ãƒ¼ãƒ«
        self.task_routing = {
            TaskType.CODE_IMPLEMENTATION: "backend_worker",
            TaskType.UI_DESIGN: "frontend_worker",
            TaskType.DOCUMENTATION: "documentation_worker",
            TaskType.PR_REVIEW: "review_worker",
            TaskType.DATA_ANALYSIS: "analytics_worker",
            TaskType.IMAGE_GENERATION: "creative_worker"
        }
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®ã‚¿ã‚¹ã‚¯åˆ†é¡
        self.task_keywords = {
            TaskType.CODE_IMPLEMENTATION: [
                'ã‚³ãƒ¼ãƒ‰', 'code', 'å®Ÿè£…', 'implement', 'ãƒã‚°', 'bug', 'API', 
                'ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹', 'database', 'ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰', 'backend'
            ],
            TaskType.UI_DESIGN: [
                'UI', 'UX', 'ãƒ‡ã‚¶ã‚¤ãƒ³', 'design', 'ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰', 'frontend',
                'ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆ', 'layout', 'ã‚¹ã‚¿ã‚¤ãƒ«', 'style', 'React'
            ],
            TaskType.DOCUMENTATION: [
                'ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ', 'document', 'README', 'ãƒãƒ‹ãƒ¥ã‚¢ãƒ«', 'manual',
                'ã‚¬ã‚¤ãƒ‰', 'guide', 'èª¬æ˜', 'explain'
            ],
            TaskType.PR_REVIEW: [
                'PR', 'ãƒ—ãƒ«ãƒªã‚¯', 'ãƒ¬ãƒ“ãƒ¥ãƒ¼', 'review', 'ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ“ãƒ¥ãƒ¼',
                'ãƒãƒ¼ã‚¸', 'merge'
            ],
            TaskType.DATA_ANALYSIS: [
                'åˆ†æ', 'analyze', 'analysis', 'ãƒ‡ãƒ¼ã‚¿', 'data', 'çµ±è¨ˆ',
                'statistics', 'ãƒ¬ãƒãƒ¼ãƒˆ', 'report'
            ],
            TaskType.IMAGE_GENERATION: [
                'ç”»åƒ', 'image', 'ã‚¤ãƒ©ã‚¹ãƒˆ', 'illustration', 'å›³', 'diagram',
                'ãƒ‡ã‚¶ã‚¤ãƒ³ç”Ÿæˆ', 'generate'
            ]
        }
    
    async def initialize(self):
        """Orchestratorã®åˆæœŸåŒ–"""
        logger.info("ğŸ¯ MultiLLM Orchestrator initializing...")
        
        # Worker LLMsã®åˆæœŸåŒ–
        await self._initialize_workers()
        
        # ãƒ¡ãƒ¢ãƒªåŒæœŸã‚µãƒ¼ãƒ“ã‚¹ã®é–‹å§‹
        asyncio.create_task(self._memory_sync_loop())
        
        # ã‚¿ã‚¹ã‚¯å‡¦ç†ãƒ«ãƒ¼ãƒ—ã®é–‹å§‹
        asyncio.create_task(self._task_processing_loop())
        
        logger.info("âœ… Orchestrator initialized successfully")
    
    async def _initialize_workers(self):
        """Worker LLMsã®åˆæœŸåŒ–"""
        worker_configs = self.config.get('workers', {})
        
        for worker_name, worker_config in worker_configs.items():
            # ã“ã“ã§Worker LLMã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ
            # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€å„Workerã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦åˆæœŸåŒ–
            self.workers[worker_name] = {
                'config': worker_config,
                'status': 'active',
                'current_task': None
            }
            logger.info(f"âœ… Initialized worker: {worker_name}")
    
    async def process_user_request(self, request: str, user_id: str, context: Dict = None) -> Dict:
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‡¦ç†
        1. ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’åˆ†æ
        2. ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã«åˆ†è§£
        3. å„Workerã«æŒ¯ã‚Šåˆ†ã‘
        4. çµæœã‚’çµ±åˆã—ã¦è¿”ã™
        """
        logger.info(f"ğŸ“¥ Processing user request: {request[:100]}...")
        
        # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
        task_type = self._analyze_task_type(request)
        
        # ã‚¿ã‚¹ã‚¯ã‚’ä½œæˆ
        task = Task(
            id=str(uuid.uuid4()),
            type=task_type,
            description=request,
            priority=self._determine_priority(request),
            user_id=user_id,
            created_at=datetime.now(),
            metadata=context or {}
        )
        
        # è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã®å ´åˆã¯ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã«åˆ†è§£
        if self._is_complex_task(request):
            subtasks = await self._decompose_task(task)
            results = await self._process_parallel_tasks(subtasks)
            return await self._integrate_results(results)
        else:
            # å˜ä¸€ã‚¿ã‚¹ã‚¯ã¨ã—ã¦å‡¦ç†
            return await self._process_single_task(task)
    
    def _analyze_task_type(self, request: str) -> TaskType:
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆå†…å®¹ã‹ã‚‰ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š"""
        request_lower = request.lower()
        scores = {}
        
        for task_type, keywords in self.task_keywords.items():
            score = sum(1 for keyword in keywords if keyword in request_lower)
            if score > 0:
                scores[task_type] = score
        
        if scores:
            return max(scores, key=scores.get)
        
        return TaskType.GENERAL
    
    def _determine_priority(self, request: str) -> TaskPriority:
        """ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‹ã‚‰å„ªå…ˆåº¦ã‚’åˆ¤å®š"""
        urgent_keywords = ['ç·Šæ€¥', 'urgent', 'è‡³æ€¥', 'ASAP', 'critical']
        high_keywords = ['é‡è¦', 'important', 'å„ªå…ˆ', 'priority']
        
        request_lower = request.lower()
        
        if any(keyword in request_lower for keyword in urgent_keywords):
            return TaskPriority.CRITICAL
        elif any(keyword in request_lower for keyword in high_keywords):
            return TaskPriority.HIGH
        
        return TaskPriority.MEDIUM
    
    def _is_complex_task(self, request: str) -> bool:
        """è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        # è¤‡æ•°ã®å‹•è©ã‚„æ¥ç¶šè©ã‚’å«ã‚€å ´åˆã¯è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã¨åˆ¤å®š
        complex_indicators = ['ãã—ã¦', 'ã¾ãŸ', 'ã•ã‚‰ã«', 'and', 'also', 'then']
        return any(indicator in request for indicator in complex_indicators)
    
    async def _decompose_task(self, task: Task) -> List[Task]:
        """è¤‡é›‘ãªã‚¿ã‚¹ã‚¯ã‚’ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã«åˆ†è§£"""
        # LLMã‚’ä½¿ç”¨ã—ã¦ã‚¿ã‚¹ã‚¯ã‚’åˆ†è§£
        prompt = f"""
ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ã‚’ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã«åˆ†è§£ã—ã¦ãã ã•ã„ï¼š

ã‚¿ã‚¹ã‚¯: {task.description}

ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã‚’JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼š
[
  {{
    "description": "ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã®èª¬æ˜",
    "type": "ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—",
    "dependencies": []
  }}
]
"""
        
        # LLMã«å•ã„åˆã‚ã›ï¼ˆå®Ÿéš›ã®å®Ÿè£…ï¼‰
        # response = await self.llm_client.generate(prompt)
        
        # ãƒ‡ãƒ¢ç”¨ã®ç°¡æ˜“å®Ÿè£…
        subtasks = []
        if "å®Ÿè£…" in task.description and "ãƒ†ã‚¹ãƒˆ" in task.description:
            subtasks.append(Task(
                id=f"{task.id}_1",
                type=TaskType.CODE_IMPLEMENTATION,
                description="å®Ÿè£…éƒ¨åˆ†",
                priority=task.priority,
                user_id=task.user_id,
                created_at=datetime.now()
            ))
            subtasks.append(Task(
                id=f"{task.id}_2",
                type=TaskType.CODE_IMPLEMENTATION,
                description="ãƒ†ã‚¹ãƒˆä½œæˆ",
                priority=task.priority,
                user_id=task.user_id,
                created_at=datetime.now()
            ))
        else:
            subtasks.append(task)
        
        return subtasks
    
    async def _process_single_task(self, task: Task) -> Dict:
        """å˜ä¸€ã‚¿ã‚¹ã‚¯ã‚’å‡¦ç†"""
        # é©åˆ‡ãªWorkerã‚’é¸æŠ
        worker_name = self.task_routing.get(task.type, "backend_worker")
        worker = self.workers.get(worker_name)
        
        if not worker:
            logger.error(f"Worker not found: {worker_name}")
            return {"error": "é©åˆ‡ãªWorkerãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}
        
        # ã‚¿ã‚¹ã‚¯ã‚’ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
        await self.task_queue.put(task)
        self.active_tasks[task.id] = task
        
        # Workerã§ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯éåŒæœŸã§å®Ÿè¡Œï¼‰
        result = await self._execute_task_on_worker(task, worker)
        
        # çµæœã‚’æ›´æ–°
        task.result = result
        task.status = "completed"
        del self.active_tasks[task.id]
        
        return result
    
    async def _process_parallel_tasks(self, tasks: List[Task]) -> List[Dict]:
        """è¤‡æ•°ã®ã‚¿ã‚¹ã‚¯ã‚’ä¸¦åˆ—å‡¦ç†"""
        tasks_coroutines = [self._process_single_task(task) for task in tasks]
        results = await asyncio.gather(*tasks_coroutines, return_exceptions=True)
        
        # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"Task {tasks[i].id} failed: {result}")
                processed_results.append({"error": str(result)})
            else:
                processed_results.append(result)
        
        return processed_results
    
    async def _integrate_results(self, results: List[Dict]) -> Dict:
        """è¤‡æ•°ã®çµæœã‚’çµ±åˆ"""
        # çµæœã‚’çµ±åˆã™ã‚‹ãƒ­ã‚¸ãƒƒã‚¯
        integrated = {
            "type": "integrated_result",
            "subtask_count": len(results),
            "results": results,
            "summary": "ã‚¿ã‚¹ã‚¯ãŒæ­£å¸¸ã«å®Œäº†ã—ã¾ã—ãŸ"
        }
        
        # ã‚¨ãƒ©ãƒ¼ãŒã‚ã‚‹å ´åˆã¯å ±å‘Š
        errors = [r for r in results if "error" in r]
        if errors:
            integrated["errors"] = errors
            integrated["summary"] = f"ä¸€éƒ¨ã®ã‚¿ã‚¹ã‚¯ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {len(errors)}ä»¶"
        
        return integrated
    
    async def _execute_task_on_worker(self, task: Task, worker: Dict) -> Dict:
        """Workerã§ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œï¼ˆå®Ÿéš›ã®å®Ÿè£…ã¯Workerã‚¯ãƒ©ã‚¹ã§ï¼‰"""
        # ã‚¨ã‚¯ã‚¹ãƒãƒãƒ³ã‚·ãƒ£ãƒ«ãƒãƒƒã‚¯ã‚ªãƒ•ã§ãƒªãƒˆãƒ©ã‚¤
        last_error = None
        
        for attempt in range(self.max_retries):
            try:
                # ãƒ‡ãƒ¢ç”¨ã®ç°¡æ˜“å®Ÿè£…ï¼ˆå®Ÿéš›ã¯Workerã‚¯ãƒ©ã‚¹ã§å®Ÿè¡Œï¼‰
                await asyncio.sleep(1)  # å‡¦ç†æ™‚é–“ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
                
                # ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚¨ãƒ©ãƒ¼ã‚’ç™ºç”Ÿã•ã›ã‚‹ï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰
                # if random.random() < 0.3:  # 30%ã®ç¢ºç‡ã§ã‚¨ãƒ©ãƒ¼
                #     raise Exception("Simulated worker error")
                
                return {
                    "task_id": task.id,
                    "worker": worker['config'].get('model', 'unknown'),
                    "result": f"{task.type.value}ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ",
                    "timestamp": datetime.now().isoformat(),
                    "attempts": attempt + 1
                }
                
            except Exception as e:
                last_error = e
                
                if attempt < self.max_retries - 1:
                    # ã‚¨ã‚¯ã‚¹ãƒãƒãƒ³ã‚·ãƒ£ãƒ«ãƒãƒƒã‚¯ã‚ªãƒ•è¨ˆç®—
                    delay = min(
                        self.base_delay * (2 ** attempt) + random.uniform(0, 1),
                        self.max_delay
                    )
                    
                    logger.warning(
                        f"Task {task.id} failed on attempt {attempt + 1}/{self.max_retries}. "
                        f"Retrying in {delay:.1f}s... Error: {e}"
                    )
                    
                    await asyncio.sleep(delay)
                else:
                    logger.error(f"Task {task.id} failed after {self.max_retries} attempts: {e}")
        
        # ã™ã¹ã¦ã®ãƒªãƒˆãƒ©ã‚¤ãŒå¤±æ•—ã—ãŸå ´åˆ
        return {
            "task_id": task.id,
            "worker": worker['config'].get('model', 'unknown'),
            "error": str(last_error),
            "attempts": self.max_retries,
            "status": "failed"
        }
    
    async def _memory_sync_loop(self):
        """å®šæœŸçš„ãªãƒ¡ãƒ¢ãƒªåŒæœŸ"""
        while True:
            await asyncio.sleep(self.memory_sync_interval)
            await self._sync_memory()
    
    async def _sync_memory(self):
        """å…¨LLMã®è¨˜æ†¶ã‚’OpenMemoryã«åŒæœŸ"""
        logger.info("ğŸ”„ Syncing memory across all LLMs...")
        
        # å„Workerã®è¨˜æ†¶ã‚’åé›†
        memories = {}
        for worker_name, worker in self.workers.items():
            # Workerå›ºæœ‰ã®è¨˜æ†¶ã‚’å–å¾—ï¼ˆå®Ÿè£…ã¯å„Workerã‚¯ãƒ©ã‚¹ã§ï¼‰
            memories[worker_name] = {
                "last_sync": datetime.now().isoformat(),
                "status": worker['status']
            }
        
        # OpenMemoryã«ä¿å­˜ï¼ˆå®Ÿéš›ã®å®Ÿè£…ï¼‰
        # await self.memory_service.sync(memories)
        
        logger.info("âœ… Memory sync completed")
    
    async def _task_processing_loop(self):
        """ã‚¿ã‚¹ã‚¯å‡¦ç†ã®ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—"""
        while True:
            try:
                # ã‚­ãƒ¥ãƒ¼ã‹ã‚‰ã‚¿ã‚¹ã‚¯ã‚’å–å¾—
                task = await self.task_queue.get()
                logger.info(f"ğŸ“‹ Processing task: {task.id}")
                
                # ã‚¿ã‚¹ã‚¯ã®å‡¦ç†ï¼ˆå®Ÿéš›ã¯Workerã«å§”è­²ï¼‰
                # ã“ã“ã§ã¯å‡¦ç†æ¸ˆã¿ã¨ãƒãƒ¼ã‚¯
                
            except Exception as e:
                logger.error(f"Task processing error: {e}")
            
            await asyncio.sleep(0.1)  # CPUä½¿ç”¨ç‡ã‚’æŠ‘ãˆã‚‹
    
    def get_status(self) -> Dict:
        """Orchestratorã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å–å¾—"""
        return {
            "status": "active",
            "workers": {name: w['status'] for name, w in self.workers.items()},
            "active_tasks": len(self.active_tasks),
            "queued_tasks": self.task_queue.qsize(),
            "timestamp": datetime.now().isoformat()
        }
    
    async def handle_conversation_overflow(self, conversation_id: str, token_usage: float):
        """ä¼šè©±ã®ãƒˆãƒ¼ã‚¯ãƒ³ä½¿ç”¨ç‡ãŒé–¾å€¤ã‚’è¶…ãˆãŸå ´åˆã®å‡¦ç†"""
        if token_usage > 0.8:
            logger.warning(f"âš ï¸ Conversation {conversation_id} reaching token limit: {token_usage:.1%}")
            
            # ä¼šè©±ã‚’è¦ç´„
            summary = await self._summarize_conversation(conversation_id)
            
            # OpenMemoryã«ä¿å­˜
            await self._save_to_memory(conversation_id, summary)
            
            # æ–°ã—ã„LLMã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã«åˆ‡ã‚Šæ›¿ãˆ
            await self._rotate_llm_instance(conversation_id, summary)
    
    async def _summarize_conversation(self, conversation_id: str) -> str:
        """ä¼šè©±ã‚’è¦ç´„"""
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ä¼šè©±å±¥æ­´ã‚’å–å¾—ã—ã¦è¦ç´„
        return f"Conversation {conversation_id} summary"
    
    async def _save_to_memory(self, conversation_id: str, summary: str):
        """OpenMemoryã«ä¿å­˜"""
        # å®Ÿéš›ã®å®Ÿè£…
        pass
    
    async def _rotate_llm_instance(self, conversation_id: str, context: str):
        """æ–°ã—ã„LLMã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã«åˆ‡ã‚Šæ›¿ãˆ"""
        # å®Ÿéš›ã®å®Ÿè£…
        pass


# ä½¿ç”¨ä¾‹
async def main():
    config = {
        "workers": {
            "backend_worker": {"model": "gpt-4-turbo"},
            "frontend_worker": {"model": "claude-3-sonnet"},
            "review_worker": {"model": "gpt-4"}
        },
        "memory": {
            "syncInterval": 300
        }
    }
    
    orchestrator = MultiLLMOrchestrator(config)
    await orchestrator.initialize()
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‡¦ç†
    result = await orchestrator.process_user_request(
        "ãƒã‚°ä¿®æ­£ã¨ãƒ†ã‚¹ãƒˆã‚³ãƒ¼ãƒ‰ã®ä½œæˆã‚’ãŠé¡˜ã„ã—ã¾ã™",
        user_id="user123"
    )
    
    print(json.dumps(result, indent=2, ensure_ascii=False))


if __name__ == "__main__":
    asyncio.run(main())