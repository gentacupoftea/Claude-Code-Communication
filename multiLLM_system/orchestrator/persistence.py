"""
OrchestratorÁî®„ÅÆÊ∞∏Á∂öÂåñÂ±§
„Çø„Çπ„ÇØ„ÄÅ‰ºöË©±„ÄÅÁä∂ÊÖã„ÅÆÊ∞∏Á∂öÂåñ„ÇíÁÆ°ÁêÜ
"""
import os
import json
import asyncio
import logging
from typing import Dict, List, Any, Optional
from datetime import datetime
from contextlib import asynccontextmanager
import redis.asyncio as redis
import asyncpg
from dataclasses import asdict
import pickle

logger = logging.getLogger(__name__)


class PersistenceManager:
    """Ê∞∏Á∂öÂåñ„Éû„Éç„Éº„Ç∏„É£„Éº - RedisÔºàÁü≠ÊúüÔºâ„Å®PostgreSQLÔºàÈï∑ÊúüÔºâ„ÇíÁÆ°ÁêÜ"""
    
    def __init__(self, config: Dict):
        self.config = config
        self.redis_client: Optional[redis.Redis] = None
        self.pg_pool: Optional[asyncpg.Pool] = None
        
        # RedisË®≠ÂÆö
        self.redis_config = config.get('redis', {
            'host': os.getenv('REDIS_HOST', 'localhost'),
            'port': int(os.getenv('REDIS_PORT', 6379)),
            'db': int(os.getenv('REDIS_DB', 0)),
            'password': os.getenv('REDIS_PASSWORD')
        })
        
        # PostgreSQLË®≠ÂÆö
        self.pg_config = {
            'host': os.getenv('PG_HOST', 'localhost'),
            'port': int(os.getenv('PG_PORT', 5432)),
            'database': os.getenv('PG_DATABASE', 'multillm'),
            'user': os.getenv('PG_USER', 'postgres'),
            'password': os.getenv('PG_PASSWORD', 'postgres')
        }
        
        # „Ç≠„É£„ÉÉ„Ç∑„É•Ë®≠ÂÆö
        self.cache_ttl = config.get('cache_ttl', 3600)  # 1ÊôÇÈñì
        self.max_retries = 3
        self.retry_delay = 1.0
    
    async def initialize(self):
        """Êé•Á∂ö„ÅÆÂàùÊúüÂåñ"""
        logger.info("üîß Initializing persistence layer...")
        
        # RedisÊé•Á∂ö
        try:
            self.redis_client = await redis.Redis(
                host=self.redis_config['host'],
                port=self.redis_config['port'],
                db=self.redis_config['db'],
                password=self.redis_config.get('password'),
                decode_responses=False  # „Éê„Ç§„Éä„É™„Éá„Éº„Çø„ÇíÊâ±„ÅÜ„Åü„ÇÅ
            )
            await self.redis_client.ping()
            logger.info("‚úÖ Redis connected successfully")
        except Exception as e:
            logger.error(f"‚ùå Redis connection failed: {e}")
            # Redis„ÅåÂà©Áî®„Åß„Åç„Å™„ÅÑÂ†¥Âêà„ÇÇÁ∂öË°åÔºà„Éá„Ç∞„É¨„Éº„ÉâÂãï‰ΩúÔºâ
            self.redis_client = None
        
        # PostgreSQLÊé•Á∂ö
        try:
            self.pg_pool = await asyncpg.create_pool(
                **self.pg_config,
                min_size=2,
                max_size=10,
                command_timeout=60
            )
            await self._create_tables()
            logger.info("‚úÖ PostgreSQL connected successfully")
        except Exception as e:
            logger.error(f"‚ùå PostgreSQL connection failed: {e}")
            raise  # PostgreSQL„ÅØÂøÖÈ†à
    
    async def shutdown(self):
        """Êé•Á∂ö„ÅÆ„ÇØ„É™„Éº„É≥„Ç¢„ÉÉ„Éó"""
        if self.redis_client:
            await self.redis_client.close()
        if self.pg_pool:
            await self.pg_pool.close()
    
    async def _create_tables(self):
        """ÂøÖË¶Å„Å™„ÉÜ„Éº„Éñ„É´„ÅÆ‰ΩúÊàê"""
        async with self.pg_pool.acquire() as conn:
            # „Çø„Çπ„ÇØ„ÉÜ„Éº„Éñ„É´
            await conn.execute('''
                CREATE TABLE IF NOT EXISTS tasks (
                    id VARCHAR(64) PRIMARY KEY,
                    type VARCHAR(50) NOT NULL,
                    description TEXT,
                    priority INTEGER NOT NULL,
                    user_id VARCHAR(64) NOT NULL,
                    status VARCHAR(20) NOT NULL DEFAULT 'pending',
                    assigned_worker VARCHAR(64),
                    result JSONB,
                    metadata JSONB,
                    created_at TIMESTAMP NOT NULL DEFAULT NOW(),
                    updated_at TIMESTAMP NOT NULL DEFAULT NOW(),
                    completed_at TIMESTAMP
                )
            ''')
            
            # ‰ºöË©±„ÉÜ„Éº„Éñ„É´
            await conn.execute('''
                CREATE TABLE IF NOT EXISTS conversations (
                    id VARCHAR(64) PRIMARY KEY,
                    user_id VARCHAR(64) NOT NULL,
                    messages JSONB NOT NULL DEFAULT '[]',
                    total_tokens INTEGER DEFAULT 0,
                    start_time TIMESTAMP NOT NULL DEFAULT NOW(),
                    end_time TIMESTAMP,
                    metadata JSONB,
                    created_at TIMESTAMP NOT NULL DEFAULT NOW(),
                    updated_at TIMESTAMP NOT NULL DEFAULT NOW()
                )
            ''')
            
            # LLMÂøúÁ≠î„É≠„Ç∞„ÉÜ„Éº„Éñ„É´
            await conn.execute('''
                CREATE TABLE IF NOT EXISTS llm_responses (
                    id VARCHAR(64) PRIMARY KEY,
                    conversation_id VARCHAR(64) REFERENCES conversations(id),
                    task_id VARCHAR(64) REFERENCES tasks(id),
                    provider VARCHAR(50) NOT NULL,
                    model VARCHAR(50) NOT NULL,
                    content TEXT NOT NULL,
                    tokens JSONB,
                    metadata JSONB,
                    duration FLOAT,
                    error TEXT,
                    created_at TIMESTAMP NOT NULL DEFAULT NOW()
                )
            ''')
            
            # OrchestratorÁä∂ÊÖã„ÉÜ„Éº„Éñ„É´
            await conn.execute('''
                CREATE TABLE IF NOT EXISTS orchestrator_state (
                    id VARCHAR(64) PRIMARY KEY,
                    state_type VARCHAR(50) NOT NULL,
                    state_data JSONB NOT NULL,
                    version INTEGER DEFAULT 1,
                    created_at TIMESTAMP NOT NULL DEFAULT NOW(),
                    updated_at TIMESTAMP NOT NULL DEFAULT NOW()
                )
            ''')
            
            # „Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„ÅÆ‰ΩúÊàê
            await conn.execute('CREATE INDEX IF NOT EXISTS idx_tasks_user_id ON tasks(user_id)')
            await conn.execute('CREATE INDEX IF NOT EXISTS idx_tasks_status ON tasks(status)')
            await conn.execute('CREATE INDEX IF NOT EXISTS idx_conversations_user_id ON conversations(user_id)')
            await conn.execute('CREATE INDEX IF NOT EXISTS idx_llm_responses_conversation ON llm_responses(conversation_id)')
    
    # ==================== „Çø„Çπ„ÇØÁÆ°ÁêÜ ====================
    
    async def save_task(self, task: 'Task') -> bool:
        """„Çø„Çπ„ÇØ„ÅÆ‰øùÂ≠ò"""
        try:
            # Redis„Å´Áü≠Êúü‰øùÂ≠ò
            if self.redis_client:
                key = f"task:{task.id}"
                await self.redis_client.setex(
                    key, 
                    self.cache_ttl,
                    pickle.dumps(asdict(task))
                )
            
            # PostgreSQL„Å´Ê∞∏Á∂ö‰øùÂ≠ò
            async with self.pg_pool.acquire() as conn:
                await conn.execute('''
                    INSERT INTO tasks (id, type, description, priority, user_id, 
                                     status, assigned_worker, result, metadata, created_at)
                    VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10)
                    ON CONFLICT (id) DO UPDATE SET
                        status = EXCLUDED.status,
                        assigned_worker = EXCLUDED.assigned_worker,
                        result = EXCLUDED.result,
                        metadata = EXCLUDED.metadata,
                        updated_at = NOW()
                ''', 
                    task.id,
                    task.type.value,
                    task.description,
                    task.priority.value,
                    task.user_id,
                    task.status,
                    task.assigned_worker,
                    json.dumps(task.result) if task.result else None,
                    json.dumps(task.metadata) if task.metadata else None,
                    task.created_at
                )
            
            return True
        except Exception as e:
            logger.error(f"Failed to save task {task.id}: {e}")
            return False
    
    async def get_task(self, task_id: str) -> Optional[Dict]:
        """„Çø„Çπ„ÇØ„ÅÆÂèñÂæó"""
        try:
            # „Åæ„ÅöRedis„Åã„ÇâÂèñÂæó„ÇíË©¶„Åø„Çã
            if self.redis_client:
                key = f"task:{task_id}"
                data = await self.redis_client.get(key)
                if data:
                    return pickle.loads(data)
            
            # Redis„Å´„Å™„Åë„Çå„Å∞PostgreSQL„Åã„ÇâÂèñÂæó
            async with self.pg_pool.acquire() as conn:
                row = await conn.fetchrow(
                    'SELECT * FROM tasks WHERE id = $1',
                    task_id
                )
                if row:
                    task_data = dict(row)
                    # Redis„Å´„Ç≠„É£„ÉÉ„Ç∑„É•
                    if self.redis_client:
                        await self.redis_client.setex(
                            f"task:{task_id}",
                            self.cache_ttl,
                            pickle.dumps(task_data)
                        )
                    return task_data
            
            return None
        except Exception as e:
            logger.error(f"Failed to get task {task_id}: {e}")
            return None
    
    async def update_task_status(self, task_id: str, status: str, result: Optional[Dict] = None, metadata: Optional[Dict] = None):
        """„Çø„Çπ„ÇØ„Çπ„ÉÜ„Éº„Çø„Çπ„ÅÆÊõ¥Êñ∞"""
        try:
            async with self.pg_pool.acquire() as conn:
                query = '''
                    UPDATE tasks 
                    SET status = $2, updated_at = NOW()
                '''
                params = [task_id, status]
                param_count = 2
                
                if result is not None:
                    param_count += 1
                    query += f', result = ${param_count}'
                    params.append(json.dumps(result))
                
                if metadata is not None:
                    param_count += 1
                    query += f', metadata = ${param_count}'
                    params.append(json.dumps(metadata))
                
                if status == 'completed':
                    query += f', completed_at = NOW()'
                elif status == 'interrupted':
                    # ‰∏≠Êñ≠„Åï„Çå„Åü„Çø„Çπ„ÇØ„ÅÆÂ†¥Âêà„ÇÇÁµÇ‰∫ÜÊôÇÂàª„ÇíË®òÈå≤
                    query += f', completed_at = NOW()'
                
                query += ' WHERE id = $1'
                
                await conn.execute(query, *params)
            
            # Redis„Ç≠„É£„ÉÉ„Ç∑„É•„Çí„ÇØ„É™„Ç¢
            if self.redis_client:
                await self.redis_client.delete(f"task:{task_id}")
                
        except Exception as e:
            logger.error(f"Failed to update task status {task_id}: {e}")
    
    async def get_pending_tasks(self, limit: int = 100) -> List[Dict]:
        """Êú™Âá¶ÁêÜ„Çø„Çπ„ÇØ„ÅÆÂèñÂæó"""
        try:
            async with self.pg_pool.acquire() as conn:
                rows = await conn.fetch('''
                    SELECT * FROM tasks 
                    WHERE status = 'pending' 
                    ORDER BY priority ASC, created_at ASC
                    LIMIT $1
                ''', limit)
                return [dict(row) for row in rows]
        except Exception as e:
            logger.error(f"Failed to get pending tasks: {e}")
            return []
    
    async def get_tasks_by_status(self, status: str, limit: int = 100) -> List[Dict]:
        """„Çπ„ÉÜ„Éº„Çø„ÇπÂà•„Çø„Çπ„ÇØ„ÅÆÂèñÂæó"""
        try:
            async with self.pg_pool.acquire() as conn:
                rows = await conn.fetch('''
                    SELECT * FROM tasks 
                    WHERE status = $1 
                    ORDER BY priority ASC, created_at ASC
                    LIMIT $2
                ''', status, limit)
                return [dict(row) for row in rows]
        except Exception as e:
            logger.error(f"Failed to get tasks by status {status}: {e}")
            return []
    
    # ==================== ‰ºöË©±ÁÆ°ÁêÜ ====================
    
    async def save_conversation(self, conversation: 'ConversationLog') -> bool:
        """‰ºöË©±„ÅÆ‰øùÂ≠ò"""
        try:
            conv_dict = asdict(conversation)
            
            # Redis„Å´Áü≠Êúü‰øùÂ≠ò
            if self.redis_client:
                key = f"conversation:{conversation.conversation_id}"
                await self.redis_client.setex(
                    key,
                    self.cache_ttl * 24,  # ‰ºöË©±„ÅØ24ÊôÇÈñì„Ç≠„É£„ÉÉ„Ç∑„É•
                    pickle.dumps(conv_dict)
                )
            
            # PostgreSQL„Å´Ê∞∏Á∂ö‰øùÂ≠ò
            async with self.pg_pool.acquire() as conn:
                await conn.execute('''
                    INSERT INTO conversations (id, user_id, messages, total_tokens, 
                                             start_time, end_time, metadata)
                    VALUES ($1, $2, $3, $4, $5, $6, $7)
                    ON CONFLICT (id) DO UPDATE SET
                        messages = EXCLUDED.messages,
                        total_tokens = EXCLUDED.total_tokens,
                        end_time = EXCLUDED.end_time,
                        metadata = EXCLUDED.metadata,
                        updated_at = NOW()
                ''',
                    conversation.conversation_id,
                    conv_dict.get('user_id', 'unknown'),
                    json.dumps(conversation.messages),
                    conversation.total_tokens,
                    conversation.start_time,
                    conversation.end_time,
                    json.dumps(conv_dict.get('metadata', {}))
                )
                
                # LLMÂøúÁ≠î„ÅÆ‰øùÂ≠ò
                for llm_response in conversation.llm_responses:
                    await self._save_llm_response(conn, llm_response, conversation.conversation_id)
            
            return True
        except Exception as e:
            logger.error(f"Failed to save conversation {conversation.conversation_id}: {e}")
            return False
    
    async def get_conversation(self, conversation_id: str) -> Optional[Dict]:
        """‰ºöË©±„ÅÆÂèñÂæó"""
        try:
            # „Åæ„ÅöRedis„Åã„ÇâÂèñÂæó
            if self.redis_client:
                key = f"conversation:{conversation_id}"
                data = await self.redis_client.get(key)
                if data:
                    return pickle.loads(data)
            
            # Redis„Å´„Å™„Åë„Çå„Å∞PostgreSQL„Åã„ÇâÂèñÂæó
            async with self.pg_pool.acquire() as conn:
                row = await conn.fetchrow(
                    'SELECT * FROM conversations WHERE id = $1',
                    conversation_id
                )
                if row:
                    conv_data = dict(row)
                    
                    # LLMÂøúÁ≠î„ÇÇÂèñÂæó
                    llm_rows = await conn.fetch(
                        'SELECT * FROM llm_responses WHERE conversation_id = $1',
                        conversation_id
                    )
                    conv_data['llm_responses'] = [dict(row) for row in llm_rows]
                    
                    # Redis„Å´„Ç≠„É£„ÉÉ„Ç∑„É•
                    if self.redis_client:
                        await self.redis_client.setex(
                            f"conversation:{conversation_id}",
                            self.cache_ttl * 24,
                            pickle.dumps(conv_data)
                        )
                    
                    return conv_data
            
            return None
        except Exception as e:
            logger.error(f"Failed to get conversation {conversation_id}: {e}")
            return None
    
    async def _save_llm_response(self, conn: asyncpg.Connection, 
                                llm_response: 'LLMResponse', 
                                conversation_id: str):
        """LLMÂøúÁ≠î„ÅÆ‰øùÂ≠òÔºàÂÜÖÈÉ®„É°„ÇΩ„ÉÉ„ÉâÔºâ"""
        await conn.execute('''
            INSERT INTO llm_responses (id, conversation_id, provider, model,
                                     content, tokens, metadata, duration, error)
            VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
        ''',
            llm_response.id,
            conversation_id,
            llm_response.provider,
            llm_response.model,
            llm_response.content,
            json.dumps(llm_response.tokens),
            json.dumps(llm_response.metadata),
            llm_response.duration,
            llm_response.error
        )
    
    # ==================== OrchestratorÁä∂ÊÖãÁÆ°ÁêÜ ====================
    
    async def save_orchestrator_state(self, state_type: str, state_data: Dict) -> bool:
        """Orchestrator„ÅÆÁä∂ÊÖã‰øùÂ≠ò"""
        try:
            state_id = f"{state_type}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            async with self.pg_pool.acquire() as conn:
                await conn.execute('''
                    INSERT INTO orchestrator_state (id, state_type, state_data)
                    VALUES ($1, $2, $3)
                ''',
                    state_id,
                    state_type,
                    json.dumps(state_data)
                )
            
            return True
        except Exception as e:
            logger.error(f"Failed to save orchestrator state: {e}")
            return False
    
    async def get_latest_orchestrator_state(self, state_type: str) -> Optional[Dict]:
        """ÊúÄÊñ∞„ÅÆOrchestratorÁä∂ÊÖãÂèñÂæó"""
        try:
            async with self.pg_pool.acquire() as conn:
                row = await conn.fetchrow('''
                    SELECT * FROM orchestrator_state 
                    WHERE state_type = $1 
                    ORDER BY created_at DESC 
                    LIMIT 1
                ''', state_type)
                
                if row:
                    return dict(row)
            
            return None
        except Exception as e:
            logger.error(f"Failed to get orchestrator state: {e}")
            return None
    
    # ==================== „É¶„Éº„ÉÜ„Ç£„É™„ÉÜ„Ç£ ====================
    
    @asynccontextmanager
    async def redis_pipeline(self):
        """Redis„Éë„Ç§„Éó„É©„Ç§„É≥Áî®„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„Éû„Éç„Éº„Ç∏„É£„Éº"""
        if self.redis_client:
            async with self.redis_client.pipeline() as pipe:
                yield pipe
        else:
            yield None
    
    async def health_check(self) -> Dict[str, bool]:
        """„Éò„É´„Çπ„ÉÅ„Çß„ÉÉ„ÇØ"""
        health = {
            'redis': False,
            'postgresql': False
        }
        
        # Redis„ÉÅ„Çß„ÉÉ„ÇØ
        if self.redis_client:
            try:
                await self.redis_client.ping()
                health['redis'] = True
            except:
                pass
        
        # PostgreSQL„ÉÅ„Çß„ÉÉ„ÇØ
        if self.pg_pool:
            try:
                async with self.pg_pool.acquire() as conn:
                    await conn.fetchval('SELECT 1')
                health['postgresql'] = True
            except:
                pass
        
        return health
    
    # ==================== „Éà„É©„É≥„Ç∂„ÇØ„Ç∑„Éß„É≥ÁÆ°ÁêÜ ====================
    
    @asynccontextmanager
    async def transaction(self):
        """„Éà„É©„É≥„Ç∂„ÇØ„Ç∑„Éß„É≥„Ç≥„É≥„ÉÜ„Ç≠„Çπ„Éà„Éû„Éç„Éº„Ç∏„É£„Éº"""
        async with self.pg_pool.acquire() as conn:
            async with conn.transaction():
                yield conn
    
    async def archive_old_conversations(self, days_old: int = 30) -> int:
        """Âè§„ÅÑ‰ºöË©±„Çí„Ç¢„Éº„Ç´„Ç§„Éñ"""
        try:
            archived_count = 0
            async with self.transaction() as conn:
                # „Ç¢„Éº„Ç´„Ç§„Éñ„ÉÜ„Éº„Éñ„É´„ÅÆ‰ΩúÊàê
                await conn.execute('''
                    CREATE TABLE IF NOT EXISTS conversations_archive (
                        LIKE conversations INCLUDING ALL
                    )
                ''')
                
                # Âè§„ÅÑ‰ºöË©±„Çí„Ç¢„Éº„Ç´„Ç§„Éñ„Å´ÁßªÂãï
                result = await conn.execute('''
                    WITH moved AS (
                        DELETE FROM conversations 
                        WHERE end_time IS NOT NULL 
                        AND end_time < NOW() - INTERVAL '%s days'
                        RETURNING *
                    )
                    INSERT INTO conversations_archive 
                    SELECT * FROM moved
                ''', days_old)
                
                archived_count = int(result.split()[-1]) if result else 0
                logger.info(f"Archived {archived_count} conversations older than {days_old} days")
            
            return archived_count
        except Exception as e:
            logger.error(f"Failed to archive conversations: {e}")
            return 0
    
    async def get_task_statistics(self, user_id: Optional[str] = None) -> Dict:
        """„Çø„Çπ„ÇØÁµ±Ë®à„ÅÆÂèñÂæó"""
        try:
            async with self.pg_pool.acquire() as conn:
                if user_id:
                    query = '''
                        SELECT 
                            status, 
                            COUNT(*) as count,
                            AVG(EXTRACT(EPOCH FROM (COALESCE(completed_at, NOW()) - created_at))) as avg_duration
                        FROM tasks 
                        WHERE user_id = $1
                        GROUP BY status
                    '''
                    rows = await conn.fetch(query, user_id)
                else:
                    query = '''
                        SELECT 
                            status, 
                            COUNT(*) as count,
                            AVG(EXTRACT(EPOCH FROM (COALESCE(completed_at, NOW()) - created_at))) as avg_duration
                        FROM tasks 
                        GROUP BY status
                    '''
                    rows = await conn.fetch(query)
                
                stats = {
                    'by_status': {row['status']: {
                        'count': row['count'],
                        'avg_duration_seconds': float(row['avg_duration']) if row['avg_duration'] else None
                    } for row in rows},
                    'total': sum(row['count'] for row in rows)
                }
                
                return stats
        except Exception as e:
            logger.error(f"Failed to get task statistics: {e}")
            return {}