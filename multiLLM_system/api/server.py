"""
MultiLLM API Server - SSEå¯¾å¿œã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°APIã‚µãƒ¼ãƒãƒ¼

ã“ã®APIã‚µãƒ¼ãƒãƒ¼ã¯è¤‡æ•°ã®LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼ˆClaudeã€GPT-4ã€Geminiã€Local LLMï¼‰ã‚’
çµ±åˆçš„ã«ç®¡ç†ã—ã€ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒãƒ£ãƒƒãƒˆã€åˆ†æã€è‡ªå‹•åŒ–æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚

ä¸»ãªæ©Ÿèƒ½:
- ãƒãƒ«ãƒLLMãƒãƒ£ãƒƒãƒˆï¼ˆã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œï¼‰
- ãƒ¯ãƒ¼ã‚«ãƒ¼ç®¡ç†ã¨ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
- é«˜åº¦ãªãƒ‡ãƒ¼ã‚¿åˆ†æã¨ãƒ¬ãƒãƒ¼ãƒˆ
- è‡ªå‹•åŒ–ãƒ«ãƒ¼ãƒ«ä½œæˆãƒ»å®Ÿè¡Œ
- ãƒ—ãƒ­ãƒ¡ãƒ†ã‚¦ã‚¹å¯¾å¿œãƒ¡ãƒˆãƒªã‚¯ã‚¹
"""

import asyncio
import json
import logging
import uuid
from datetime import datetime
from typing import Dict, Optional, List, Any, Union
from fastapi import FastAPI, HTTPException, Request, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel, Field, validator
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded
import sys
import os
import requests

# è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from orchestrator.orchestrator import MultiLLMOrchestrator
from orchestrator.response_formatter import ResponseFormatter, MessageProcessor
from orchestrator.analyzers.data_analyzer import DataAnalyzer
from orchestrator.automation.task_automator import TaskAutomator
from orchestrator.worker_factory import WorkerFactory
from config.settings import settings
from utils.exceptions import (
    MultiLLMBaseException, OllamaServerError, OllamaConnectionError, 
    APIKeyError, ModelNotFoundError, WorkerNotFoundError,
    GenerationError, ValidationError, exception_to_http_status
)
from config.logging_config import setup_logging, get_logger, request_context
import traceback

# ãƒ­ã‚°è¨­å®šã®åˆæœŸåŒ–
setup_logging(
    log_level=settings.LOG_LEVEL,
    log_file="logs/multillm_api.log",
    enable_json_format=not settings.DEBUG
)
logger = get_logger("multiLLM.api")

# ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã®è¨­å®š
limiter = Limiter(key_func=get_remote_address)

# FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³è¨­å®š
app = FastAPI(
    title="Conea MultiLLM Integration API",
    version="1.0.0",
    description="""
    ## Conea MultiLLMçµ±åˆãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ  API

    è¤‡æ•°ã®LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’çµ±åˆã—ã€ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºå‘ã‘AIæ©Ÿèƒ½ã‚’æä¾›ã™ã‚‹APIã‚µãƒ¼ãƒãƒ¼ã§ã™ã€‚

    ### ä¸»è¦æ©Ÿèƒ½:
    - **ãƒãƒ«ãƒLLMãƒãƒ£ãƒƒãƒˆ**: Claudeã€GPT-4ã€Geminiã€Local LLMã«ã‚ˆã‚‹ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒãƒ£ãƒƒãƒˆ
    - **ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œ**: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ SSEã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒ¬ã‚¹ãƒãƒ³ã‚¹
    - **ãƒ¯ãƒ¼ã‚«ãƒ¼ç®¡ç†**: å„LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®çŠ¶æ…‹ç›£è¦–ã¨å‹•çš„ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
    - **åˆ†æã‚¨ãƒ³ã‚¸ãƒ³**: ä¼šè©±ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã€ã‚¿ã‚¹ã‚¯ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®š
    - **è‡ªå‹•åŒ–ã‚·ã‚¹ãƒ†ãƒ **: ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹è‡ªå‹•åŒ–ã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°
    - **ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯**: ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ç›£è¦–ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†

    ### å¯¾å¿œLLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼:
    - **Anthropic Claude**: é«˜åº¦ãªæ¨è«–ã€ãƒ‡ãƒ¼ã‚¿åˆ†æã€è¤‡é›‘ãªã‚¿ã‚¹ã‚¯å‡¦ç†
    - **OpenAI GPT-4**: æ±ç”¨çš„ãªå¯¾è©±ã€ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã€æ•°å€¤è¨ˆç®—
    - **Local LLM**: ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆç’°å¢ƒã§ã®ã‚ªãƒ³ãƒ—ãƒ¬ãƒŸã‚¹å‡¦ç†ï¼ˆOllamaçµŒç”±ï¼‰

    ### èªè¨¼ãƒ»ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£:
    - ãƒ¬ãƒ¼ãƒˆåˆ¶é™ï¼ˆ30ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/åˆ†ï¼‰
    - CORSå¯¾å¿œ
    - ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

    *Powered by Conea AI Platform v1.0.0*
    """,
    terms_of_service="https://conea.ai/terms",
    contact={
        "name": "Conea Support Team",
        "url": "https://conea.ai/support",
        "email": "support@conea.ai",
    },
    license_info={
        "name": "MIT License",
        "url": "https://opensource.org/licenses/MIT",
    },
    openapi_tags=[
        {
            "name": "chat",
            "description": "ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ - LLMã¨ã®å¯¾è©±ã€ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã€ä¼šè©±å±¥æ­´ç®¡ç†"
        },
        {
            "name": "workers",
            "description": "ãƒ¯ãƒ¼ã‚«ãƒ¼ç®¡ç† - LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®çŠ¶æ…‹ç¢ºèªã€ãƒ¢ãƒ‡ãƒ«æƒ…å ±å–å¾—"
        },
        {
            "name": "analytics", 
            "description": "åˆ†ææ©Ÿèƒ½ - ãƒ‡ãƒ¼ã‚¿åˆ†æã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šã€ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"
        },
        {
            "name": "automation",
            "description": "è‡ªå‹•åŒ–æ©Ÿèƒ½ - ãƒ«ãƒ¼ãƒ«ä½œæˆã€ã‚¿ã‚¹ã‚¯è‡ªå‹•å®Ÿè¡Œã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°"
        },
        {
            "name": "health",
            "description": "ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ - ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ç›£è¦–ã€æ¥ç¶šç¢ºèª"
        }
    ]
)

# ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’è¿½åŠ 
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)


# === ã‚°ãƒ­ãƒ¼ãƒãƒ«ä¾‹å¤–ãƒãƒ³ãƒ‰ãƒ© ===

@app.exception_handler(MultiLLMBaseException)
async def multillm_exception_handler(request: Request, exc: MultiLLMBaseException):
    """MultiLLMã‚·ã‚¹ãƒ†ãƒ å›ºæœ‰ã®ä¾‹å¤–ãƒãƒ³ãƒ‰ãƒ©"""
    status_code = exception_to_http_status(exc)
    
    logger.log_error(
        exc,
        context={
            "endpoint": str(request.url),
            "method": request.method,
            "status_code": status_code
        }
    )
    
    return JSONResponse(
        status_code=status_code,
        content={
            "success": False,
            "error": exc.to_dict(),
            "timestamp": datetime.utcnow().isoformat() + "Z"
        }
    )


@app.exception_handler(HTTPException)
async def http_exception_handler(request: Request, exc: HTTPException):
    """HTTPä¾‹å¤–ã®æ§‹é€ åŒ–ãƒãƒ³ãƒ‰ãƒ©"""
    logger.logger.warning(
        f"HTTP {exc.status_code}: {exc.detail}",
        extra={
            "event_type": "http_error",
            "status_code": exc.status_code,
            "endpoint": str(request.url),
            "method": request.method
        }
    )
    
    return JSONResponse(
        status_code=exc.status_code,
        content={
            "success": False,
            "error": {
                "error_code": f"HTTP_{exc.status_code}",
                "message": exc.detail,
                "type": "HTTPException"
            },
            "timestamp": datetime.utcnow().isoformat() + "Z"
        }
    )


@app.exception_handler(Exception)
async def general_exception_handler(request: Request, exc: Exception):
    """ä¸€èˆ¬çš„ãªä¾‹å¤–ã®æ§‹é€ åŒ–ãƒãƒ³ãƒ‰ãƒ©"""
    logger.log_error(
        exc,
        context={
            "endpoint": str(request.url),
            "method": request.method,
            "traceback": traceback.format_exc()
        }
    )
    
    # ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã¯è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’è¿”ã™
    error_detail = str(exc) if settings.DEBUG else "Internal server error"
    
    return JSONResponse(
        status_code=500,
        content={
            "success": False,
            "error": {
                "error_code": "INTERNAL_SERVER_ERROR",
                "message": error_detail,
                "type": type(exc).__name__,
                "traceback": traceback.format_exc() if settings.DEBUG else None
            },
            "timestamp": datetime.utcnow().isoformat() + "Z"
        }
    )

# CORSè¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://localhost:3001", "http://localhost:3500", "*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ============ Pydantic Models (Request/Response Schemas) ============

class ChatRequest(BaseModel):
    """
    ãƒãƒ£ãƒƒãƒˆãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¹ã‚­ãƒ¼ãƒ
    
    LLMã¨ã®å¯¾è©±ã‚’é–‹å§‹ã™ã‚‹ãŸã‚ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã€‚
    è¤‡æ•°ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¿ã‚¤ãƒ—ã¨ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å¯¾å¿œã€‚
    """
    message: str = Field(
        ..., 
        min_length=1, 
        max_length=10000,
        description="ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å†…å®¹",
        example="ECã‚µã‚¤ãƒˆã®å£²ä¸Šåˆ†æã‚’è¡Œã£ã¦ãã ã•ã„"
    )
    conversation_id: Optional[str] = Field(
        None,
        description="æ—¢å­˜ã®ä¼šè©±ã‚’ç¶™ç¶šã™ã‚‹å ´åˆã®ä¼šè©±ID",
        example="conv_12345-abcde"
    )
    user_id: str = Field(
        default="default_user",
        description="ãƒ¦ãƒ¼ã‚¶ãƒ¼è­˜åˆ¥å­",
        example="user_123"
    )
    context: Optional[Dict[str, Any]] = Field(
        None,
        description="è¿½åŠ ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ãƒ¼ã‚¿ã€è¨­å®šãªã©ï¼‰",
        example={"project_id": "proj_123", "language": "ja"}
    )
    worker_type: Optional[str] = Field(
        None,
        description="ä½¿ç”¨ã™ã‚‹LLMãƒ¯ãƒ¼ã‚«ãƒ¼ã®ã‚¿ã‚¤ãƒ—ã€‚æŒ‡å®šã—ãªã„å ´åˆã¯è‡ªå‹•é¸æŠ",
        example="claude",
        regex="^(openai|anthropic|claude|local_llm)$"
    )


class ChatResponse(BaseModel):
    """ãƒãƒ£ãƒƒãƒˆãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¹ã‚­ãƒ¼ãƒ"""
    success: bool = Field(description="ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒæˆåŠŸã—ãŸã‹ã©ã†ã‹")
    response: str = Field(description="LLMã‹ã‚‰ã®å›ç­”ãƒ†ã‚­ã‚¹ãƒˆ")
    conversation_id: str = Field(description="ä¼šè©±ID")
    worker_type: Optional[str] = Field(description="å®Ÿéš›ã«ä½¿ç”¨ã•ã‚ŒãŸãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¿ã‚¤ãƒ—")
    task_analysis: Optional[Dict[str, Any]] = Field(description="ã‚¿ã‚¹ã‚¯åˆ†æçµæœ")
    fallback_info: Optional[Dict[str, Any]] = Field(description="ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æƒ…å ±")
    metadata: Optional[Dict[str, Any]] = Field(description="è¿½åŠ ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿")


class ConversationDebugRequest(BaseModel):
    """
    ä¼šè©±ãƒ‡ãƒãƒƒã‚°ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
    
    ç‰¹å®šã®ä¼šè©±ã®è©³ç´°ãªå®Ÿè¡Œæƒ…å ±ã‚’å–å¾—ã™ã‚‹ãŸã‚ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã€‚
    """
    conversation_id: str = Field(
        ..., 
        description="ãƒ‡ãƒãƒƒã‚°å¯¾è±¡ã®ä¼šè©±ID",
        example="conv_12345-abcde"
    )


class AnalysisRequest(BaseModel):
    """
    ãƒ‡ãƒ¼ã‚¿åˆ†æãƒªã‚¯ã‚¨ã‚¹ãƒˆ
    
    ä¼šè©±ãƒ‘ã‚¿ãƒ¼ãƒ³ã€ã‚¿ã‚¹ã‚¯ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã€ãƒªã‚½ãƒ¼ã‚¹äºˆæ¸¬ãªã©ã®
    é«˜åº¦ãªãƒ‡ãƒ¼ã‚¿åˆ†æã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã€‚
    """
    analysis_type: str = Field(
        ...,
        description="åˆ†æã‚¿ã‚¤ãƒ—",
        example="conversation_patterns",
        regex="^(conversation_patterns|task_performance|resource_prediction)$"
    )
    data: Optional[Dict[str, Any]] = Field(
        None,
        description="åˆ†æå¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿",
        example={"tasks": [], "conversations": []}
    )
    time_range: Optional[Dict[str, str]] = Field(
        None,
        description="åˆ†æå¯¾è±¡ã®æ™‚é–“ç¯„å›²",
        example={"start": "2024-01-01", "end": "2024-01-31"}
    )


class AutomationRuleRequest(BaseModel):
    """
    è‡ªå‹•åŒ–ãƒ«ãƒ¼ãƒ«ä½œæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆ
    
    ã‚·ã‚¹ãƒ†ãƒ ã®è‡ªå‹•åŒ–ãƒ«ãƒ¼ãƒ«ã‚’å®šç¾©ã—ã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã‚„
    ã‚¤ãƒ™ãƒ³ãƒˆé§†å‹•ã®è‡ªå‹•å‡¦ç†ã‚’è¨­å®šã™ã‚‹ãŸã‚ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã€‚
    """
    name: str = Field(
        ...,
        min_length=1,
        max_length=100,
        description="è‡ªå‹•åŒ–ãƒ«ãƒ¼ãƒ«ã®åå‰",
        example="å®šæœŸå£²ä¸Šãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"
    )
    description: str = Field(
        ...,
        max_length=500,
        description="ãƒ«ãƒ¼ãƒ«ã®è©³ç´°èª¬æ˜",
        example="æ¯é€±æœˆæ›œæ—¥ã«Shopifyã®å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¦ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"
    )
    trigger_type: str = Field(
        ...,
        description="ãƒˆãƒªã‚¬ãƒ¼ã®ã‚¿ã‚¤ãƒ—",
        example="time_based",
        regex="^(time_based|event_based|condition_based|pattern_based)$"
    )
    trigger_config: Dict[str, Any] = Field(
        ...,
        description="ãƒˆãƒªã‚¬ãƒ¼ã®è¨­å®š",
        example={"schedule": "0 9 * * MON", "timezone": "Asia/Tokyo"}
    )
    actions: List[Dict[str, Any]] = Field(
        ...,
        min_items=1,
        description="å®Ÿè¡Œã™ã‚‹ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®ãƒªã‚¹ãƒˆ",
        example=[{"type": "analysis", "target": "shopify_sales"}]
    )
    active: bool = Field(
        default=True,
        description="ãƒ«ãƒ¼ãƒ«ãŒã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‹ã©ã†ã‹"
    )


class GenerationRequest(BaseModel):
    """
    ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆ
    
    æŒ‡å®šã•ã‚ŒãŸLLMãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’ä½¿ç”¨ã—ã¦ç›´æ¥ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚’è¡Œã†ãŸã‚ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã€‚
    ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½ã‚ˆã‚Šã‚‚ã‚·ãƒ³ãƒ—ãƒ«ã§ã€å˜ç™ºã®ç”Ÿæˆã‚¿ã‚¹ã‚¯ã«é©ã—ã¦ã„ã‚‹ã€‚
    """
    prompt: str = Field(
        ...,
        min_length=1,
        max_length=8000,
        description="ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ",
        example="Pythonã§ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿é–¢æ•°ã‚’ä½œæˆã—ã¦ãã ã•ã„"
    )
    worker_type: str = Field(
        default="openai",
        description="ä½¿ç”¨ã™ã‚‹ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¿ã‚¤ãƒ—",
        example="claude",
        regex="^(openai|anthropic|claude|local_llm)$"
    )
    model_id: Optional[str] = Field(
        None,
        description="ç‰¹å®šã®ãƒ¢ãƒ‡ãƒ«IDï¼ˆãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¿ã‚¤ãƒ—å†…ã§ã®é¸æŠï¼‰",
        example="gpt-4-turbo"
    )


class GenerationResponse(BaseModel):
    """ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆãƒ¬ã‚¹ãƒãƒ³ã‚¹"""
    success: bool = Field(description="ç”ŸæˆãŒæˆåŠŸã—ãŸã‹ã©ã†ã‹")
    response: str = Field(description="ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ")
    worker_type: str = Field(description="ä½¿ç”¨ã•ã‚ŒãŸãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¿ã‚¤ãƒ—")
    model_id: Optional[str] = Field(description="ä½¿ç”¨ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ID")
    metadata: Optional[Dict[str, Any]] = Field(description="ç”Ÿæˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿")


class HealthResponse(BaseModel):
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹"""
    status: str = Field(description="ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹", example="healthy")
    timestamp: str = Field(description="ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œæ™‚åˆ»")
    orchestrator: Optional[Dict[str, Any]] = Field(description="ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼çŠ¶æ…‹")
    services: Optional[Dict[str, bool]] = Field(description="å„ã‚µãƒ¼ãƒ“ã‚¹ã®çŠ¶æ…‹")


class ErrorResponse(BaseModel):
    """ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹"""
    success: bool = Field(default=False, description="å¸¸ã«false")
    error: str = Field(description="ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸")
    error_type: Optional[str] = Field(description="ã‚¨ãƒ©ãƒ¼ã‚¿ã‚¤ãƒ—")
    details: Optional[Dict[str, Any]] = Field(description="ã‚¨ãƒ©ãƒ¼è©³ç´°æƒ…å ±")
    timestamp: str = Field(description="ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚åˆ»")


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
orchestrator = None
data_analyzer = None
task_automator = None


@app.on_event("startup")
async def startup_event():
    """ã‚µãƒ¼ãƒãƒ¼èµ·å‹•æ™‚ã®åˆæœŸåŒ–"""
    global orchestrator, data_analyzer, task_automator
    
    config = {
        "workers": {
            "backend_worker": {"model": "claude-3.5-sonnet"},
            "frontend_worker": {"model": "claude-3.5-sonnet"},
            "review_worker": {"model": "claude-3.5-sonnet"},
            "analytics_worker": {"model": "claude-3.5-sonnet"},
            "documentation_worker": {"model": "claude-3.5-sonnet"},
            "mcp_worker": {"model": "claude-3.5-sonnet"}
        },
        "memory": {
            "syncInterval": 300
        }
    }
    
    orchestrator = MultiLLMOrchestrator(config)
    await orchestrator.initialize()
    
    # åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã¨è‡ªå‹•åŒ–ã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆæœŸåŒ–
    data_analyzer = DataAnalyzer()
    task_automator = TaskAutomator(orchestrator)
    
    logger.info("âœ… MultiLLM API Server started with Advanced Analytics")


@app.on_event("shutdown")
async def shutdown_event():
    """ã‚µãƒ¼ãƒãƒ¼çµ‚äº†æ™‚ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    global orchestrator
    if orchestrator:
        await orchestrator.shutdown()
    logger.info("ğŸ‘‹ MultiLLM API Server shutdown")


@app.get(
    "/",
    summary="ã‚µãƒ¼ãƒ“ã‚¹æƒ…å ±å–å¾—",
    description="MultiLLM APIã‚µãƒ¼ãƒ“ã‚¹ã®åŸºæœ¬æƒ…å ±ã‚’å–å¾—ã—ã¾ã™",
    response_model=Dict[str, str],
    tags=["system"]
)
async def root():
    """
    ### ã‚µãƒ¼ãƒ“ã‚¹æƒ…å ±ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    
    MultiLLM APIã‚µãƒ¼ãƒãƒ¼ã®åŸºæœ¬æƒ…å ±ã¨ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’è¿”ã—ã¾ã™ã€‚
    
    **åˆ©ç”¨ä¾‹:**
    ```bash
    curl http://localhost:8000/
    ```
    
    **ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹:**
    ```json
    {
        "service": "MultiLLM API",
        "version": "1.0.0", 
        "status": "active"
    }
    ```
    """
    return {
        "service": "Conea MultiLLM Integration API",
        "version": "1.0.0",
        "status": "active",
        "docs": "/docs",
        "redoc": "/redoc"
    }


@app.get(
    "/health",
    summary="ã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯",
    description="ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®ç¨¼åƒçŠ¶æ³ã‚’ç¢ºèªã—ã¾ã™",
    response_model=HealthResponse,
    responses={
        200: {"description": "ã‚·ã‚¹ãƒ†ãƒ æ­£å¸¸ç¨¼åƒ"},
        503: {"description": "ã‚·ã‚¹ãƒ†ãƒ ç•°å¸¸", "model": ErrorResponse}
    },
    tags=["health"]
)
async def health():
    """
    ### ã‚·ã‚¹ãƒ†ãƒ ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
    
    MultiLLMã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®ç¨¼åƒçŠ¶æ³ã‚’ç¢ºèªã—ã€å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®çŠ¶æ…‹ã‚’è¿”ã—ã¾ã™ã€‚
    
    **ãƒã‚§ãƒƒã‚¯é …ç›®:**
    - ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ã®çŠ¶æ…‹
    - å„LLMãƒ¯ãƒ¼ã‚«ãƒ¼ã®æ¥ç¶šçŠ¶æ³
    - ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ¥ç¶š
    - ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡
    
    **åˆ©ç”¨ä¾‹:**
    ```bash
    curl http://localhost:8000/health
    ```
    
    **æ­£å¸¸æ™‚ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹:**
    ```json
    {
        "status": "healthy",
        "timestamp": "2024-01-01T12:00:00",
        "orchestrator": {
            "status": "active",
            "workers": 4,
            "uptime": 3600
        }
    }
    ```
    """
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "orchestrator": orchestrator.get_status() if orchestrator else None
    }


@app.get("/health/ollama", tags=["Health Checks"])
async def health_check_ollama():
    """
    Ollamaã‚µãƒ¼ãƒãƒ¼ã®ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã¨æ¥ç¶šæ€§ã‚’ç¢ºèªã—ã¾ã™ã€‚

    Returns:
        dict: ã‚µãƒ¼ãƒãƒ¼ã®çŠ¶æ…‹ã‚’ç¤ºã™JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹
            - status: "ok" (æ­£å¸¸) ã¾ãŸã¯ "error" (ç•°å¸¸)
            - message: çŠ¶æ…‹ã®èª¬æ˜
            - url: ãƒã‚§ãƒƒã‚¯ã—ãŸOllamaã‚µãƒ¼ãƒãƒ¼ã®URL
    """
    try:
        # A simple GET request to the Ollama base URL should be enough
        # to verify that the server is running and reachable.
        response = requests.get(settings.OLLAMA_API_URL, timeout=settings.HEALTHCHECK_TIMEOUT)
        response.raise_for_status()
        
        # If the request is successful, the service is considered healthy.
        return {
            "status": "ok",
            "message": "Ollama server is reachable.",
            "url": settings.OLLAMA_API_URL
        }
    except requests.exceptions.RequestException as e:
        # If the request fails for any reason (timeout, connection error, etc.)
        return JSONResponse(
            status_code=503,  # Service Unavailable
            content={
                "status": "error",
                "message": "Ollama server is unreachable.",
                "url": settings.OLLAMA_API_URL,
                "error_details": str(e)
            }
        )


@app.post(
    "/chat",
    summary="ãƒãƒ£ãƒƒãƒˆ - é€šå¸¸ãƒ¬ã‚¹ãƒãƒ³ã‚¹",
    description="LLMã¨ã®å¯¾è©±ã‚’è¡Œã„ã€å®Œå…¨ãªãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ä¸€æ‹¬ã§è¿”ã—ã¾ã™",
    response_model=ChatResponse,
    responses={
        200: {"description": "ãƒãƒ£ãƒƒãƒˆæˆåŠŸ", "model": ChatResponse},
        400: {"description": "ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼", "model": ErrorResponse},
        500: {"description": "ã‚µãƒ¼ãƒãƒ¼ã‚¨ãƒ©ãƒ¼", "model": ErrorResponse}
    },
    tags=["chat"]
)
async def chat(request: ChatRequest):
    """
    ### é€šå¸¸ãƒãƒ£ãƒƒãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆéã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰
    
    æŒ‡å®šã•ã‚ŒãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«å¯¾ã—ã¦LLMã‹ã‚‰å®Œå…¨ãªå›ç­”ã‚’å–å¾—ã—ã¾ã™ã€‚
    ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§ã¯ãªãã€å®Œæˆã—ãŸå›ç­”ã‚’ä¸€æ‹¬ã§è¿”ã—ã¾ã™ã€‚
    
    **ä¸»ãªæ©Ÿèƒ½:**
    - è¤‡æ•°LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®è‡ªå‹•é¸æŠã¾ãŸã¯æ‰‹å‹•æŒ‡å®š
    - ä¼šè©±ã®ç¶™ç¶šï¼ˆconversation_idæŒ‡å®šï¼‰
    - ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã®æ´»ç”¨
    - ã‚¿ã‚¹ã‚¯ã®è‡ªå‹•åˆ†æã¨æœ€é©ãªãƒ¯ãƒ¼ã‚«ãƒ¼é¸æŠ
    
    **åˆ©ç”¨ä¾‹:**
    ```bash
    curl -X POST http://localhost:8000/chat \\
      -H "Content-Type: application/json" \\
      -d '{
        "message": "Shopifyã®å£²ä¸Šãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¦ãã ã•ã„",
        "worker_type": "claude",
        "context": {"project_id": "ecommerce_analysis"}
      }'
    ```
    
    **ãƒ¬ã‚¹ãƒãƒ³ã‚¹ä¾‹:**
    ```json
    {
        "success": true,
        "response": "Shopifyã®å£²ä¸Šãƒ‡ãƒ¼ã‚¿åˆ†æã‚’é–‹å§‹ã—ã¾ã™...",
        "conversation_id": "conv_12345-abcde",
        "worker_type": "claude",
        "task_analysis": {
            "task_type": "data_analysis",
            "complexity": "medium",
            "estimated_duration": 30
        }
    }
    ```
    
    **ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¿ã‚¤ãƒ—:**
    - `claude`: é«˜åº¦ãªåˆ†æã€è¤‡é›‘ãªæ¨è«–
    - `openai`: æ±ç”¨çš„ãªå¯¾è©±ã€ã‚³ãƒ¼ãƒ‰ç”Ÿæˆ
    - `local_llm`: ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆå‡¦ç†ï¼ˆOllamaï¼‰
    - æœªæŒ‡å®š: è‡ªå‹•é¸æŠï¼ˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å†…å®¹ã«åŸºã¥ãï¼‰
    """
    try:
        fallback_info = None
        
        # worker_typeãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ç›´æ¥ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’ä½¿ç”¨
        if request.worker_type:
            try:
                # WorkerFactoryã‚’ä½¿ç”¨ã—ã¦ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’ä½œæˆ
                worker = WorkerFactory.create_worker(
                    worker_type=request.worker_type,
                    model_id=None  # model_idã¯GenerationRequestã§ã®ã¿ã‚µãƒãƒ¼ãƒˆ
                )
                
                # ãƒ¯ãƒ¼ã‚«ãƒ¼ã§ã‚¿ã‚¹ã‚¯ã‚’å‡¦ç†
                response = await worker.process_task(request.message)
                
                return JSONResponse({
                    "success": True,
                    "response": response,
                    "conversation_id": request.conversation_id,
                    "worker_type": request.worker_type,
                    "task_analysis": None  # ç›´æ¥ãƒ¯ãƒ¼ã‚«ãƒ¼å‘¼ã³å‡ºã—ã®å ´åˆã¯ã‚¿ã‚¹ã‚¯åˆ†æãªã—
                })
            except ValueError as e:
                # ä¸æ˜ãªworker_typeã®å ´åˆã¯Orchestratorã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                logger.warning(f"Unknown worker_type '{request.worker_type}', falling back to orchestrator: {e}")
                fallback_info = {
                    "used": True,
                    "requested_worker": request.worker_type,
                    "actual_worker": "orchestrator"
                }
        
        # worker_typeãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„ã€ã¾ãŸã¯ä¸æ˜ãªå ´åˆã¯æ—¢å­˜ã®Orchestratorãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨
        result = await orchestrator.process_user_request(
            request=request.message,
            user_id=request.user_id,
            context=request.context,
            conversation_id=request.conversation_id
        )
        
        response_data = {
            "success": True,
            "response": result.get('response'),
            "conversation_id": result.get('conversation_log', {}).get('conversation_id'),
            "task_analysis": result.get('task_analysis') or {}  # å¸¸ã«task_analysisã‚’å«ã‚ã‚‹
        }
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æƒ…å ±ã‚’è¿½åŠ ï¼ˆã‚‚ã—ã‚ã‚Œã°ï¼‰
        if fallback_info:
            response_data["fallback_info"] = fallback_info
        
        return JSONResponse(response_data)
        
    except Exception as e:
        logger.error(f"Chat error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ£ãƒƒãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆSSEï¼‰"""
    conversation_id = request.conversation_id or f"conv_{uuid.uuid4()}"
    formatter = ResponseFormatter()
    message_processor = MessageProcessor()
    
    async def generate():
        """SSEã‚¤ãƒ™ãƒ³ãƒˆã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼"""
        try:
            # æœ€åˆã®ã‚¤ãƒ™ãƒ³ãƒˆ - ä¼šè©±é–‹å§‹
            yield f"data: {json.dumps({'type': 'start', 'conversation_id': conversation_id})}\n\n"
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”¨ã®ã‚­ãƒ¥ãƒ¼
            stream_queue = asyncio.Queue()
            
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ—ãƒ­ã‚»ãƒƒã‚µã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
            async def process_callback(message: dict):
                await stream_queue.put(('message', message))
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒ ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
            async def stream_handler(chunk: str):
                try:
                    # JSONã‚¤ãƒ™ãƒ³ãƒˆã®å ´åˆã¯ãƒ‘ãƒ¼ã‚¹
                    if chunk.strip().startswith('{'):
                        event = json.loads(chunk.strip())
                        await stream_queue.put(('message', event))
                    else:
                        # ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã®å ´åˆ
                        await stream_queue.put(('chunk', chunk))
                except json.JSONDecodeError:
                    # JSONã§ãªã„å ´åˆã¯é€šå¸¸ã®ãƒãƒ£ãƒ³ã‚¯ã¨ã—ã¦æ‰±ã†
                    await stream_queue.put(('chunk', chunk))
            
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†ã®ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
            message_lower = request.message.lower()
            
            # worker_typeãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ç›´æ¥ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’ä½¿ç”¨
            if request.worker_type:
                try:
                    # WorkerFactoryã‚’ä½¿ç”¨ã—ã¦ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’ä½œæˆ
                    worker = WorkerFactory.create_worker(
                        worker_type=request.worker_type,
                        model_id=None
                    )
                    
                    await process_callback(formatter.create_thinking_message(f"{request.worker_type}ãƒ¯ãƒ¼ã‚«ãƒ¼ã§å‡¦ç†ä¸­..."))
                    
                    # ãƒ¯ãƒ¼ã‚«ãƒ¼ã§ã‚¿ã‚¹ã‚¯ã‚’å‡¦ç†ï¼ˆéåŒæœŸï¼‰
                    async def worker_process():
                        response = await worker.process_task(request.message)
                        return {
                            'response': response,
                            'worker_type': request.worker_type
                        }
                    
                    process_task = asyncio.create_task(worker_process())
                    
                except ValueError as e:
                    # ä¸æ˜ãªworker_typeã®å ´åˆã¯Orchestratorã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    logger.warning(f"Unknown worker_type '{request.worker_type}', falling back to orchestrator: {e}")
                    request.worker_type = None  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®ãŸã‚ãƒªã‚»ãƒƒãƒˆ
            
            # worker_typeãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„ã€ã¾ãŸã¯ç„¡åŠ¹ãªå ´åˆã¯æ—¢å­˜ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨
            if not request.worker_type:
                # å‡¦ç†å‰ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹é€ä¿¡
                if any(kw in message_lower for kw in ['æ€ã„å‡ºã—ã¦', 'è¨˜æ†¶', 'ãƒ¡ãƒ¢ãƒª']):
                    await process_callback(formatter.create_thinking_message("ãƒ¡ãƒ¢ãƒªã‚’æ¤œç´¢ä¸­..."))
                    await process_callback(formatter.create_response_message(
                        "ãƒ¡ãƒ¢ãƒªã‚’æ¤œç´¢ã—ã¾ã™ã€‚å°‘ã€…ãŠå¾…ã¡ãã ã•ã„...",
                        intermediate=True
                    ))
                    await process_callback(formatter.create_tool_message("OpenMemory", "search"))
                elif any(kw in message_lower for kw in ['å®Ÿè£…', 'ã‚³ãƒ¼ãƒ‰', 'ä½œæˆ']):
                    await process_callback(formatter.create_thinking_message("ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã®æº–å‚™ä¸­..."))
                else:
                    await process_callback(formatter.create_thinking_message("ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’åˆ†æä¸­..."))
                
                # Orchestratorã§ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†ï¼ˆéåŒæœŸï¼‰
                logger.info(f"Starting orchestrator processing for conversation {conversation_id}")
                process_task = asyncio.create_task(
                    orchestrator.process_user_request(
                        request=request.message,
                        user_id=request.user_id,
                        context=request.context,
                        conversation_id=conversation_id,
                        stream_handler=stream_handler
                    )
                )
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†
            while True:
                try:
                    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§ã‚­ãƒ¥ãƒ¼ã‹ã‚‰å–å¾—
                    event_type, data = await asyncio.wait_for(
                        stream_queue.get(), 
                        timeout=0.1
                    )
                    
                    if event_type == 'chunk':
                        # ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã‚’SSEã‚¤ãƒ™ãƒ³ãƒˆã¨ã—ã¦é€ä¿¡
                        yield f"data: {json.dumps({'type': 'chunk', 'content': data})}\n\n"
                    elif event_type == 'message':
                        # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¸ˆã¿ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡
                        yield f"data: {json.dumps(data)}\n\n"
                    
                except asyncio.TimeoutError:
                    # ã‚¿ã‚¹ã‚¯ãŒå®Œäº†ã—ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    if process_task.done():
                        break
                    continue
            
            # æœ€çµ‚çµæœã‚’å–å¾—
            logger.info(f"Waiting for orchestrator result for conversation {conversation_id}")
            result = await process_task
            logger.info(f"Orchestrator result received: {list(result.keys()) if result else 'None'}")
            
            # æœ€çµ‚å¿œç­”ã‚’æ•´å½¢ã—ã¦é€ä¿¡
            if result:
                # resultã¾ãŸã¯responseã‚­ãƒ¼ã‹ã‚‰å†…å®¹ã‚’å–å¾—
                content = result.get('response') or result.get('result') or result.get('summary', '')
                
                if content:
                    logger.info(f"Sending final response: {content[:100]}...")
                    final_message = formatter.create_response_message(
                        content,
                        intermediate=False
                    )
                    yield f"data: {json.dumps(final_message)}\n\n"
                else:
                    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
                    logger.warning(f"No content found in result: {result.keys()}")
                    error_msg = formatter.create_error_message(
                        "å¿œç­”ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
                    )
                    yield f"data: {json.dumps(error_msg)}\n\n"
            
            # çµ‚äº†ã‚¤ãƒ™ãƒ³ãƒˆã®ã¿é€ä¿¡ï¼ˆcompleteã¯ä¸è¦ï¼‰
            yield f"data: {json.dumps({'type': 'end'})}\n\n"
            
        except Exception as e:
            logger.error(f"Stream error: {e}")
            error_message = formatter.create_error_message(
                "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
                str(e) if app.debug else None
            )
            yield f"data: {json.dumps(error_message)}\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # Nginxãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ç„¡åŠ¹åŒ–
        }
    )


@app.get("/conversations")
async def get_conversations():
    """å…¨ä¼šè©±ãƒ­ã‚°ã‚’å–å¾—"""
    conversations = orchestrator.get_all_conversations()
    return {
        "conversations": conversations,
        "count": len(conversations)
    }


@app.get("/conversations/{conversation_id}")
async def get_conversation(conversation_id: str):
    """ç‰¹å®šã®ä¼šè©±ãƒ­ã‚°ã‚’å–å¾—"""
    conversation = orchestrator.get_conversation_log(conversation_id)
    if not conversation:
        raise HTTPException(status_code=404, detail="Conversation not found")
    return conversation


@app.post("/debug/conversation")
async def debug_conversation(request: ConversationDebugRequest):
    """ä¼šè©±ã®ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’å–å¾—"""
    conversation = orchestrator.get_conversation_log(request.conversation_id)
    if not conversation:
        raise HTTPException(status_code=404, detail="Conversation not found")
    
    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’æ•´å½¢
    debug_info = {
        "conversation_id": conversation['conversation_id'],
        "message_count": len(conversation['messages']),
        "llm_response_count": len(conversation['llm_responses']),
        "mcp_connection_count": len(conversation['mcp_connections']),
        "total_tokens": conversation['total_tokens'],
        "duration": None,
        "messages": conversation['messages'],
        "llm_responses": [
            {
                "id": resp['id'],
                "provider": resp['provider'],
                "model": resp['model'],
                "tokens": resp['tokens'],
                "duration": resp['duration'],
                "timestamp": resp['timestamp']
            }
            for resp in conversation['llm_responses']
        ],
        "mcp_connections": [
            {
                "id": conn['id'],
                "service": conn['service'],
                "action": conn['action'],
                "success": conn['success'],
                "duration": conn['duration'],
                "timestamp": conn['timestamp'],
                "error": conn.get('error')
            }
            for conn in conversation['mcp_connections']
        ]
    }
    
    # ä¼šè©±æ™‚é–“ã‚’è¨ˆç®—
    if conversation['start_time'] and conversation['end_time']:
        start = datetime.fromisoformat(conversation['start_time'].replace('Z', '+00:00'))
        end = datetime.fromisoformat(conversation['end_time'].replace('Z', '+00:00'))
        debug_info['duration'] = (end - start).total_seconds()
    
    return debug_info


@app.get("/test/claude")
async def test_claude():
    """Claude APIæ¥ç¶šãƒ†ã‚¹ãƒˆ"""
    try:
        # ç°¡å˜ãªãƒ†ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        result = await orchestrator.process_user_request(
            request="ã“ã‚“ã«ã¡ã¯",
            user_id="test_user"
        )
        return {
            "success": True,
            "response": result.get('response'),
            "api_status": "connected"
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "api_status": "error"
        }


@app.post("/generate", response_model=Dict[str, Any])
@limiter.limit("30/minute")
async def generate(request: Request, generation_request: GenerationRequest):
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¿ã‚¤ãƒ—ã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚

    Args:
        request (Request): FastAPIã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™ç”¨ï¼‰ã€‚
        generation_request (GenerationRequest): ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¿ã‚¤ãƒ—ã€ãƒ¢ãƒ‡ãƒ«IDã‚’å«ã‚€ãƒªã‚¯ã‚¨ã‚¹ãƒˆã€‚

    Returns:
        Dict[str, Any]: ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã€‚

    Raises:
        HTTPException(400): ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¿ã‚¤ãƒ—ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã«ç™ºç”Ÿã—ã¾ã™ã€‚
        HTTPException(500): ãƒ¯ãƒ¼ã‚«ãƒ¼ã®å‡¦ç†ä¸­ã«å†…éƒ¨ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã«ç™ºç”Ÿã—ã¾ã™ã€‚
    """
    try:
        # WorkerFactoryã‚’ä½¿ç”¨ã—ã¦ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’ä½œæˆ
        worker = WorkerFactory.create_worker(
            worker_type=generation_request.worker_type, 
            model_id=generation_request.model_id
        )
    except ValueError as e:
        # WorkerFactoryãŒä¸æ˜ãªworker_typeã«å¯¾ã—ã¦ValueErrorã‚’ç™ºç”Ÿã•ã›ã‚‹ã“ã¨ã‚’æƒ³å®š
        raise HTTPException(status_code=400, detail=str(e))

    try:
        # ãƒ¯ãƒ¼ã‚«ãƒ¼ã§ã‚¿ã‚¹ã‚¯ã‚’å‡¦ç†
        # BaseWorkerã®process_taskãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã™
        response = await worker.process_task(generation_request.prompt)
        
        return {
            "success": True,
            "response": response,
            "worker_type": generation_request.worker_type,
            "model_id": generation_request.model_id
        }
    except Exception as e:
        # ãƒ¯ãƒ¼ã‚«ãƒ¼å®Ÿè¡Œä¸­ã®ä¸€èˆ¬çš„ãªã‚¨ãƒ©ãƒ¼ã‚’æ•æ‰
        logger.error(f"Worker execution error: {e}")
        raise HTTPException(status_code=500, detail=f"Error processing task: {e}")


@app.get("/workers/types")
async def get_worker_types():
    """
    ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¿ã‚¤ãƒ—ã®ä¸€è¦§ã‚’å–å¾—
    """
    return {
        "success": True,
        "worker_types": WorkerFactory.get_supported_worker_types(),
        "description": {
            "anthropic": "Claude AI by Anthropic",
            "claude": "Claude AI by Anthropic (alias)",
            "openai": "OpenAI GPT models",
            "local_llm": "Local LLM models (Ollama, etc.)"
        }
    }


@app.get("/workers")
async def get_workers():
    """
    åˆ©ç”¨å¯èƒ½ãªãƒ¯ãƒ¼ã‚«ãƒ¼ä¸€è¦§ã‚’å–å¾—ã™ã‚‹API
    
    Returns:
        dict: ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¿ã‚¤ãƒ—ã®ãƒªã‚¹ãƒˆã‚’å«ã‚€ãƒ¬ã‚¹ãƒãƒ³ã‚¹
    """
    return {
        "workers": WorkerFactory.list_worker_types()
    }


@app.get("/workers/{worker_type}/models")
async def get_worker_models(worker_type: str):
    """
    ç‰¹å®šãƒ¯ãƒ¼ã‚«ãƒ¼ã®åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—ã™ã‚‹API
    
    Args:
        worker_type: ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¿ã‚¤ãƒ— ('anthropic', 'openai', 'local_llm' ãªã©)
    
    Returns:
        dict: ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¿ã‚¤ãƒ—ã¨ãƒ¢ãƒ‡ãƒ«ãƒªã‚¹ãƒˆã‚’å«ã‚€ãƒ¬ã‚¹ãƒãƒ³ã‚¹
    
    Raises:
        HTTPException: ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¿ã‚¤ãƒ—ã®å ´åˆ
    """
    try:
        models = await WorkerFactory.get_available_models(worker_type)
        return {
            "worker_type": worker_type,
            "models": models
        }
    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))
    except Exception as e:
        logger.error(f"Error getting models for {worker_type}: {str(e)}")
        raise HTTPException(status_code=500, detail="Failed to retrieve models")


# ========== Analytics Endpoints ==========

@app.post("/api/analytics/analyze")
async def analyze_data(request: AnalysisRequest):
    """ãƒ‡ãƒ¼ã‚¿åˆ†æã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    try:
        if request.analysis_type == "conversation_patterns":
            # ä¼šè©±å±¥æ­´ã‚’å–å¾—
            conversations = orchestrator.get_all_conversations()
            result = await data_analyzer.analyze_conversation_patterns(conversations)
            
        elif request.analysis_type == "task_performance":
            # ã‚¿ã‚¹ã‚¯å±¥æ­´ã‚’å–å¾—ï¼ˆå®Ÿè£…ã«å¿œã˜ã¦èª¿æ•´ï¼‰
            tasks = request.data.get('tasks', []) if request.data else []
            result = await data_analyzer.analyze_task_performance(tasks)
            
        elif request.analysis_type == "resource_prediction":
            # ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨å±¥æ­´ã‚’å–å¾—
            historical_data = request.data.get('historical_data', []) if request.data else []
            result = await data_analyzer.predict_resource_needs(historical_data)
            
        else:
            raise HTTPException(status_code=400, detail="Invalid analysis type")
        
        return {
            "success": True,
            "analysis_type": request.analysis_type,
            "result": result
        }
        
    except Exception as e:
        logger.error(f"Analytics error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/analytics/conversations")
async def get_conversation_analytics():
    """ä¼šè©±ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã®å–å¾—"""
    try:
        conversations = orchestrator.get_all_conversations()
        result = await data_analyzer.analyze_conversation_patterns(conversations)
        
        return {
            "success": True,
            "total_conversations": len(conversations),
            "analysis": result
        }
        
    except Exception as e:
        logger.error(f"Conversation analytics error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/analytics/tasks")
async def get_task_analytics():
    """ã‚¿ã‚¹ã‚¯ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æã®å–å¾—"""
    try:
        # ã‚¿ã‚¹ã‚¯å±¥æ­´ã‚’ä¼šè©±ãƒ­ã‚°ã‹ã‚‰æŠ½å‡º
        conversations = orchestrator.get_all_conversations()
        tasks = []
        
        for conv in conversations:
            if conv.get('task_analysis'):
                tasks.append({
                    'conversation_id': conv['conversation_id'],
                    'task': conv['task_analysis'],
                    'duration': conv.get('duration'),
                    'success': conv.get('success', True),
                    'timestamp': conv['start_time']
                })
        
        result = await data_analyzer.analyze_task_performance(tasks)
        
        return {
            "success": True,
            "total_tasks": len(tasks),
            "analysis": result
        }
        
    except Exception as e:
        logger.error(f"Task analytics error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ========== Automation Endpoints ==========

@app.post("/api/automation/rules")
async def create_automation_rule(request: AutomationRuleRequest):
    """è‡ªå‹•åŒ–ãƒ«ãƒ¼ãƒ«ã®ä½œæˆ"""
    try:
        rule = {
            "id": f"rule_{uuid.uuid4()}",
            "name": request.name,
            "description": request.description,
            "trigger": {
                "type": request.trigger_type,
                "config": request.trigger_config
            },
            "actions": request.actions,
            "active": request.active,
            "created_at": datetime.now().isoformat()
        }
        
        # AutomationRuleã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
        from orchestrator.automation.task_automator import AutomationRule, AutomationTrigger
        
        automation_rule = AutomationRule(
            id=rule["id"],
            name=rule["name"],
            description=rule["description"],
            trigger_type=AutomationTrigger(rule["trigger"]["type"]),
            trigger_config=rule["trigger"]["config"],
            actions=rule["actions"],
            enabled=rule["active"],
            created_at=datetime.now()
        )
        
        success = task_automator.add_rule(automation_rule)
        
        if success:
            return {
                "success": True,
                "rule_id": rule["id"],
                "message": "è‡ªå‹•åŒ–ãƒ«ãƒ¼ãƒ«ãŒä½œæˆã•ã‚Œã¾ã—ãŸ"
            }
        else:
            raise HTTPException(status_code=400, detail="ãƒ«ãƒ¼ãƒ«ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
            
    except Exception as e:
        logger.error(f"Automation rule creation error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/automation/rules")
async def get_automation_rules():
    """è‡ªå‹•åŒ–ãƒ«ãƒ¼ãƒ«ä¸€è¦§ã®å–å¾—"""
    try:
        rules = task_automator.list_rules()
        return {
            "success": True,
            "rules": rules,
            "count": len(rules)
        }
        
    except Exception as e:
        logger.error(f"Get automation rules error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/api/automation/rules/{rule_id}")
async def delete_automation_rule(rule_id: str):
    """è‡ªå‹•åŒ–ãƒ«ãƒ¼ãƒ«ã®å‰Šé™¤"""
    try:
        success = task_automator.remove_rule(rule_id)
        
        if success:
            return {
                "success": True,
                "message": f"ãƒ«ãƒ¼ãƒ« {rule_id} ãŒå‰Šé™¤ã•ã‚Œã¾ã—ãŸ"
            }
        else:
            raise HTTPException(status_code=404, detail="ãƒ«ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
    except Exception as e:
        logger.error(f"Delete automation rule error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/automation/execute/{rule_id}")
async def execute_automation_rule(rule_id: str):
    """è‡ªå‹•åŒ–ãƒ«ãƒ¼ãƒ«ã®æ‰‹å‹•å®Ÿè¡Œ"""
    try:
        result = await task_automator.execute_rule(rule_id)
        
        return {
            "success": result.success,
            "executed_actions": result.executed_actions,
            "results": result.outputs,
            "timestamp": result.timestamp.isoformat() if result.timestamp else None,
            "error": result.error,
            "duration": result.duration
        }
        
    except Exception as e:
        logger.error(f"Execute automation rule error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/automation/suggest")
async def suggest_automations():
    """è‡ªå‹•åŒ–ã®ææ¡ˆã‚’å–å¾—"""
    try:
        # ã‚¿ã‚¹ã‚¯å±¥æ­´ã‚’å–å¾—
        conversations = orchestrator.get_all_conversations()
        task_history = []
        
        for conv in conversations:
            if conv.get('messages'):
                for msg in conv['messages']:
                    task_history.append({
                        'message': msg['content'],
                        'timestamp': msg['timestamp'],
                        'user_id': conv.get('user_id', 'default_user')
                    })
        
        suggestions = task_automator.suggest_automations(task_history)
        
        return {
            "success": True,
            "suggestions": suggestions,
            "count": len(suggestions)
        }
        
    except Exception as e:
        logger.error(f"Suggest automations error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", 9000))
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=port,
        log_level="info",
        access_log=True
    )