"""
MultiLLM API Server - SSEå¯¾å¿œã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°APIã‚µãƒ¼ãƒãƒ¼
"""

import asyncio
import json
import logging
import uuid
from datetime import datetime
from typing import Dict, Optional
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel
import sys
import os

# è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from orchestrator.orchestrator import MultiLLMOrchestrator
from orchestrator.response_formatter import ResponseFormatter, MessageProcessor

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
app = FastAPI(title="MultiLLM API", version="1.0.0")

# CORSè¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://localhost:3001", "http://localhost:3500", "*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


class ChatRequest(BaseModel):
    """ãƒãƒ£ãƒƒãƒˆãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    message: str
    conversation_id: Optional[str] = None
    user_id: str = "default_user"
    context: Optional[Dict] = None


class ConversationDebugRequest(BaseModel):
    """ä¼šè©±ãƒ‡ãƒãƒƒã‚°ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    conversation_id: str


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼
orchestrator = None


@app.on_event("startup")
async def startup_event():
    """ã‚µãƒ¼ãƒãƒ¼èµ·å‹•æ™‚ã®åˆæœŸåŒ–"""
    global orchestrator
    
    config = {
        "workers": {
            "backend_worker": {"model": "claude-3.5-sonnet"},
            "frontend_worker": {"model": "claude-3.5-sonnet"},
            "review_worker": {"model": "claude-3.5-sonnet"},
            "analytics_worker": {"model": "claude-3.5-sonnet"},
            "documentation_worker": {"model": "claude-3.5-sonnet"}
        },
        "memory": {
            "syncInterval": 300
        }
    }
    
    orchestrator = MultiLLMOrchestrator(config)
    await orchestrator.initialize()
    logger.info("âœ… MultiLLM API Server started")


@app.on_event("shutdown")
async def shutdown_event():
    """ã‚µãƒ¼ãƒãƒ¼çµ‚äº†æ™‚ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    global orchestrator
    if orchestrator:
        await orchestrator.shutdown()
    logger.info("ğŸ‘‹ MultiLLM API Server shutdown")


@app.get("/")
async def root():
    """ãƒ«ãƒ¼ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return {
        "service": "MultiLLM API",
        "version": "1.0.0",
        "status": "active"
    }


@app.get("/health")
async def health():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "orchestrator": orchestrator.get_status() if orchestrator else None
    }


@app.post("/chat")
async def chat(request: ChatRequest):
    """é€šå¸¸ã®ãƒãƒ£ãƒƒãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆéã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰"""
    try:
        result = await orchestrator.process_user_request(
            request=request.message,
            user_id=request.user_id,
            context=request.context,
            conversation_id=request.conversation_id
        )
        
        return JSONResponse({
            "success": True,
            "response": result.get('response'),
            "conversation_id": result.get('conversation_log', {}).get('conversation_id'),
            "task_analysis": result.get('task_analysis')
        })
        
    except Exception as e:
        logger.error(f"Chat error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ£ãƒƒãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆSSEï¼‰"""
    conversation_id = request.conversation_id or f"conv_{uuid.uuid4()}"
    formatter = ResponseFormatter()
    message_processor = MessageProcessor()
    
    async def generate():
        """SSEã‚¤ãƒ™ãƒ³ãƒˆã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼"""
        try:
            # æœ€åˆã®ã‚¤ãƒ™ãƒ³ãƒˆ - ä¼šè©±é–‹å§‹
            yield f"data: {json.dumps({'type': 'start', 'conversation_id': conversation_id})}\n\n"
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”¨ã®ã‚­ãƒ¥ãƒ¼
            stream_queue = asyncio.Queue()
            
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ—ãƒ­ã‚»ãƒƒã‚µã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
            async def process_callback(message: dict):
                await stream_queue.put(('message', message))
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒ ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
            async def stream_handler(chunk: str):
                await stream_queue.put(('chunk', chunk))
            
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†ã®ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
            message_lower = request.message.lower()
            
            # å‡¦ç†å‰ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹é€ä¿¡
            if any(kw in message_lower for kw in ['æ€ã„å‡ºã—ã¦', 'è¨˜æ†¶', 'ãƒ¡ãƒ¢ãƒª']):
                await process_callback(formatter.create_thinking_message("ãƒ¡ãƒ¢ãƒªã‚’æ¤œç´¢ä¸­..."))
                await process_callback(formatter.create_response_message(
                    "ãƒ¡ãƒ¢ãƒªã‚’æ¤œç´¢ã—ã¾ã™ã€‚å°‘ã€…ãŠå¾…ã¡ãã ã•ã„...",
                    intermediate=True
                ))
                await process_callback(formatter.create_tool_message("OpenMemory", "search"))
            elif any(kw in message_lower for kw in ['å®Ÿè£…', 'ã‚³ãƒ¼ãƒ‰', 'ä½œæˆ']):
                await process_callback(formatter.create_thinking_message("ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã®æº–å‚™ä¸­..."))
            else:
                await process_callback(formatter.create_thinking_message("ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’åˆ†æä¸­..."))
            
            # Orchestratorã§ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†ï¼ˆéåŒæœŸï¼‰
            process_task = asyncio.create_task(
                orchestrator.process_user_request(
                    request=request.message,
                    user_id=request.user_id,
                    context=request.context,
                    conversation_id=conversation_id,
                    stream_handler=stream_handler
                )
            )
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†
            while True:
                try:
                    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§ã‚­ãƒ¥ãƒ¼ã‹ã‚‰å–å¾—
                    event_type, data = await asyncio.wait_for(
                        stream_queue.get(), 
                        timeout=0.1
                    )
                    
                    if event_type == 'chunk':
                        # ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã‚’SSEã‚¤ãƒ™ãƒ³ãƒˆã¨ã—ã¦é€ä¿¡
                        yield f"data: {json.dumps({'type': 'chunk', 'content': data})}\n\n"
                    elif event_type == 'message':
                        # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¸ˆã¿ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡
                        yield f"data: {json.dumps(data)}\n\n"
                    
                except asyncio.TimeoutError:
                    # ã‚¿ã‚¹ã‚¯ãŒå®Œäº†ã—ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    if process_task.done():
                        break
                    continue
            
            # æœ€çµ‚çµæœã‚’å–å¾—
            result = await process_task
            
            # æœ€çµ‚å¿œç­”ã‚’æ•´å½¢ã—ã¦é€ä¿¡
            if result:
                # resultã¾ãŸã¯responseã‚­ãƒ¼ã‹ã‚‰å†…å®¹ã‚’å–å¾—
                content = result.get('response') or result.get('result') or result.get('summary', '')
                
                if content:
                    final_message = formatter.create_response_message(
                        content,
                        intermediate=False
                    )
                    yield f"data: {json.dumps(final_message)}\n\n"
                else:
                    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
                    logger.warning(f"No content found in result: {result.keys()}")
                    error_msg = formatter.create_error_message(
                        "å¿œç­”ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
                    )
                    yield f"data: {json.dumps(error_msg)}\n\n"
            
            # çµ‚äº†ã‚¤ãƒ™ãƒ³ãƒˆã®ã¿é€ä¿¡ï¼ˆcompleteã¯ä¸è¦ï¼‰
            yield f"data: {json.dumps({'type': 'end'})}\n\n"
            
        except Exception as e:
            logger.error(f"Stream error: {e}")
            error_message = formatter.create_error_message(
                "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
                str(e) if app.debug else None
            )
            yield f"data: {json.dumps(error_message)}\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # Nginxãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ç„¡åŠ¹åŒ–
        }
    )


@app.get("/conversations")
async def get_conversations():
    """å…¨ä¼šè©±ãƒ­ã‚°ã‚’å–å¾—"""
    conversations = orchestrator.get_all_conversations()
    return {
        "conversations": conversations,
        "count": len(conversations)
    }


@app.get("/conversations/{conversation_id}")
async def get_conversation(conversation_id: str):
    """ç‰¹å®šã®ä¼šè©±ãƒ­ã‚°ã‚’å–å¾—"""
    conversation = orchestrator.get_conversation_log(conversation_id)
    if not conversation:
        raise HTTPException(status_code=404, detail="Conversation not found")
    return conversation


@app.post("/debug/conversation")
async def debug_conversation(request: ConversationDebugRequest):
    """ä¼šè©±ã®ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’å–å¾—"""
    conversation = orchestrator.get_conversation_log(request.conversation_id)
    if not conversation:
        raise HTTPException(status_code=404, detail="Conversation not found")
    
    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’æ•´å½¢
    debug_info = {
        "conversation_id": conversation['conversation_id'],
        "message_count": len(conversation['messages']),
        "llm_response_count": len(conversation['llm_responses']),
        "mcp_connection_count": len(conversation['mcp_connections']),
        "total_tokens": conversation['total_tokens'],
        "duration": None,
        "messages": conversation['messages'],
        "llm_responses": [
            {
                "id": resp['id'],
                "provider": resp['provider'],
                "model": resp['model'],
                "tokens": resp['tokens'],
                "duration": resp['duration'],
                "timestamp": resp['timestamp']
            }
            for resp in conversation['llm_responses']
        ],
        "mcp_connections": [
            {
                "id": conn['id'],
                "service": conn['service'],
                "action": conn['action'],
                "success": conn['success'],
                "duration": conn['duration'],
                "timestamp": conn['timestamp'],
                "error": conn.get('error')
            }
            for conn in conversation['mcp_connections']
        ]
    }
    
    # ä¼šè©±æ™‚é–“ã‚’è¨ˆç®—
    if conversation['start_time'] and conversation['end_time']:
        start = datetime.fromisoformat(conversation['start_time'].replace('Z', '+00:00'))
        end = datetime.fromisoformat(conversation['end_time'].replace('Z', '+00:00'))
        debug_info['duration'] = (end - start).total_seconds()
    
    return debug_info


@app.get("/test/claude")
async def test_claude():
    """Claude APIæ¥ç¶šãƒ†ã‚¹ãƒˆ"""
    try:
        # ç°¡å˜ãªãƒ†ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        result = await orchestrator.process_user_request(
            request="ã“ã‚“ã«ã¡ã¯",
            user_id="test_user"
        )
        return {
            "success": True,
            "response": result.get('response'),
            "api_status": "connected"
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "api_status": "error"
        }


if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", 9000))
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=port,
        log_level="info",
        access_log=True
    )