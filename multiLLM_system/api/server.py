"""
MultiLLM API Server - SSEå¯¾å¿œã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°APIã‚µãƒ¼ãƒãƒ¼
"""

import asyncio
import json
import logging
import uuid
from datetime import datetime
from typing import Dict, Optional, List
from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse, JSONResponse
from pydantic import BaseModel, Field
import sys
import os
import requests

# è¦ªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from orchestrator.orchestrator import MultiLLMOrchestrator
from orchestrator.response_formatter import ResponseFormatter, MessageProcessor
from orchestrator.analyzers.data_analyzer import DataAnalyzer
from orchestrator.automation.task_automator import TaskAutomator
from orchestrator.worker_factory import WorkerFactory
from config.settings import settings

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# FastAPIã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
app = FastAPI(title="MultiLLM API", version="1.0.0")

# CORSè¨­å®š
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000", "http://localhost:3001", "http://localhost:3500", "*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


class ChatRequest(BaseModel):
    """ãƒãƒ£ãƒƒãƒˆãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    message: str
    conversation_id: Optional[str] = None
    user_id: str = "default_user"
    context: Optional[Dict] = None
    worker_type: Optional[str] = None  # æ–°è¦è¿½åŠ : ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¿ã‚¤ãƒ—ã®æŒ‡å®š (ä¾‹: 'openai', 'anthropic', 'local_llm')


class ConversationDebugRequest(BaseModel):
    """ä¼šè©±ãƒ‡ãƒãƒƒã‚°ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    conversation_id: str


class AnalysisRequest(BaseModel):
    """åˆ†æãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    analysis_type: str  # conversation_patterns, task_performance, resource_prediction
    data: Optional[Dict] = None
    time_range: Optional[Dict] = None  # {"start": "2024-01-01", "end": "2024-01-31"}


class AutomationRuleRequest(BaseModel):
    """è‡ªå‹•åŒ–ãƒ«ãƒ¼ãƒ«ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    name: str
    description: str
    trigger_type: str  # time_based, event_based, condition_based, pattern_based
    trigger_config: Dict
    actions: List[Dict]
    active: bool = True


class GenerationRequest(BaseModel):
    """ã‚·ãƒ³ãƒ—ãƒ«ãªç”Ÿæˆãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆãƒ¯ãƒ¼ã‚«ãƒ¼ç›´æ¥å‘¼ã³å‡ºã—ç”¨ï¼‰"""
    prompt: str
    worker_type: str = Field(default="openai", description="The type of worker to use (e.g., 'openai', 'anthropic', 'local_llm')")
    model_id: Optional[str] = None


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
orchestrator = None
data_analyzer = None
task_automator = None


@app.on_event("startup")
async def startup_event():
    """ã‚µãƒ¼ãƒãƒ¼èµ·å‹•æ™‚ã®åˆæœŸåŒ–"""
    global orchestrator, data_analyzer, task_automator
    
    config = {
        "workers": {
            "backend_worker": {"model": "claude-3.5-sonnet"},
            "frontend_worker": {"model": "claude-3.5-sonnet"},
            "review_worker": {"model": "claude-3.5-sonnet"},
            "analytics_worker": {"model": "claude-3.5-sonnet"},
            "documentation_worker": {"model": "claude-3.5-sonnet"},
            "mcp_worker": {"model": "claude-3.5-sonnet"}
        },
        "memory": {
            "syncInterval": 300
        }
    }
    
    orchestrator = MultiLLMOrchestrator(config)
    await orchestrator.initialize()
    
    # åˆ†æã‚¨ãƒ³ã‚¸ãƒ³ã¨è‡ªå‹•åŒ–ã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆæœŸåŒ–
    data_analyzer = DataAnalyzer()
    task_automator = TaskAutomator(orchestrator)
    
    logger.info("âœ… MultiLLM API Server started with Advanced Analytics")


@app.on_event("shutdown")
async def shutdown_event():
    """ã‚µãƒ¼ãƒãƒ¼çµ‚äº†æ™‚ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    global orchestrator
    if orchestrator:
        await orchestrator.shutdown()
    logger.info("ğŸ‘‹ MultiLLM API Server shutdown")


@app.get("/")
async def root():
    """ãƒ«ãƒ¼ãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    return {
        "service": "MultiLLM API",
        "version": "1.0.0",
        "status": "active"
    }


@app.get("/health")
async def health():
    """ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "orchestrator": orchestrator.get_status() if orchestrator else None
    }


@app.get("/health/ollama", tags=["Health Checks"])
async def health_check_ollama():
    """
    Ollamaã‚µãƒ¼ãƒãƒ¼ã®ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã¨æ¥ç¶šæ€§ã‚’ç¢ºèªã—ã¾ã™ã€‚

    Returns:
        dict: ã‚µãƒ¼ãƒãƒ¼ã®çŠ¶æ…‹ã‚’ç¤ºã™JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹
            - status: "ok" (æ­£å¸¸) ã¾ãŸã¯ "error" (ç•°å¸¸)
            - message: çŠ¶æ…‹ã®èª¬æ˜
            - url: ãƒã‚§ãƒƒã‚¯ã—ãŸOllamaã‚µãƒ¼ãƒãƒ¼ã®URL
    """
    try:
        # A simple GET request to the Ollama base URL should be enough
        # to verify that the server is running and reachable.
        response = requests.get(settings.OLLAMA_API_URL, timeout=settings.HEALTHCHECK_TIMEOUT)
        response.raise_for_status()
        
        # If the request is successful, the service is considered healthy.
        return {
            "status": "ok",
            "message": "Ollama server is reachable.",
            "url": settings.OLLAMA_API_URL
        }
    except requests.exceptions.RequestException as e:
        # If the request fails for any reason (timeout, connection error, etc.)
        return JSONResponse(
            status_code=503,  # Service Unavailable
            content={
                "status": "error",
                "message": "Ollama server is unreachable.",
                "url": settings.OLLAMA_API_URL,
                "error_details": str(e)
            }
        )


@app.post("/chat")
async def chat(request: ChatRequest):
    """é€šå¸¸ã®ãƒãƒ£ãƒƒãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆéã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰"""
    try:
        # worker_typeãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ç›´æ¥ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’ä½¿ç”¨
        if request.worker_type:
            try:
                # WorkerFactoryã‚’ä½¿ç”¨ã—ã¦ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’ä½œæˆ
                worker = WorkerFactory.create_worker(
                    worker_type=request.worker_type,
                    model_id=None  # model_idã¯GenerationRequestã§ã®ã¿ã‚µãƒãƒ¼ãƒˆ
                )
                
                # ãƒ¯ãƒ¼ã‚«ãƒ¼ã§ã‚¿ã‚¹ã‚¯ã‚’å‡¦ç†
                response = await worker.process_task(request.message)
                
                return JSONResponse({
                    "success": True,
                    "response": response,
                    "conversation_id": request.conversation_id,
                    "worker_type": request.worker_type
                })
            except ValueError as e:
                # ä¸æ˜ãªworker_typeã®å ´åˆã¯Orchestratorã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                logger.warning(f"Unknown worker_type '{request.worker_type}', falling back to orchestrator: {e}")
        
        # worker_typeãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„ã€ã¾ãŸã¯ä¸æ˜ãªå ´åˆã¯æ—¢å­˜ã®Orchestratorãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨
        result = await orchestrator.process_user_request(
            request=request.message,
            user_id=request.user_id,
            context=request.context,
            conversation_id=request.conversation_id
        )
        
        return JSONResponse({
            "success": True,
            "response": result.get('response'),
            "conversation_id": result.get('conversation_log', {}).get('conversation_id'),
            "task_analysis": result.get('task_analysis')
        })
        
    except Exception as e:
        logger.error(f"Chat error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/chat/stream")
async def chat_stream(request: ChatRequest):
    """ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ãƒãƒ£ãƒƒãƒˆã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆSSEï¼‰"""
    conversation_id = request.conversation_id or f"conv_{uuid.uuid4()}"
    formatter = ResponseFormatter()
    message_processor = MessageProcessor()
    
    async def generate():
        """SSEã‚¤ãƒ™ãƒ³ãƒˆã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚¿ãƒ¼"""
        try:
            # æœ€åˆã®ã‚¤ãƒ™ãƒ³ãƒˆ - ä¼šè©±é–‹å§‹
            yield f"data: {json.dumps({'type': 'start', 'conversation_id': conversation_id})}\n\n"
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ç”¨ã®ã‚­ãƒ¥ãƒ¼
            stream_queue = asyncio.Queue()
            
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ—ãƒ­ã‚»ãƒƒã‚µã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
            async def process_callback(message: dict):
                await stream_queue.put(('message', message))
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒ ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
            async def stream_handler(chunk: str):
                try:
                    # JSONã‚¤ãƒ™ãƒ³ãƒˆã®å ´åˆã¯ãƒ‘ãƒ¼ã‚¹
                    if chunk.strip().startswith('{'):
                        event = json.loads(chunk.strip())
                        await stream_queue.put(('message', event))
                    else:
                        # ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã®å ´åˆ
                        await stream_queue.put(('chunk', chunk))
                except json.JSONDecodeError:
                    # JSONã§ãªã„å ´åˆã¯é€šå¸¸ã®ãƒãƒ£ãƒ³ã‚¯ã¨ã—ã¦æ‰±ã†
                    await stream_queue.put(('chunk', chunk))
            
            # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†ã®ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã‚’åˆ¤å®š
            message_lower = request.message.lower()
            
            # worker_typeãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€ç›´æ¥ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’ä½¿ç”¨
            if request.worker_type:
                try:
                    # WorkerFactoryã‚’ä½¿ç”¨ã—ã¦ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’ä½œæˆ
                    worker = WorkerFactory.create_worker(
                        worker_type=request.worker_type,
                        model_id=None
                    )
                    
                    await process_callback(formatter.create_thinking_message(f"{request.worker_type}ãƒ¯ãƒ¼ã‚«ãƒ¼ã§å‡¦ç†ä¸­..."))
                    
                    # ãƒ¯ãƒ¼ã‚«ãƒ¼ã§ã‚¿ã‚¹ã‚¯ã‚’å‡¦ç†ï¼ˆéåŒæœŸï¼‰
                    async def worker_process():
                        response = await worker.process_task(request.message)
                        return {
                            'response': response,
                            'worker_type': request.worker_type
                        }
                    
                    process_task = asyncio.create_task(worker_process())
                    
                except ValueError as e:
                    # ä¸æ˜ãªworker_typeã®å ´åˆã¯Orchestratorã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    logger.warning(f"Unknown worker_type '{request.worker_type}', falling back to orchestrator: {e}")
                    request.worker_type = None  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®ãŸã‚ãƒªã‚»ãƒƒãƒˆ
            
            # worker_typeãŒæŒ‡å®šã•ã‚Œã¦ã„ãªã„ã€ã¾ãŸã¯ç„¡åŠ¹ãªå ´åˆã¯æ—¢å­˜ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨
            if not request.worker_type:
                # å‡¦ç†å‰ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹é€ä¿¡
                if any(kw in message_lower for kw in ['æ€ã„å‡ºã—ã¦', 'è¨˜æ†¶', 'ãƒ¡ãƒ¢ãƒª']):
                    await process_callback(formatter.create_thinking_message("ãƒ¡ãƒ¢ãƒªã‚’æ¤œç´¢ä¸­..."))
                    await process_callback(formatter.create_response_message(
                        "ãƒ¡ãƒ¢ãƒªã‚’æ¤œç´¢ã—ã¾ã™ã€‚å°‘ã€…ãŠå¾…ã¡ãã ã•ã„...",
                        intermediate=True
                    ))
                    await process_callback(formatter.create_tool_message("OpenMemory", "search"))
                elif any(kw in message_lower for kw in ['å®Ÿè£…', 'ã‚³ãƒ¼ãƒ‰', 'ä½œæˆ']):
                    await process_callback(formatter.create_thinking_message("ã‚³ãƒ¼ãƒ‰ç”Ÿæˆã®æº–å‚™ä¸­..."))
                else:
                    await process_callback(formatter.create_thinking_message("ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’åˆ†æä¸­..."))
                
                # Orchestratorã§ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†ï¼ˆéåŒæœŸï¼‰
                logger.info(f"Starting orchestrator processing for conversation {conversation_id}")
                process_task = asyncio.create_task(
                    orchestrator.process_user_request(
                        request=request.message,
                        user_id=request.user_id,
                        context=request.context,
                        conversation_id=conversation_id,
                        stream_handler=stream_handler
                    )
                )
            
            # ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†
            while True:
                try:
                    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§ã‚­ãƒ¥ãƒ¼ã‹ã‚‰å–å¾—
                    event_type, data = await asyncio.wait_for(
                        stream_queue.get(), 
                        timeout=0.1
                    )
                    
                    if event_type == 'chunk':
                        # ãƒ†ã‚­ã‚¹ãƒˆãƒãƒ£ãƒ³ã‚¯ã‚’SSEã‚¤ãƒ™ãƒ³ãƒˆã¨ã—ã¦é€ä¿¡
                        yield f"data: {json.dumps({'type': 'chunk', 'content': data})}\n\n"
                    elif event_type == 'message':
                        # ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆæ¸ˆã¿ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡
                        yield f"data: {json.dumps(data)}\n\n"
                    
                except asyncio.TimeoutError:
                    # ã‚¿ã‚¹ã‚¯ãŒå®Œäº†ã—ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                    if process_task.done():
                        break
                    continue
            
            # æœ€çµ‚çµæœã‚’å–å¾—
            logger.info(f"Waiting for orchestrator result for conversation {conversation_id}")
            result = await process_task
            logger.info(f"Orchestrator result received: {list(result.keys()) if result else 'None'}")
            
            # æœ€çµ‚å¿œç­”ã‚’æ•´å½¢ã—ã¦é€ä¿¡
            if result:
                # resultã¾ãŸã¯responseã‚­ãƒ¼ã‹ã‚‰å†…å®¹ã‚’å–å¾—
                content = result.get('response') or result.get('result') or result.get('summary', '')
                
                if content:
                    logger.info(f"Sending final response: {content[:100]}...")
                    final_message = formatter.create_response_message(
                        content,
                        intermediate=False
                    )
                    yield f"data: {json.dumps(final_message)}\n\n"
                else:
                    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã®ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°
                    logger.warning(f"No content found in result: {result.keys()}")
                    error_msg = formatter.create_error_message(
                        "å¿œç­”ã®ç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚"
                    )
                    yield f"data: {json.dumps(error_msg)}\n\n"
            
            # çµ‚äº†ã‚¤ãƒ™ãƒ³ãƒˆã®ã¿é€ä¿¡ï¼ˆcompleteã¯ä¸è¦ï¼‰
            yield f"data: {json.dumps({'type': 'end'})}\n\n"
            
        except Exception as e:
            logger.error(f"Stream error: {e}")
            error_message = formatter.create_error_message(
                "ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚",
                str(e) if app.debug else None
            )
            yield f"data: {json.dumps(error_message)}\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no",  # Nginxãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ç„¡åŠ¹åŒ–
        }
    )


@app.get("/conversations")
async def get_conversations():
    """å…¨ä¼šè©±ãƒ­ã‚°ã‚’å–å¾—"""
    conversations = orchestrator.get_all_conversations()
    return {
        "conversations": conversations,
        "count": len(conversations)
    }


@app.get("/conversations/{conversation_id}")
async def get_conversation(conversation_id: str):
    """ç‰¹å®šã®ä¼šè©±ãƒ­ã‚°ã‚’å–å¾—"""
    conversation = orchestrator.get_conversation_log(conversation_id)
    if not conversation:
        raise HTTPException(status_code=404, detail="Conversation not found")
    return conversation


@app.post("/debug/conversation")
async def debug_conversation(request: ConversationDebugRequest):
    """ä¼šè©±ã®ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’å–å¾—"""
    conversation = orchestrator.get_conversation_log(request.conversation_id)
    if not conversation:
        raise HTTPException(status_code=404, detail="Conversation not found")
    
    # ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’æ•´å½¢
    debug_info = {
        "conversation_id": conversation['conversation_id'],
        "message_count": len(conversation['messages']),
        "llm_response_count": len(conversation['llm_responses']),
        "mcp_connection_count": len(conversation['mcp_connections']),
        "total_tokens": conversation['total_tokens'],
        "duration": None,
        "messages": conversation['messages'],
        "llm_responses": [
            {
                "id": resp['id'],
                "provider": resp['provider'],
                "model": resp['model'],
                "tokens": resp['tokens'],
                "duration": resp['duration'],
                "timestamp": resp['timestamp']
            }
            for resp in conversation['llm_responses']
        ],
        "mcp_connections": [
            {
                "id": conn['id'],
                "service": conn['service'],
                "action": conn['action'],
                "success": conn['success'],
                "duration": conn['duration'],
                "timestamp": conn['timestamp'],
                "error": conn.get('error')
            }
            for conn in conversation['mcp_connections']
        ]
    }
    
    # ä¼šè©±æ™‚é–“ã‚’è¨ˆç®—
    if conversation['start_time'] and conversation['end_time']:
        start = datetime.fromisoformat(conversation['start_time'].replace('Z', '+00:00'))
        end = datetime.fromisoformat(conversation['end_time'].replace('Z', '+00:00'))
        debug_info['duration'] = (end - start).total_seconds()
    
    return debug_info


@app.get("/test/claude")
async def test_claude():
    """Claude APIæ¥ç¶šãƒ†ã‚¹ãƒˆ"""
    try:
        # ç°¡å˜ãªãƒ†ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        result = await orchestrator.process_user_request(
            request="ã“ã‚“ã«ã¡ã¯",
            user_id="test_user"
        )
        return {
            "success": True,
            "response": result.get('response'),
            "api_status": "connected"
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "api_status": "error"
        }


@app.post("/generate", response_model=Dict[str, Any])
async def generate(request: GenerationRequest):
    """
    æŒ‡å®šã•ã‚ŒãŸãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¿ã‚¤ãƒ—ã‚’ä½¿ç”¨ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚

    Args:
        request (GenerationRequest): ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¿ã‚¤ãƒ—ã€ãƒ¢ãƒ‡ãƒ«IDã‚’å«ã‚€ãƒªã‚¯ã‚¨ã‚¹ãƒˆã€‚

    Returns:
        Dict[str, Any]: ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’å«ã‚€ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã€‚

    Raises:
        HTTPException(400): ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¿ã‚¤ãƒ—ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã«ç™ºç”Ÿã—ã¾ã™ã€‚
        HTTPException(500): ãƒ¯ãƒ¼ã‚«ãƒ¼ã®å‡¦ç†ä¸­ã«å†…éƒ¨ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã«ç™ºç”Ÿã—ã¾ã™ã€‚
    """
    try:
        # WorkerFactoryã‚’ä½¿ç”¨ã—ã¦ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’ä½œæˆ
        worker = WorkerFactory.create_worker(
            worker_type=request.worker_type, 
            model_id=request.model_id
        )
    except ValueError as e:
        # WorkerFactoryãŒä¸æ˜ãªworker_typeã«å¯¾ã—ã¦ValueErrorã‚’ç™ºç”Ÿã•ã›ã‚‹ã“ã¨ã‚’æƒ³å®š
        raise HTTPException(status_code=400, detail=str(e))

    try:
        # ãƒ¯ãƒ¼ã‚«ãƒ¼ã§ã‚¿ã‚¹ã‚¯ã‚’å‡¦ç†
        # BaseWorkerã®process_taskãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã™
        response = await worker.process_task(request.prompt)
        
        return {
            "success": True,
            "response": response,
            "worker_type": request.worker_type,
            "model_id": request.model_id
        }
    except Exception as e:
        # ãƒ¯ãƒ¼ã‚«ãƒ¼å®Ÿè¡Œä¸­ã®ä¸€èˆ¬çš„ãªã‚¨ãƒ©ãƒ¼ã‚’æ•æ‰
        logger.error(f"Worker execution error: {e}")
        raise HTTPException(status_code=500, detail=f"Error processing task: {e}")


@app.get("/workers/types")
async def get_worker_types():
    """
    ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚¿ã‚¤ãƒ—ã®ä¸€è¦§ã‚’å–å¾—
    """
    return {
        "success": True,
        "worker_types": WorkerFactory.get_supported_worker_types(),
        "description": {
            "anthropic": "Claude AI by Anthropic",
            "claude": "Claude AI by Anthropic (alias)",
            "openai": "OpenAI GPT models",
            "local_llm": "Local LLM models (Ollama, etc.)"
        }
    }


# ========== Analytics Endpoints ==========

@app.post("/api/analytics/analyze")
async def analyze_data(request: AnalysisRequest):
    """ãƒ‡ãƒ¼ã‚¿åˆ†æã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ"""
    try:
        if request.analysis_type == "conversation_patterns":
            # ä¼šè©±å±¥æ­´ã‚’å–å¾—
            conversations = orchestrator.get_all_conversations()
            result = await data_analyzer.analyze_conversation_patterns(conversations)
            
        elif request.analysis_type == "task_performance":
            # ã‚¿ã‚¹ã‚¯å±¥æ­´ã‚’å–å¾—ï¼ˆå®Ÿè£…ã«å¿œã˜ã¦èª¿æ•´ï¼‰
            tasks = request.data.get('tasks', []) if request.data else []
            result = await data_analyzer.analyze_task_performance(tasks)
            
        elif request.analysis_type == "resource_prediction":
            # ãƒªã‚½ãƒ¼ã‚¹ä½¿ç”¨å±¥æ­´ã‚’å–å¾—
            historical_data = request.data.get('historical_data', []) if request.data else []
            result = await data_analyzer.predict_resource_needs(historical_data)
            
        else:
            raise HTTPException(status_code=400, detail="Invalid analysis type")
        
        return {
            "success": True,
            "analysis_type": request.analysis_type,
            "result": result
        }
        
    except Exception as e:
        logger.error(f"Analytics error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/analytics/conversations")
async def get_conversation_analytics():
    """ä¼šè©±ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æã®å–å¾—"""
    try:
        conversations = orchestrator.get_all_conversations()
        result = await data_analyzer.analyze_conversation_patterns(conversations)
        
        return {
            "success": True,
            "total_conversations": len(conversations),
            "analysis": result
        }
        
    except Exception as e:
        logger.error(f"Conversation analytics error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/analytics/tasks")
async def get_task_analytics():
    """ã‚¿ã‚¹ã‚¯ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æã®å–å¾—"""
    try:
        # ã‚¿ã‚¹ã‚¯å±¥æ­´ã‚’ä¼šè©±ãƒ­ã‚°ã‹ã‚‰æŠ½å‡º
        conversations = orchestrator.get_all_conversations()
        tasks = []
        
        for conv in conversations:
            if conv.get('task_analysis'):
                tasks.append({
                    'conversation_id': conv['conversation_id'],
                    'task': conv['task_analysis'],
                    'duration': conv.get('duration'),
                    'success': conv.get('success', True),
                    'timestamp': conv['start_time']
                })
        
        result = await data_analyzer.analyze_task_performance(tasks)
        
        return {
            "success": True,
            "total_tasks": len(tasks),
            "analysis": result
        }
        
    except Exception as e:
        logger.error(f"Task analytics error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ========== Automation Endpoints ==========

@app.post("/api/automation/rules")
async def create_automation_rule(request: AutomationRuleRequest):
    """è‡ªå‹•åŒ–ãƒ«ãƒ¼ãƒ«ã®ä½œæˆ"""
    try:
        rule = {
            "id": f"rule_{uuid.uuid4()}",
            "name": request.name,
            "description": request.description,
            "trigger": {
                "type": request.trigger_type,
                "config": request.trigger_config
            },
            "actions": request.actions,
            "active": request.active,
            "created_at": datetime.now().isoformat()
        }
        
        # AutomationRuleã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
        from orchestrator.automation.task_automator import AutomationRule, AutomationTrigger
        
        automation_rule = AutomationRule(
            id=rule["id"],
            name=rule["name"],
            description=rule["description"],
            trigger_type=AutomationTrigger(rule["trigger"]["type"]),
            trigger_config=rule["trigger"]["config"],
            actions=rule["actions"],
            enabled=rule["active"],
            created_at=datetime.now()
        )
        
        success = task_automator.add_rule(automation_rule)
        
        if success:
            return {
                "success": True,
                "rule_id": rule["id"],
                "message": "è‡ªå‹•åŒ–ãƒ«ãƒ¼ãƒ«ãŒä½œæˆã•ã‚Œã¾ã—ãŸ"
            }
        else:
            raise HTTPException(status_code=400, detail="ãƒ«ãƒ¼ãƒ«ã®ä½œæˆã«å¤±æ•—ã—ã¾ã—ãŸ")
            
    except Exception as e:
        logger.error(f"Automation rule creation error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/automation/rules")
async def get_automation_rules():
    """è‡ªå‹•åŒ–ãƒ«ãƒ¼ãƒ«ä¸€è¦§ã®å–å¾—"""
    try:
        rules = task_automator.list_rules()
        return {
            "success": True,
            "rules": rules,
            "count": len(rules)
        }
        
    except Exception as e:
        logger.error(f"Get automation rules error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/api/automation/rules/{rule_id}")
async def delete_automation_rule(rule_id: str):
    """è‡ªå‹•åŒ–ãƒ«ãƒ¼ãƒ«ã®å‰Šé™¤"""
    try:
        success = task_automator.remove_rule(rule_id)
        
        if success:
            return {
                "success": True,
                "message": f"ãƒ«ãƒ¼ãƒ« {rule_id} ãŒå‰Šé™¤ã•ã‚Œã¾ã—ãŸ"
            }
        else:
            raise HTTPException(status_code=404, detail="ãƒ«ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            
    except Exception as e:
        logger.error(f"Delete automation rule error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/automation/execute/{rule_id}")
async def execute_automation_rule(rule_id: str):
    """è‡ªå‹•åŒ–ãƒ«ãƒ¼ãƒ«ã®æ‰‹å‹•å®Ÿè¡Œ"""
    try:
        result = await task_automator.execute_rule(rule_id)
        
        return {
            "success": result.success,
            "executed_actions": result.executed_actions,
            "results": result.outputs,
            "timestamp": result.timestamp.isoformat() if result.timestamp else None,
            "error": result.error,
            "duration": result.duration
        }
        
    except Exception as e:
        logger.error(f"Execute automation rule error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/automation/suggest")
async def suggest_automations():
    """è‡ªå‹•åŒ–ã®ææ¡ˆã‚’å–å¾—"""
    try:
        # ã‚¿ã‚¹ã‚¯å±¥æ­´ã‚’å–å¾—
        conversations = orchestrator.get_all_conversations()
        task_history = []
        
        for conv in conversations:
            if conv.get('messages'):
                for msg in conv['messages']:
                    task_history.append({
                        'message': msg['content'],
                        'timestamp': msg['timestamp'],
                        'user_id': conv.get('user_id', 'default_user')
                    })
        
        suggestions = task_automator.suggest_automations(task_history)
        
        return {
            "success": True,
            "suggestions": suggestions,
            "count": len(suggestions)
        }
        
    except Exception as e:
        logger.error(f"Suggest automations error: {e}")
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", 9000))
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=port,
        log_level="info",
        access_log=True
    )