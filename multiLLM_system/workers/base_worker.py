"""
Base Worker - ã™ã¹ã¦ã®Worker LLMã®åŸºåº•ã‚¯ãƒ©ã‚¹
å…±é€šæ©Ÿèƒ½ã¨ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æä¾›
"""

import asyncio
import logging
from abc import ABC, abstractmethod
from datetime import datetime
from typing import Dict, Any, Optional, List
from dataclasses import dataclass
import json
import uuid

logger = logging.getLogger(__name__)


@dataclass
class WorkerTask:
    """WorkerãŒå‡¦ç†ã™ã‚‹ã‚¿ã‚¹ã‚¯"""
    id: str
    type: str
    description: str
    context: Dict[str, Any]
    priority: str = "medium"
    created_at: datetime = None
    started_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    status: str = "pending"
    result: Optional[Dict] = None
    error: Optional[str] = None


class BaseWorker(ABC):
    """
    ã™ã¹ã¦ã®Worker LLMã®åŸºåº•ã‚¯ãƒ©ã‚¹
    å…±é€šã®æ©Ÿèƒ½ã¨ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æä¾›
    """
    
    def __init__(self, name: str, config: Dict):
        self.name = name
        self.config = config
        self.model = config.get('model', 'gpt-4')
        self.specialization = config.get('specialization', [])
        self.max_concurrent_tasks = config.get('maxConcurrentTasks', 3)
        self.temperature = config.get('temperature', 0.7)
        
        # LLMã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆï¼ˆå¾Œã§æ³¨å…¥ï¼‰
        self.llm_client = None
        
        # ã‚¿ã‚¹ã‚¯ç®¡ç†
        self.active_tasks = {}
        self.task_queue = asyncio.Queue()
        self.processing = False
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        self.metrics = {
            'total_tasks': 0,
            'successful_tasks': 0,
            'failed_tasks': 0,
            'total_tokens': 0,
            'total_cost': 0.0,
            'average_latency': 0.0
        }
        
        # ãƒ¡ãƒ¢ãƒªï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¿æŒï¼‰
        self.memory = {
            'short_term': [],  # æœ€è¿‘ã®10ã‚¿ã‚¹ã‚¯
            'long_term': {},   # é‡è¦ãªæ±ºå®šäº‹é …
            'context': {}      # æ°¸ç¶šçš„ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        }
    
    async def initialize(self):
        """Workerã®åˆæœŸåŒ–"""
        logger.info(f"ğŸš€ Initializing {self.name} worker...")
        
        # ã‚¿ã‚¹ã‚¯å‡¦ç†ãƒ«ãƒ¼ãƒ—ã‚’é–‹å§‹
        self.processing = True
        for i in range(self.max_concurrent_tasks):
            asyncio.create_task(self._process_task_loop(i))
        
        logger.info(f"âœ… {self.name} worker initialized with {self.max_concurrent_tasks} concurrent processors")
    
    async def shutdown(self):
        """Workerã®ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³"""
        logger.info(f"ğŸ›‘ Shutting down {self.name} worker...")
        self.processing = False
        
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚¿ã‚¹ã‚¯ã®å®Œäº†ã‚’å¾…ã¤
        if self.active_tasks:
            await asyncio.gather(*[
                self._wait_for_task(task_id) 
                for task_id in self.active_tasks
            ])
        
        logger.info(f"âœ… {self.name} worker shut down")
    
    async def submit_task(self, task: WorkerTask) -> str:
        """ã‚¿ã‚¹ã‚¯ã‚’é€ä¿¡"""
        task.id = task.id or str(uuid.uuid4())
        task.created_at = task.created_at or datetime.now()
        
        await self.task_queue.put(task)
        logger.info(f"ğŸ“¥ Task {task.id} submitted to {self.name} worker")
        
        return task.id
    
    async def _process_task_loop(self, processor_id: int):
        """ã‚¿ã‚¹ã‚¯å‡¦ç†ãƒ«ãƒ¼ãƒ—"""
        while self.processing:
            try:
                # ã‚¿ã‚¹ã‚¯ã‚’å–å¾—ï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãï¼‰
                task = await asyncio.wait_for(
                    self.task_queue.get(), 
                    timeout=1.0
                )
                
                # ã‚¿ã‚¹ã‚¯ã‚’å‡¦ç†
                await self._process_task(task, processor_id)
                
            except asyncio.TimeoutError:
                continue
            except Exception as e:
                logger.error(f"Task processing error in {self.name}[{processor_id}]: {e}")
    
    async def _process_task(self, task: WorkerTask, processor_id: int):
        """ã‚¿ã‚¹ã‚¯ã‚’å‡¦ç†"""
        logger.info(f"ğŸ”§ {self.name}[{processor_id}] processing task {task.id}")
        
        task.started_at = datetime.now()
        task.status = "processing"
        self.active_tasks[task.id] = task
        
        try:
            # Workerå›ºæœ‰ã®å‡¦ç†ã‚’å®Ÿè¡Œ
            result = await self.process(task)
            
            # æˆåŠŸ
            task.completed_at = datetime.now()
            task.status = "completed"
            task.result = result
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
            self._update_metrics(task, success=True)
            
            # çŸ­æœŸè¨˜æ†¶ã«è¿½åŠ 
            self._update_memory(task)
            
            logger.info(f"âœ… {self.name}[{processor_id}] completed task {task.id}")
            
        except Exception as e:
            # ã‚¨ãƒ©ãƒ¼
            task.completed_at = datetime.now()
            task.status = "failed"
            task.error = str(e)
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
            self._update_metrics(task, success=False)
            
            logger.error(f"âŒ {self.name}[{processor_id}] failed task {task.id}: {e}")
        
        finally:
            del self.active_tasks[task.id]
    
    @abstractmethod
    async def process(self, task: WorkerTask) -> Dict[str, Any]:
        """
        Workerå›ºæœ‰ã®å‡¦ç†ã‚’å®Ÿè£…
        ã‚µãƒ–ã‚¯ãƒ©ã‚¹ã§å¿…ãšå®Ÿè£…ã™ã‚‹
        """
        pass
    
    async def generate_llm_response(self, prompt: str, system_prompt: Optional[str] = None) -> str:
        """LLMã‚’ä½¿ç”¨ã—ã¦ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’ç”Ÿæˆ"""
        if not self.llm_client:
            raise ValueError("LLM client not initialized")
        
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        # ãƒ¢ãƒ‡ãƒ«ã«å¿œã˜ã¦ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’é¸æŠ
        if self.model.startswith('gpt'):
            response = await self.llm_client.openai_generate(
                messages=messages,
                model=self.model,
                temperature=self.temperature
            )
        elif self.model.startswith('claude'):
            response = await self.llm_client.anthropic_generate(
                messages=messages,
                model=self.model,
                temperature=self.temperature
            )
        else:
            response = await self.llm_client.generate(
                prompt=prompt,
                model=self.model,
                temperature=self.temperature
            )
        
        return response
    
    def _update_metrics(self, task: WorkerTask, success: bool):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’æ›´æ–°"""
        self.metrics['total_tasks'] += 1
        
        if success:
            self.metrics['successful_tasks'] += 1
        else:
            self.metrics['failed_tasks'] += 1
        
        # ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·è¨ˆç®—
        if task.started_at and task.completed_at:
            latency = (task.completed_at - task.started_at).total_seconds()
            # ç§»å‹•å¹³å‡ã§ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’æ›´æ–°
            self.metrics['average_latency'] = (
                self.metrics['average_latency'] * 0.9 + latency * 0.1
            )
    
    def _update_memory(self, task: WorkerTask):
        """ãƒ¡ãƒ¢ãƒªã‚’æ›´æ–°"""
        # çŸ­æœŸè¨˜æ†¶ã«è¿½åŠ ï¼ˆæœ€å¤§10ä»¶ï¼‰
        self.memory['short_term'].append({
            'task_id': task.id,
            'type': task.type,
            'description': task.description[:100],
            'result': task.result,
            'timestamp': task.completed_at.isoformat()
        })
        
        if len(self.memory['short_term']) > 10:
            self.memory['short_term'].pop(0)
        
        # é‡è¦ãªã‚¿ã‚¹ã‚¯ã¯é•·æœŸè¨˜æ†¶ã«ä¿å­˜
        if task.priority == 'high' or 'important' in task.description.lower():
            self.memory['long_term'][task.id] = {
                'description': task.description,
                'result': task.result,
                'timestamp': task.completed_at.isoformat()
            }
    
    async def _wait_for_task(self, task_id: str, timeout: float = 60.0):
        """ã‚¿ã‚¹ã‚¯ã®å®Œäº†ã‚’å¾…ã¤"""
        start_time = asyncio.get_event_loop().time()
        
        while task_id in self.active_tasks:
            if asyncio.get_event_loop().time() - start_time > timeout:
                logger.warning(f"Task {task_id} timed out")
                break
            await asyncio.sleep(0.1)
    
    def get_status(self) -> Dict[str, Any]:
        """Workerã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å–å¾—"""
        return {
            'name': self.name,
            'model': self.model,
            'specialization': self.specialization,
            'status': 'active' if self.processing else 'inactive',
            'active_tasks': len(self.active_tasks),
            'queued_tasks': self.task_queue.qsize(),
            'metrics': self.metrics,
            'memory': {
                'short_term_count': len(self.memory['short_term']),
                'long_term_count': len(self.memory['long_term'])
            }
        }
    
    def get_memory_snapshot(self) -> Dict[str, Any]:
        """ãƒ¡ãƒ¢ãƒªã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’å–å¾—"""
        return {
            'worker': self.name,
            'timestamp': datetime.now().isoformat(),
            'short_term': self.memory['short_term'][-5:],  # æœ€æ–°5ä»¶
            'long_term_keys': list(self.memory['long_term'].keys()),
            'context': self.memory['context']
        }
    
    def load_memory_snapshot(self, snapshot: Dict[str, Any]):
        """ãƒ¡ãƒ¢ãƒªã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰"""
        if snapshot.get('context'):
            self.memory['context'].update(snapshot['context'])
        
        logger.info(f"ğŸ“¥ Loaded memory snapshot for {self.name}")
    
    async def handle_emergency(self, emergency_type: str, data: Dict):
        """ç·Šæ€¥äº‹æ…‹ã¸ã®å¯¾å¿œ"""
        logger.warning(f"ğŸš¨ {self.name} handling emergency: {emergency_type}")
        
        # å‡¦ç†ä¸­ã®ã‚¿ã‚¹ã‚¯ã‚’ä¸€æ™‚åœæ­¢
        self.processing = False
        
        # Workerå›ºæœ‰ã®ç·Šæ€¥å¯¾å¿œ
        await self.emergency_response(emergency_type, data)
        
        # å‡¦ç†ã‚’å†é–‹
        self.processing = True
    
    async def emergency_response(self, emergency_type: str, data: Dict):
        """Workerå›ºæœ‰ã®ç·Šæ€¥å¯¾å¿œï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰å¯èƒ½ï¼‰"""
        pass


# ä½¿ç”¨ä¾‹ï¼šå…·ä½“çš„ãªWorkerã®å®Ÿè£…
class BackendWorker(BaseWorker):
    """ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰é–‹ç™ºå°‚é–€Worker"""
    
    async def process(self, task: WorkerTask) -> Dict[str, Any]:
        """ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯ã‚’å‡¦ç†"""
        
        # ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒ—ã«å¿œã˜ãŸå‡¦ç†
        if 'bug' in task.description.lower() or 'fix' in task.description.lower():
            return await self._fix_bug(task)
        elif 'api' in task.description.lower():
            return await self._implement_api(task)
        else:
            return await self._general_backend_task(task)
    
    async def _fix_bug(self, task: WorkerTask) -> Dict[str, Any]:
        """ãƒã‚°ä¿®æ­£å‡¦ç†"""
        prompt = f"""
ä»¥ä¸‹ã®ãƒã‚°ã‚’ä¿®æ­£ã—ã¦ãã ã•ã„ï¼š

{task.description}

ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼š
{json.dumps(task.context, ensure_ascii=False, indent=2)}

ä¿®æ­£æ–¹æ³•ã¨ä¿®æ­£å¾Œã®ã‚³ãƒ¼ãƒ‰ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚
"""
        
        response = await self.generate_llm_response(
            prompt,
            system_prompt="ã‚ãªãŸã¯çµŒé¨“è±Šå¯Œãªãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã§ã™ã€‚"
        )
        
        return {
            'type': 'bug_fix',
            'solution': response,
            'confidence': 0.85
        }
    
    async def _implement_api(self, task: WorkerTask) -> Dict[str, Any]:
        """APIå®Ÿè£…å‡¦ç†"""
        # APIå®Ÿè£…ã®ãƒ­ã‚¸ãƒƒã‚¯
        return {
            'type': 'api_implementation',
            'code': '// API implementation code',
            'documentation': '// API documentation'
        }
    
    async def _general_backend_task(self, task: WorkerTask) -> Dict[str, Any]:
        """ä¸€èˆ¬çš„ãªãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚¿ã‚¹ã‚¯"""
        response = await self.generate_llm_response(task.description)
        
        return {
            'type': 'general',
            'response': response
        }


# Worker ãƒ•ã‚¡ã‚¯ãƒˆãƒªãƒ¼
def create_worker(worker_type: str, config: Dict) -> BaseWorker:
    """Workerã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ"""
    worker_classes = {
        'backend': BackendWorker,
        # 'frontend': FrontendWorker,
        # 'review': ReviewWorker,
        # ä»–ã®Workerã‚¯ãƒ©ã‚¹ã‚’è¿½åŠ 
    }
    
    worker_class = worker_classes.get(worker_type, BaseWorker)
    return worker_class(name=f"{worker_type}_worker", config=config)