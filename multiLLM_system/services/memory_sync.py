"""
Memory Sync Service - OpenMemoryåŒæœŸã‚µãƒ¼ãƒ“ã‚¹
å…¨LLMã®è¨˜æ†¶ã‚’å®šæœŸçš„ã«åŒæœŸã—ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å…±æœ‰
"""

import asyncio
import logging
import json
import aiohttp
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
import hashlib
from urllib.parse import urlparse

logger = logging.getLogger(__name__)


@dataclass
class MemoryEntry:
    """ãƒ¡ãƒ¢ãƒªã‚¨ãƒ³ãƒˆãƒªãƒ¼"""
    id: str
    worker_name: str
    content: str
    metadata: Dict[str, Any]
    timestamp: datetime
    importance: float = 0.5  # 0.0-1.0
    
    def to_dict(self) -> Dict:
        """è¾æ›¸ã«å¤‰æ›"""
        return {
            'id': self.id,
            'worker_name': self.worker_name,
            'content': self.content,
            'metadata': self.metadata,
            'timestamp': self.timestamp.isoformat(),
            'importance': self.importance
        }


class MemorySyncService:
    """
    OpenMemoryã¨é€£æºã—ã¦LLMé–“ã§ãƒ¡ãƒ¢ãƒªã‚’åŒæœŸ
    """
    
    def __init__(self, config: Dict):
        self.config = config
        self.openmemory_url = config.get('storage', {}).get('connectionString', 'http://localhost:8765')
        self.sync_interval = config.get('syncInterval', 300)  # 5åˆ†
        self.conflict_resolution = config.get('conflictResolution', 'latest')
        
        # ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.memory_cache = {}
        self.last_sync = {}
        
        # åŒæœŸçŠ¶æ…‹
        self.syncing = False
        self.sync_lock = asyncio.Lock()
        
        # HTTPã‚»ãƒƒã‚·ãƒ§ãƒ³
        self.session = None
    
    async def initialize(self):
        """ã‚µãƒ¼ãƒ“ã‚¹ã®åˆæœŸåŒ–"""
        logger.info("ğŸ§  Initializing Memory Sync Service...")
        
        # HTTPã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’ä½œæˆ
        self.session = aiohttp.ClientSession()
        
        # åˆæœŸåŒæœŸ
        await self.sync_all()
        
        # å®šæœŸåŒæœŸã‚’é–‹å§‹
        asyncio.create_task(self._sync_loop())
        
        logger.info("âœ… Memory Sync Service initialized")
    
    async def shutdown(self):
        """ã‚µãƒ¼ãƒ“ã‚¹ã®ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³"""
        if self.session:
            await self.session.close()
    
    async def _sync_loop(self):
        """å®šæœŸçš„ãªåŒæœŸãƒ«ãƒ¼ãƒ—"""
        while True:
            await asyncio.sleep(self.sync_interval)
            try:
                await self.sync_all()
            except Exception as e:
                logger.error(f"Sync loop error: {e}")
    
    async def sync_all(self):
        """å…¨Workerã®ãƒ¡ãƒ¢ãƒªã‚’åŒæœŸ"""
        async with self.sync_lock:
            if self.syncing:
                logger.warning("Sync already in progress, skipping...")
                return
            
            self.syncing = True
            logger.info("ğŸ”„ Starting memory synchronization...")
            
            try:
                # OpenMemoryã‹ã‚‰æœ€æ–°ã®ãƒ¡ãƒ¢ãƒªã‚’å–å¾—
                remote_memories = await self._fetch_remote_memories()
                
                # ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¡ãƒ¢ãƒªã¨çµ±åˆ
                merged_memories = await self._merge_memories(remote_memories)
                
                # OpenMemoryã«ä¿å­˜
                await self._save_to_openmemory(merged_memories)
                
                # ãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ›´æ–°
                self.memory_cache = merged_memories
                
                logger.info(f"âœ… Memory sync completed. Total entries: {len(merged_memories)}")
                
            except Exception as e:
                logger.error(f"Memory sync failed: {e}")
            finally:
                self.syncing = False
    
    async def _fetch_remote_memories(self) -> Dict[str, List[MemoryEntry]]:
        """OpenMemoryã‹ã‚‰è¨˜æ†¶ã‚’å–å¾—"""
        try:
            # URLãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
            if not self._validate_url(self.openmemory_url):
                raise ValueError(f"Invalid OpenMemory URL: {self.openmemory_url}")
            
            # æœ€å¾Œã®åŒæœŸæ™‚åˆ»ä»¥é™ã®æ›´æ–°ã‚’å–å¾—
            since = datetime.now() - timedelta(seconds=self.sync_interval * 2)
            
            async with self.session.get(
                f"{self.openmemory_url}/api/memories",
                params={
                    'since': since.isoformat(),
                    'client': 'multiLLM'
                },
                timeout=aiohttp.ClientTimeout(total=30)
            ) as response:
                if response.status == 200:
                    data = await response.json()
                    
                    # Workeråˆ¥ã«æ•´ç†
                    memories_by_worker = {}
                    for item in data.get('memories', []):
                        worker_name = item.get('metadata', {}).get('worker_name', 'unknown')
                        
                        if worker_name not in memories_by_worker:
                            memories_by_worker[worker_name] = []
                        
                        memories_by_worker[worker_name].append(MemoryEntry(
                            id=item['id'],
                            worker_name=worker_name,
                            content=item['content'],
                            metadata=item.get('metadata', {}),
                            timestamp=datetime.fromisoformat(item['created_at']),
                            importance=item.get('metadata', {}).get('importance', 0.5)
                        ))
                    
                    return memories_by_worker
                else:
                    logger.error(f"Failed to fetch memories: {response.status}")
                    return {}
                    
        except Exception as e:
            logger.error(f"Error fetching remote memories: {e}")
            return {}
    
    async def _merge_memories(self, remote_memories: Dict[str, List[MemoryEntry]]) -> Dict[str, List[MemoryEntry]]:
        """ãƒ­ãƒ¼ã‚«ãƒ«ã¨ãƒªãƒ¢ãƒ¼ãƒˆã®ãƒ¡ãƒ¢ãƒªã‚’ãƒãƒ¼ã‚¸"""
        merged = {}
        
        # ã™ã¹ã¦ã®Workerã‚’å‡¦ç†
        all_workers = set(self.memory_cache.keys()) | set(remote_memories.keys())
        
        for worker_name in all_workers:
            local_entries = self.memory_cache.get(worker_name, [])
            remote_entries = remote_memories.get(worker_name, [])
            
            # ã‚³ãƒ³ãƒ•ãƒªã‚¯ãƒˆè§£æ±º
            if self.conflict_resolution == 'latest':
                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§ã‚½ãƒ¼ãƒˆã—ã¦æœ€æ–°ã‚’å„ªå…ˆ
                all_entries = local_entries + remote_entries
                all_entries.sort(key=lambda x: x.timestamp, reverse=True)
                
                # é‡è¤‡ã‚’é™¤å»ï¼ˆIDãƒ™ãƒ¼ã‚¹ï¼‰
                seen_ids = set()
                unique_entries = []
                for entry in all_entries:
                    if entry.id not in seen_ids:
                        seen_ids.add(entry.id)
                        unique_entries.append(entry)
                
                merged[worker_name] = unique_entries[:100]  # æœ€æ–°100ä»¶ã‚’ä¿æŒ
                
            elif self.conflict_resolution == 'merge':
                # å†…å®¹ã‚’ãƒãƒ¼ã‚¸ï¼ˆå®Ÿè£…ã¯è¤‡é›‘ã«ãªã‚‹ãŸã‚ç°¡ç•¥åŒ–ï¼‰
                merged[worker_name] = self._merge_entries(local_entries, remote_entries)
            
            else:  # manual
                # æ‰‹å‹•è§£æ±ºãŒå¿…è¦ãªå ´åˆã¯ãƒ­ãƒ¼ã‚«ãƒ«ã‚’å„ªå…ˆ
                merged[worker_name] = local_entries
        
        return merged
    
    def _merge_entries(self, local: List[MemoryEntry], remote: List[MemoryEntry]) -> List[MemoryEntry]:
        """ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚’ãƒãƒ¼ã‚¸ï¼ˆè©³ç´°å®Ÿè£…ï¼‰"""
        # IDã§ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–
        local_by_id = {e.id: e for e in local}
        remote_by_id = {e.id: e for e in remote}
        
        merged = []
        
        # ä¸¡æ–¹ã«å­˜åœ¨ã™ã‚‹ã‚¨ãƒ³ãƒˆãƒªãƒ¼ã‚’ãƒãƒ¼ã‚¸
        for entry_id in set(local_by_id.keys()) & set(remote_by_id.keys()):
            local_entry = local_by_id[entry_id]
            remote_entry = remote_by_id[entry_id]
            
            # ã‚ˆã‚Šæ–°ã—ã„æ–¹ã‚’é¸æŠ
            if local_entry.timestamp > remote_entry.timestamp:
                merged.append(local_entry)
            else:
                merged.append(remote_entry)
        
        # ãƒ­ãƒ¼ã‚«ãƒ«ã®ã¿ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼
        for entry_id in set(local_by_id.keys()) - set(remote_by_id.keys()):
            merged.append(local_by_id[entry_id])
        
        # ãƒªãƒ¢ãƒ¼ãƒˆã®ã¿ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼
        for entry_id in set(remote_by_id.keys()) - set(local_by_id.keys()):
            merged.append(remote_by_id[entry_id])
        
        return sorted(merged, key=lambda x: x.timestamp, reverse=True)[:100]
    
    def _validate_url(self, url: str) -> bool:
        """URLã®å¦¥å½“æ€§ã‚’æ¤œè¨¼"""
        try:
            parsed = urlparse(url)
            # HTTPã¾ãŸã¯HTTPSã®ã¿è¨±å¯
            if parsed.scheme not in ['http', 'https']:
                return False
            # ãƒ›ã‚¹ãƒˆåãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
            if not parsed.netloc:
                return False
            # ãƒ›ãƒ¯ã‚¤ãƒˆãƒªã‚¹ãƒˆï¼ˆå¿…è¦ã«å¿œã˜ã¦è¿½åŠ ï¼‰
            allowed_hosts = ['localhost', '127.0.0.1', 'openmemory.internal']
            if self.config.get('allowedHosts'):
                allowed_hosts.extend(self.config['allowedHosts'])
            # ãƒ›ã‚¹ãƒˆåãƒã‚§ãƒƒã‚¯ï¼ˆãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç’°å¢ƒã§ã¯å¿…é ˆï¼‰
            # if parsed.hostname not in allowed_hosts:
            #     return False
            return True
        except Exception:
            return False
    
    async def _save_to_openmemory(self, memories: Dict[str, List[MemoryEntry]]):
        """OpenMemoryã«ä¿å­˜"""
        try:
            # ãƒãƒƒãƒä¿å­˜ç”¨ã®ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
            batch_data = []
            
            for worker_name, entries in memories.items():
                for entry in entries[:10]:  # å„Workerã®æœ€æ–°10ä»¶ã‚’ä¿å­˜
                    batch_data.append({
                        'content': entry.content,
                        'metadata': {
                            **entry.metadata,
                            'worker_name': worker_name,
                            'importance': entry.importance,
                            'multiLLM': True
                        }
                    })
            
            if batch_data:
                async with self.session.post(
                    f"{self.openmemory_url}/api/memories/batch",
                    json={'memories': batch_data},
                    headers={'Content-Type': 'application/json'}
                ) as response:
                    if response.status == 200:
                        logger.info(f"ğŸ“¤ Saved {len(batch_data)} memories to OpenMemory")
                    else:
                        logger.error(f"Failed to save memories: {response.status}")
                        
        except Exception as e:
            logger.error(f"Error saving to OpenMemory: {e}")
    
    async def add_memory(self, worker_name: str, content: str, metadata: Optional[Dict] = None, importance: float = 0.5):
        """æ–°ã—ã„ãƒ¡ãƒ¢ãƒªã‚’è¿½åŠ """
        entry = MemoryEntry(
            id=self._generate_id(worker_name, content),
            worker_name=worker_name,
            content=content,
            metadata=metadata or {},
            timestamp=datetime.now(),
            importance=importance
        )
        
        # ãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«è¿½åŠ 
        if worker_name not in self.memory_cache:
            self.memory_cache[worker_name] = []
        
        self.memory_cache[worker_name].insert(0, entry)
        
        # é‡è¦åº¦ãŒé«˜ã„å ´åˆã¯å³åº§ã«åŒæœŸ
        if importance > 0.8:
            await self._save_single_memory(entry)
    
    async def _save_single_memory(self, entry: MemoryEntry):
        """å˜ä¸€ã®ãƒ¡ãƒ¢ãƒªã‚’å³åº§ã«ä¿å­˜"""
        try:
            async with self.session.post(
                f"{self.openmemory_url}/api/memories",
                json={
                    'content': entry.content,
                    'metadata': {
                        **entry.metadata,
                        'worker_name': entry.worker_name,
                        'importance': entry.importance,
                        'multiLLM': True
                    }
                }
            ) as response:
                if response.status == 200:
                    logger.info(f"ğŸ“¤ High importance memory saved immediately")
                    
        except Exception as e:
            logger.error(f"Error saving single memory: {e}")
    
    def _generate_id(self, worker_name: str, content: str) -> str:
        """ãƒ¡ãƒ¢ãƒªã‚¨ãƒ³ãƒˆãƒªãƒ¼ã®IDã‚’ç”Ÿæˆ"""
        data = f"{worker_name}:{content}:{datetime.now().isoformat()}"
        return hashlib.md5(data.encode()).hexdigest()
    
    async def search_memories(self, query: str, worker_filter: Optional[List[str]] = None, limit: int = 10) -> List[MemoryEntry]:
        """ãƒ¡ãƒ¢ãƒªã‚’æ¤œç´¢"""
        results = []
        
        # ãƒ­ãƒ¼ã‚«ãƒ«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰æ¤œç´¢
        for worker_name, entries in self.memory_cache.items():
            if worker_filter and worker_name not in worker_filter:
                continue
            
            for entry in entries:
                if query.lower() in entry.content.lower():
                    results.append(entry)
                    if len(results) >= limit:
                        break
        
        # é‡è¦åº¦ã§ã‚½ãƒ¼ãƒˆ
        results.sort(key=lambda x: x.importance, reverse=True)
        
        return results[:limit]
    
    async def get_worker_context(self, worker_name: str) -> Dict[str, Any]:
        """ç‰¹å®šWorkerã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—"""
        entries = self.memory_cache.get(worker_name, [])
        
        if not entries:
            return {'memories': [], 'summary': 'No memories found'}
        
        # æœ€æ–°ã®ãƒ¡ãƒ¢ãƒª
        recent_memories = [e.to_dict() for e in entries[:5]]
        
        # é‡è¦ãªãƒ¡ãƒ¢ãƒª
        important_memories = [
            e.to_dict() for e in entries 
            if e.importance > 0.7
        ][:5]
        
        # ã‚µãƒãƒªãƒ¼ç”Ÿæˆï¼ˆç°¡æ˜“ç‰ˆï¼‰
        summary = f"Worker {worker_name} has {len(entries)} memories. "
        summary += f"Most recent: {entries[0].content[:50]}..." if entries else ""
        
        return {
            'worker_name': worker_name,
            'total_memories': len(entries),
            'recent_memories': recent_memories,
            'important_memories': important_memories,
            'summary': summary
        }
    
    async def create_conversation_summary(self, conversation_id: str, messages: List[Dict], metadata: Dict) -> str:
        """ä¼šè©±ã‚’è¦ç´„ã—ã¦ãƒ¡ãƒ¢ãƒªã«ä¿å­˜"""
        # ä¼šè©±ã®è¦ç´„ã‚’ç”Ÿæˆï¼ˆå®Ÿéš›ã¯LLMã‚’ä½¿ç”¨ï¼‰
        summary = f"Conversation {conversation_id}: {len(messages)} messages exchanged. "
        summary += f"Topics: {', '.join(metadata.get('topics', ['general']))}"
        
        # è¦ç´„ã‚’ãƒ¡ãƒ¢ãƒªã«ä¿å­˜
        await self.add_memory(
            worker_name='orchestrator',
            content=summary,
            metadata={
                'type': 'conversation_summary',
                'conversation_id': conversation_id,
                'message_count': len(messages),
                **metadata
            },
            importance=0.7
        )
        
        return summary
    
    def get_sync_status(self) -> Dict[str, Any]:
        """åŒæœŸã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å–å¾—"""
        return {
            'syncing': self.syncing,
            'last_sync': max(self.last_sync.values()).isoformat() if self.last_sync else None,
            'memory_count': sum(len(entries) for entries in self.memory_cache.values()),
            'workers': list(self.memory_cache.keys()),
            'sync_interval': self.sync_interval
        }


# ä½¿ç”¨ä¾‹
async def main():
    config = {
        'syncInterval': 300,
        'conflictResolution': 'latest',
        'storage': {
            'type': 'openmemory',
            'connectionString': 'http://localhost:8765'
        }
    }
    
    service = MemorySyncService(config)
    await service.initialize()
    
    # ãƒ¡ãƒ¢ãƒªã‚’è¿½åŠ 
    await service.add_memory(
        worker_name='backend_worker',
        content='APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ/api/usersã‚’å®Ÿè£…ã—ã¾ã—ãŸ',
        metadata={'task_id': '123', 'type': 'implementation'},
        importance=0.8
    )
    
    # ãƒ¡ãƒ¢ãƒªã‚’æ¤œç´¢
    results = await service.search_memories('API')
    for result in results:
        print(f"Found: {result.content} (importance: {result.importance})")
    
    # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèª
    status = service.get_sync_status()
    print(f"Sync status: {json.dumps(status, indent=2)}")
    
    await service.shutdown()


if __name__ == "__main__":
    asyncio.run(main())