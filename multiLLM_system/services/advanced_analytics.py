"""
Advanced Analytics Service
MultiLLMã‚·ã‚¹ãƒ†ãƒ ã®è©³ç´°ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
"""

import asyncio
import logging
import json
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
import statistics
from collections import defaultdict, deque
import aiofiles

logger = logging.getLogger(__name__)


@dataclass
class TaskMetrics:
    """ã‚¿ã‚¹ã‚¯ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    task_id: str
    worker_id: str
    task_type: str
    status: str
    created_at: datetime
    started_at: Optional[datetime]
    completed_at: Optional[datetime]
    duration_ms: Optional[int]
    tokens_used: Optional[int]
    cost: Optional[float]
    success: bool
    error_type: Optional[str]
    priority: str


@dataclass
class WorkerMetrics:
    """Workerãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    worker_id: str
    worker_type: str
    model: str
    total_tasks: int
    successful_tasks: int
    failed_tasks: int
    average_duration: float
    median_duration: float
    p95_duration: float
    total_tokens: int
    total_cost: float
    utilization_rate: float
    error_rate: float
    throughput: float  # tasks per hour


@dataclass
class SystemMetrics:
    """ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ãƒ¡ãƒˆãƒªã‚¯ã‚¹"""
    timestamp: datetime
    total_requests: int
    active_workers: int
    total_workers: int
    system_load: float
    memory_usage: float
    cpu_usage: float
    error_rate: float
    average_latency: float
    throughput: float


class MetricsCollector:
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†å™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.task_metrics = deque(maxlen=10000)  # æœ€æ–°10,000ã‚¿ã‚¹ã‚¯
        self.worker_metrics = {}
        self.system_metrics = deque(maxlen=1440)  # 24æ™‚é–“åˆ†ï¼ˆ1åˆ†é–“éš”ï¼‰
        
        # çµ±è¨ˆè¨ˆç®—ç”¨
        self.worker_durations = defaultdict(list)
        self.worker_tokens = defaultdict(int)
        self.worker_costs = defaultdict(float)
        
        # æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿
        self.hourly_stats = defaultdict(lambda: {
            'tasks': 0,
            'errors': 0,
            'total_duration': 0,
            'total_tokens': 0,
            'total_cost': 0
        })
    
    async def record_task_metrics(self, metrics: TaskMetrics):
        """ã‚¿ã‚¹ã‚¯ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²"""
        self.task_metrics.append(metrics)
        
        # Workeråˆ¥çµ±è¨ˆã‚’æ›´æ–°
        worker_id = metrics.worker_id
        if metrics.duration_ms:
            self.worker_durations[worker_id].append(metrics.duration_ms)
            # æœ€æ–°1000ä»¶ã®ã¿ä¿æŒ
            if len(self.worker_durations[worker_id]) > 1000:
                self.worker_durations[worker_id] = self.worker_durations[worker_id][-1000:]
        
        if metrics.tokens_used:
            self.worker_tokens[worker_id] += metrics.tokens_used
        
        if metrics.cost:
            self.worker_costs[worker_id] += metrics.cost
        
        # æ™‚é–“åˆ¥çµ±è¨ˆã‚’æ›´æ–°
        hour_key = metrics.created_at.strftime('%Y-%m-%d-%H')
        stats = self.hourly_stats[hour_key]
        stats['tasks'] += 1
        if not metrics.success:
            stats['errors'] += 1
        if metrics.duration_ms:
            stats['total_duration'] += metrics.duration_ms
        if metrics.tokens_used:
            stats['total_tokens'] += metrics.tokens_used
        if metrics.cost:
            stats['total_cost'] += metrics.cost
        
        logger.debug(f"Recorded metrics for task {metrics.task_id}")
    
    async def record_system_metrics(self, metrics: SystemMetrics):
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²"""
        self.system_metrics.append(metrics)
        logger.debug(f"Recorded system metrics: {metrics.total_requests} requests")
    
    def calculate_worker_metrics(self, worker_id: str) -> Optional[WorkerMetrics]:
        """Workerçµ±è¨ˆã‚’è¨ˆç®—"""
        # æœ€è¿‘ã®ã‚¿ã‚¹ã‚¯ã‹ã‚‰Workeræƒ…å ±ã‚’å–å¾—
        worker_tasks = [m for m in self.task_metrics if m.worker_id == worker_id]
        
        if not worker_tasks:
            return None
        
        # åŸºæœ¬çµ±è¨ˆ
        total_tasks = len(worker_tasks)
        successful_tasks = sum(1 for t in worker_tasks if t.success)
        failed_tasks = total_tasks - successful_tasks
        
        # æœŸé–“çµ±è¨ˆï¼ˆæœ€è¿‘ã®ã‚¿ã‚¹ã‚¯ã®ã¿ï¼‰
        durations = self.worker_durations[worker_id]
        if durations:
            average_duration = statistics.mean(durations)
            median_duration = statistics.median(durations)
            p95_duration = self._calculate_percentile(durations, 95)
        else:
            average_duration = median_duration = p95_duration = 0
        
        # ãƒ¬ãƒ¼ãƒˆã¨ã‚³ã‚¹ãƒˆ
        total_tokens = self.worker_tokens[worker_id]
        total_cost = self.worker_costs[worker_id]
        error_rate = (failed_tasks / total_tasks * 100) if total_tasks > 0 else 0
        
        # ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆè¨ˆç®—ï¼ˆæœ€è¿‘1æ™‚é–“ï¼‰
        now = datetime.now()
        recent_tasks = [
            t for t in worker_tasks 
            if t.created_at > now - timedelta(hours=1)
        ]
        throughput = len(recent_tasks)  # tasks per hour
        
        # ä½¿ç”¨ç‡è¨ˆç®—ï¼ˆç°¡ç•¥åŒ–ï¼‰
        utilization_rate = min(100, throughput * 2)  # ä»®ã®è¨ˆç®—
        
        # æœ€æ–°ã‚¿ã‚¹ã‚¯ã‹ã‚‰åŸºæœ¬æƒ…å ±ã‚’å–å¾—
        latest_task = max(worker_tasks, key=lambda t: t.created_at)
        
        return WorkerMetrics(
            worker_id=worker_id,
            worker_type=latest_task.task_type,  # ç°¡ç•¥åŒ–
            model="unknown",  # å®Ÿéš›ã®å®Ÿè£…ã§ã¯è¨­å®šã‹ã‚‰å–å¾—
            total_tasks=total_tasks,
            successful_tasks=successful_tasks,
            failed_tasks=failed_tasks,
            average_duration=average_duration,
            median_duration=median_duration,
            p95_duration=p95_duration,
            total_tokens=total_tokens,
            total_cost=total_cost,
            utilization_rate=utilization_rate,
            error_rate=error_rate,
            throughput=throughput
        )
    
    def _calculate_percentile(self, data: List[float], percentile: int) -> float:
        """ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«è¨ˆç®—"""
        if not data:
            return 0
        sorted_data = sorted(data)
        index = int(len(sorted_data) * percentile / 100)
        return sorted_data[min(index, len(sorted_data) - 1)]
    
    def get_system_overview(self) -> Dict[str, Any]:
        """ã‚·ã‚¹ãƒ†ãƒ æ¦‚è¦ã‚’å–å¾—"""
        if not self.system_metrics:
            return {}
        
        latest = self.system_metrics[-1]
        
        # 24æ™‚é–“ã®ãƒˆãƒ¬ãƒ³ãƒ‰
        last_24h = [m for m in self.system_metrics if m.timestamp > datetime.now() - timedelta(hours=24)]
        
        return {
            'current': asdict(latest),
            'trends': {
                'avg_latency': statistics.mean([m.average_latency for m in last_24h]) if last_24h else 0,
                'avg_throughput': statistics.mean([m.throughput for m in last_24h]) if last_24h else 0,
                'avg_error_rate': statistics.mean([m.error_rate for m in last_24h]) if last_24h else 0,
            },
            'peak_usage': {
                'max_throughput': max([m.throughput for m in last_24h]) if last_24h else 0,
                'max_latency': max([m.average_latency for m in last_24h]) if last_24h else 0,
            }
        }


class AdvancedAnalyticsService:
    """é«˜åº¦ãªåˆ†æã‚µãƒ¼ãƒ“ã‚¹"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.metrics_collector = MetricsCollector(config)
        self.report_generator = ReportGenerator(config)
        
        # åˆ†æã‚¨ãƒ³ã‚¸ãƒ³
        self.anomaly_detector = AnomalyDetector(config)
        self.cost_optimizer = CostOptimizer(config)
        self.performance_analyzer = PerformanceAnalyzer(config)
        
        # å®šæœŸå®Ÿè¡Œã‚¿ã‚¹ã‚¯
        self.analysis_tasks = []
    
    async def initialize(self):
        """ã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–"""
        logger.info("ğŸ“Š Initializing Advanced Analytics Service...")
        
        # å®šæœŸåˆ†æã‚¿ã‚¹ã‚¯ã‚’é–‹å§‹
        self.analysis_tasks = [
            asyncio.create_task(self._periodic_analysis()),
            asyncio.create_task(self._anomaly_detection_loop()),
            asyncio.create_task(self._cost_analysis_loop()),
        ]
        
        logger.info("âœ… Advanced Analytics Service initialized")
    
    async def shutdown(self):
        """ã‚µãƒ¼ãƒ“ã‚¹çµ‚äº†"""
        for task in self.analysis_tasks:
            task.cancel()
        
        await asyncio.gather(*self.analysis_tasks, return_exceptions=True)
    
    async def record_task_completion(self, task_data: Dict[str, Any]):
        """ã‚¿ã‚¹ã‚¯å®Œäº†ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨˜éŒ²"""
        metrics = TaskMetrics(
            task_id=task_data['id'],
            worker_id=task_data['worker_id'],
            task_type=task_data['type'],
            status=task_data['status'],
            created_at=datetime.fromisoformat(task_data['created_at']),
            started_at=datetime.fromisoformat(task_data['started_at']) if task_data.get('started_at') else None,
            completed_at=datetime.fromisoformat(task_data['completed_at']) if task_data.get('completed_at') else None,
            duration_ms=task_data.get('duration_ms'),
            tokens_used=task_data.get('tokens_used'),
            cost=task_data.get('cost'),
            success=task_data['status'] == 'completed',
            error_type=task_data.get('error_type'),
            priority=task_data.get('priority', 'medium')
        )
        
        await self.metrics_collector.record_task_metrics(metrics)
    
    async def record_system_status(self, status_data: Dict[str, Any]):
        """ã‚·ã‚¹ãƒ†ãƒ ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’è¨˜éŒ²"""
        metrics = SystemMetrics(
            timestamp=datetime.now(),
            total_requests=status_data['total_requests'],
            active_workers=status_data['active_workers'],
            total_workers=status_data['total_workers'],
            system_load=status_data.get('system_load', 0),
            memory_usage=status_data.get('memory_usage', 0),
            cpu_usage=status_data.get('cpu_usage', 0),
            error_rate=status_data.get('error_rate', 0),
            average_latency=status_data.get('average_latency', 0),
            throughput=status_data.get('throughput', 0)
        )
        
        await self.metrics_collector.record_system_metrics(metrics)
    
    async def generate_performance_report(self, time_range: str = '24h') -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        logger.info(f"ğŸ“ˆ Generating performance report for {time_range}")
        
        system_overview = self.metrics_collector.get_system_overview()
        
        # Workeråˆ¥åˆ†æ
        worker_analyses = {}
        unique_workers = set(m.worker_id for m in self.metrics_collector.task_metrics)
        
        for worker_id in unique_workers:
            worker_metrics = self.metrics_collector.calculate_worker_metrics(worker_id)
            if worker_metrics:
                worker_analyses[worker_id] = asdict(worker_metrics)
        
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ
        performance_insights = await self.performance_analyzer.analyze_performance()
        
        # ã‚³ã‚¹ãƒˆåˆ†æ
        cost_analysis = await self.cost_optimizer.analyze_costs()
        
        # ç•°å¸¸æ¤œçŸ¥çµæœ
        anomalies = await self.anomaly_detector.get_recent_anomalies()
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'time_range': time_range,
            'system_overview': system_overview,
            'worker_analysis': worker_analyses,
            'performance_insights': performance_insights,
            'cost_analysis': cost_analysis,
            'anomalies': anomalies,
            'recommendations': await self._generate_recommendations(
                worker_analyses, performance_insights, cost_analysis, anomalies
            )
        }
        
        return report
    
    async def _generate_recommendations(self, worker_analysis, performance_insights, cost_analysis, anomalies) -> List[Dict[str, Any]]:
        """æ”¹å–„ææ¡ˆã‚’ç”Ÿæˆ"""
        recommendations = []
        
        # WorkeråŠ¹ç‡ã®æ”¹å–„ææ¡ˆ
        for worker_id, metrics in worker_analysis.items():
            if metrics['error_rate'] > 5:
                recommendations.append({
                    'type': 'performance',
                    'priority': 'high',
                    'title': f'Worker {worker_id}ã®ã‚¨ãƒ©ãƒ¼ç‡ãŒé«˜ã„',
                    'description': f'ã‚¨ãƒ©ãƒ¼ç‡ {metrics["error_rate"]:.1f}% - è¨­å®šã‚„ãƒ¢ãƒ‡ãƒ«ã®è¦‹ç›´ã—ã‚’æ¨å¥¨',
                    'impact': 'reliability'
                })
            
            if metrics['average_duration'] > 5000:  # 5ç§’ä»¥ä¸Š
                recommendations.append({
                    'type': 'performance',
                    'priority': 'medium',
                    'title': f'Worker {worker_id}ã®å¿œç­”æ™‚é–“ãŒé•·ã„',
                    'description': f'å¹³å‡å¿œç­”æ™‚é–“ {metrics["average_duration"]:.0f}ms - ãƒªã‚½ãƒ¼ã‚¹å¢—å¼·ã‚’æ¤œè¨',
                    'impact': 'latency'
                })
        
        # ã‚³ã‚¹ãƒˆæœ€é©åŒ–ææ¡ˆ
        if cost_analysis.get('monthly_projection', 0) > 1000:
            recommendations.append({
                'type': 'cost',
                'priority': 'medium',
                'title': 'ã‚³ã‚¹ãƒˆæœ€é©åŒ–ã®æ©Ÿä¼š',
                'description': f'æœˆé–“äºˆæƒ³ã‚³ã‚¹ãƒˆ ${cost_analysis["monthly_projection"]:.2f} - ã‚ˆã‚ŠåŠ¹ç‡çš„ãªãƒ¢ãƒ‡ãƒ«ã®æ¤œè¨ã‚’æ¨å¥¨',
                'impact': 'cost'
            })
        
        # ç•°å¸¸æ¤œçŸ¥ã«ã‚ˆã‚‹ææ¡ˆ
        if anomalies:
            recommendations.append({
                'type': 'alert',
                'priority': 'high',
                'title': f'{len(anomalies)}ä»¶ã®ç•°å¸¸ã‚’æ¤œå‡º',
                'description': 'è©³ç´°ãªèª¿æŸ»ã¨å¯¾å¿œãŒå¿…è¦ã§ã™',
                'impact': 'stability'
            })
        
        return recommendations
    
    async def _periodic_analysis(self):
        """å®šæœŸåˆ†æã‚¿ã‚¹ã‚¯"""
        while True:
            try:
                await asyncio.sleep(3600)  # 1æ™‚é–“ã”ã¨
                
                # å®šæœŸãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
                report = await self.generate_performance_report()
                
                # ãƒ¬ãƒãƒ¼ãƒˆã‚’ä¿å­˜
                await self.report_generator.save_report(report)
                
                logger.info("ğŸ“Š Periodic analysis completed")
                
            except Exception as e:
                logger.error(f"Periodic analysis error: {e}")
    
    async def _anomaly_detection_loop(self):
        """ç•°å¸¸æ¤œçŸ¥ãƒ«ãƒ¼ãƒ—"""
        while True:
            try:
                await asyncio.sleep(300)  # 5åˆ†ã”ã¨
                
                await self.anomaly_detector.detect_anomalies(self.metrics_collector)
                
            except Exception as e:
                logger.error(f"Anomaly detection error: {e}")
    
    async def _cost_analysis_loop(self):
        """ã‚³ã‚¹ãƒˆåˆ†æãƒ«ãƒ¼ãƒ—"""
        while True:
            try:
                await asyncio.sleep(1800)  # 30åˆ†ã”ã¨
                
                await self.cost_optimizer.analyze_usage_patterns(self.metrics_collector)
                
            except Exception as e:
                logger.error(f"Cost analysis error: {e}")


class AnomalyDetector:
    """ç•°å¸¸æ¤œçŸ¥ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.anomalies = deque(maxlen=100)
        
        # é–¾å€¤è¨­å®š
        self.thresholds = {
            'error_rate': 10.0,  # 10%ä»¥ä¸Š
            'latency_p95': 10000,  # 10ç§’ä»¥ä¸Š
            'throughput_drop': 0.5,  # 50%ä»¥ä¸Šã®ä½ä¸‹
        }
    
    async def detect_anomalies(self, metrics_collector: MetricsCollector):
        """ç•°å¸¸ã‚’æ¤œçŸ¥"""
        now = datetime.now()
        
        # æœ€è¿‘ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹åˆ†æ
        recent_tasks = [
            m for m in metrics_collector.task_metrics 
            if m.created_at > now - timedelta(minutes=30)
        ]
        
        if not recent_tasks:
            return
        
        # ã‚¨ãƒ©ãƒ¼ç‡ãƒã‚§ãƒƒã‚¯
        error_rate = sum(1 for t in recent_tasks if not t.success) / len(recent_tasks) * 100
        if error_rate > self.thresholds['error_rate']:
            await self._record_anomaly('high_error_rate', f'ã‚¨ãƒ©ãƒ¼ç‡ {error_rate:.1f}%', 'high')
        
        # ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒã‚§ãƒƒã‚¯
        durations = [t.duration_ms for t in recent_tasks if t.duration_ms]
        if durations:
            p95 = sorted(durations)[int(len(durations) * 0.95)]
            if p95 > self.thresholds['latency_p95']:
                await self._record_anomaly('high_latency', f'P95ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· {p95}ms', 'medium')
        
        logger.debug(f"Anomaly detection completed: {len(recent_tasks)} tasks analyzed")
    
    async def _record_anomaly(self, anomaly_type: str, description: str, severity: str):
        """ç•°å¸¸ã‚’è¨˜éŒ²"""
        anomaly = {
            'type': anomaly_type,
            'description': description,
            'severity': severity,
            'timestamp': datetime.now().isoformat(),
            'id': f"anomaly_{int(datetime.now().timestamp())}"
        }
        
        self.anomalies.append(anomaly)
        logger.warning(f"ğŸš¨ Anomaly detected: {description}")
    
    async def get_recent_anomalies(self, hours: int = 24) -> List[Dict[str, Any]]:
        """æœ€è¿‘ã®ç•°å¸¸ã‚’å–å¾—"""
        cutoff = datetime.now() - timedelta(hours=hours)
        return [
            a for a in self.anomalies 
            if datetime.fromisoformat(a['timestamp']) > cutoff
        ]


class CostOptimizer:
    """ã‚³ã‚¹ãƒˆæœ€é©åŒ–ã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.usage_patterns = {}
        
        # ãƒ¢ãƒ‡ãƒ«åˆ¥ã‚³ã‚¹ãƒˆæƒ…å ±
        self.model_costs = {
            'gpt-4-turbo': {'input': 0.01, 'output': 0.03},
            'gpt-4': {'input': 0.03, 'output': 0.06},
            'claude-3-sonnet': {'input': 0.003, 'output': 0.015},
            'gemini-1.5-flash': {'input': 0.00075, 'output': 0.003},
        }
    
    async def analyze_costs(self) -> Dict[str, Any]:
        """ã‚³ã‚¹ãƒˆåˆ†æ"""
        # ãƒ€ãƒŸãƒ¼å®Ÿè£…
        return {
            'daily_cost': 45.67,
            'monthly_projection': 1370.10,
            'cost_by_model': {
                'gpt-4-turbo': 18.45,
                'claude-3-sonnet': 15.23,
                'gemini-1.5-flash': 11.99
            },
            'optimization_potential': 125.50,
            'recommendations': [
                'Claude-3-Sonnetã®ä½¿ç”¨ã‚’å¢—ã‚„ã™ã“ã¨ã§20%ã®ã‚³ã‚¹ãƒˆå‰Šæ¸›ãŒå¯èƒ½',
                'éé‡è¦ã‚¿ã‚¹ã‚¯ã§Gemini-1.5-Flashã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨'
            ]
        }
    
    async def analyze_usage_patterns(self, metrics_collector: MetricsCollector):
        """ä½¿ç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        # å®Ÿè£…ã¯ç°¡ç•¥åŒ–
        logger.debug("Cost usage pattern analysis completed")


class PerformanceAnalyzer:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æã‚¨ãƒ³ã‚¸ãƒ³"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
    
    async def analyze_performance(self) -> Dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ"""
        # ãƒ€ãƒŸãƒ¼å®Ÿè£…
        return {
            'overall_score': 85,
            'bottlenecks': [
                {'component': 'Review Worker', 'impact': 'medium', 'description': 'ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ãŒå¹³å‡ã‚ˆã‚Šé•·ã„'},
                {'component': 'Memory Sync', 'impact': 'low', 'description': 'åŒæœŸé »åº¦ã‚’æœ€é©åŒ–å¯èƒ½'}
            ],
            'efficiency_metrics': {
                'resource_utilization': 78,
                'task_success_rate': 98.5,
                'average_queue_time': 125
            },
            'improvement_areas': [
                'Workeré–“ã®è² è·ãƒãƒ©ãƒ³ã‚¹èª¿æ•´',
                'ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥ã®æœ€é©åŒ–',
                'ä¸¦åˆ—å‡¦ç†ã®æ”¹å–„'
            ]
        }


class ReportGenerator:
    """ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.reports_dir = config.get('reportsDir', './reports')
    
    async def save_report(self, report: Dict[str, Any]):
        """ãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"{self.reports_dir}/performance_report_{timestamp}.json"
        
        try:
            async with aiofiles.open(filename, 'w') as f:
                await f.write(json.dumps(report, indent=2, ensure_ascii=False))
            
            logger.info(f"ğŸ“„ Report saved: {filename}")
            
        except Exception as e:
            logger.error(f"Failed to save report: {e}")


# ä½¿ç”¨ä¾‹
async def main():
    config = {
        'reportsDir': './reports',
        'anomalyThresholds': {
            'error_rate': 5.0,
            'latency_p95': 5000
        }
    }
    
    analytics = AdvancedAnalyticsService(config)
    await analytics.initialize()
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿è¨˜éŒ²
    await analytics.record_task_completion({
        'id': 'test-001',
        'worker_id': 'worker-001',
        'type': 'code_generation',
        'status': 'completed',
        'created_at': datetime.now().isoformat(),
        'started_at': datetime.now().isoformat(),
        'completed_at': datetime.now().isoformat(),
        'duration_ms': 1500,
        'tokens_used': 250,
        'cost': 0.05,
        'priority': 'medium'
    })
    
    # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = await analytics.generate_performance_report('1h')
    print(f"ğŸ“Š Generated report with {len(report['worker_analysis'])} workers analyzed")
    
    await analytics.shutdown()


if __name__ == "__main__":
    asyncio.run(main())