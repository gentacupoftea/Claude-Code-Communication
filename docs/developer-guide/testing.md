# Testing Guide

This guide covers the comprehensive testing strategy for Shopify MCP Server, including our adaptive testing approach that works across different environments.

## Overview

Shopify MCP Server uses an adaptive testing framework that automatically adjusts to available dependencies, ensuring tests run successfully in various environments.

## Dependency Categories

### 1. Core Dependencies (Required)
```
requests==2.31.0
python-dotenv==1.0.0
urllib3==2.0.0
backoff==2.2.1
```
These are essential for basic functionality.

### 2. Extended Dependencies (Recommended)
```
pandas==2.0.0
matplotlib==3.7.0
gql==3.5.0
requests-toolbelt~=1.0.0
requests-ratelimiter==0.7.0
```
These enable full feature set including data analysis and GraphQL.

### 3. Optional Dependencies
```
mcp==1.9.0  # MCP framework (platform-specific)
numpy==1.24.0  # Performance optimization
```
These provide additional capabilities but aren't required.

## Testing Tools

### Environment Check
```bash
python test_environment_check.py
```
This tool:
- Checks all dependency categories
- Reports available features
- Generates environment report
- Provides installation recommendations

### Adaptive Test Runner
```bash
python run_adaptive_tests.py
```
This tool:
- Automatically detects available dependencies
- Runs only compatible test suites
- Provides detailed feedback on skipped tests
- Generates comprehensive test reports

### Traditional Test Runner
```bash
./run_tests.sh
```
For environments with all dependencies installed.

## Test Categories

### 1. Core Tests
- `test_imports.py`: Basic import verification
- Requires: Core dependencies only

### 2. API Tests
- `test_server.py`: REST API functionality
- Requires: Core dependencies + backoff

### 3. GraphQL Tests
- `test_graphql_client.py`: GraphQL client tests
- `test_graphql_integration_simple.py`: Basic integration
- Requires: gql, requests-toolbelt

### 4. Data Tests
- `test_optimization.py`: Data optimization
- `test_cache_performance.py`: Cache performance
- Requires: pandas, numpy

### 5. Full Integration Tests
- `test_graphql_integration.py`: Complete integration
- Requires: All dependencies including MCP

## Environment Setup

### Quick Setup
```bash
./setup_adaptive_env.sh
```
Interactive script that:
1. Detects Python version
2. Creates virtual environment
3. Offers installation levels (minimal/standard/full)
4. Runs environment check

### Manual Setup

#### Minimal Environment
```bash
pip install -r requirements-base.txt
```

#### Standard Environment
```bash
pip install -r requirements-extended.txt
```

#### Full Environment
```bash
pip install -r requirements-extended.txt
pip install -r requirements-optional.txt
```

## CI/CD Integration

### GitHub Actions Matrix
Our CI/CD uses matrix builds to test across:
- Operating Systems: Ubuntu, Windows, macOS
- Python Versions: 3.8, 3.9, 3.10, 3.11, 3.12
- Dependency Levels: minimal, standard, full

### Local CI Simulation
```bash
# Test minimal environment
pip install -r requirements-base.txt
python run_adaptive_tests.py

# Test standard environment
pip install -r requirements-extended.txt
python run_adaptive_tests.py

# Test full environment
pip install -r requirements-extended.txt
pip install -r requirements-optional.txt
python run_adaptive_tests.py
```

## Test Reports

### Environment Report
Generated by `test_environment_check.py`:
```json
{
  "environment": {
    "python_version": "3.12.0",
    "platform": "darwin"
  },
  "dependencies": {
    "core": {...},
    "extended": {...},
    "optional": {...},
    "features": {...}
  },
  "recommendations": [...]
}
```

### Test Report
Generated by `run_adaptive_tests.py`:
```json
{
  "environment": {...},
  "test_results": {...},
  "success": true
}
```

## Troubleshooting

### Common Issues

1. **Import Errors**
   - Run `python test_environment_check.py`
   - Install missing dependencies based on recommendations

2. **Test Failures**
   - Check test category requirements
   - Ensure compatible dependencies are installed

3. **Platform-Specific Issues**
   - MCP may not be available on all platforms
   - NumPy may have platform-specific requirements

### Debug Mode
```bash
# Verbose environment check
python test_environment_check.py --verbose

# Debug test execution
python run_adaptive_tests.py --debug
```

## Best Practices

### Adding New Tests

1. **Categorize Properly**
   - Identify required dependencies
   - Add to appropriate category in `run_adaptive_tests.py`

2. **Mock External Dependencies**
   ```python
   from unittest.mock import Mock, patch
   
   @patch('external_module.function')
   def test_with_mock(mock_func):
       mock_func.return_value = "mocked"
       # Test code here
   ```

3. **Environment Guards**
   ```python
   import pytest
   
   try:
       import optional_module
       HAS_OPTIONAL = True
   except ImportError:
       HAS_OPTIONAL = False
   
   @pytest.mark.skipif(not HAS_OPTIONAL, reason="Optional module not available")
   def test_optional_feature():
       # Test code here
   ```

### Writing Environment-Agnostic Tests

1. **Use Conditional Imports**
   ```python
   try:
       import pandas as pd
       HAS_PANDAS = True
   except ImportError:
       HAS_PANDAS = False
   ```

2. **Provide Fallbacks**
   ```python
   if HAS_PANDAS:
       data = pd.DataFrame(...)
   else:
       data = {"columns": [...], "data": [...]}
   ```

3. **Clear Error Messages**
   ```python
   if not HAS_REQUIRED_MODULE:
       raise ImportError(
           "This test requires 'module_name'. "
           "Install with: pip install module_name"
       )
   ```

## Coverage Reports

When coverage is available:
```bash
# Run with coverage
coverage run -m pytest
coverage report
coverage html

# View coverage report
open htmlcov/index.html
```

## Performance Testing

For performance-critical tests:
```python
import time
import pytest

@pytest.mark.performance
def test_performance():
    start = time.time()
    # Code to test
    duration = time.time() - start
    assert duration < 1.0  # Should complete in under 1 second
```

## Continuous Improvement

1. **Monitor CI Results**
   - Review matrix build results
   - Identify environment-specific failures

2. **Update Dependencies**
   - Regularly review and update versions
   - Test compatibility across environments

3. **Expand Test Coverage**
   - Add tests for new features
   - Improve existing test quality

## Resources

- [Python Testing Best Practices](https://docs.python-guide.org/writing/tests/)
- [pytest Documentation](https://docs.pytest.org/)
- [GitHub Actions Matrix Builds](https://docs.github.com/en/actions/using-jobs/using-a-matrix-for-your-jobs)