# Conea Integration Platform - ç›£è¦–ã‚¬ã‚¤ãƒ‰

## ç›®æ¬¡

1. [ç›£è¦–æˆ¦ç•¥](#ç›£è¦–æˆ¦ç•¥)
2. [ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç›£è¦–](#ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç›£è¦–)
3. [ãƒ­ã‚°ç®¡ç†](#ãƒ­ã‚°ç®¡ç†)
4. [åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°](#åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°)
5. [ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š](#ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š)
6. [ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰](#ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰)
7. [SLI/SLOè¨­å®š](#slisloè¨­å®š)
8. [ç›£è¦–ãƒ„ãƒ¼ãƒ«ã®ä½¿ã„æ–¹](#ç›£è¦–ãƒ„ãƒ¼ãƒ«ã®ä½¿ã„æ–¹)

## ç›£è¦–æˆ¦ç•¥

### 4ã¤ã®ã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ã‚·ã‚°ãƒŠãƒ«

Coneaã®ç›£è¦–ã¯ã€Google SREã®ã€Œ4ã¤ã®ã‚´ãƒ¼ãƒ«ãƒ‡ãƒ³ã‚·ã‚°ãƒŠãƒ«ã€ã«åŸºã¥ã„ã¦ã„ã¾ã™ï¼š

1. **ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·**: ãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å¿œç­”æ™‚é–“
2. **ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯**: ã‚·ã‚¹ãƒ†ãƒ ã¸ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆé‡
3. **ã‚¨ãƒ©ãƒ¼**: å¤±æ•—ã—ãŸãƒªã‚¯ã‚¨ã‚¹ãƒˆã®å‰²åˆ
4. **é£½å’Œåº¦**: ãƒªã‚½ãƒ¼ã‚¹ã®ä½¿ç”¨ç‡

### ç›£è¦–ãƒ¬ã‚¤ãƒ¤ãƒ¼

```mermaid
graph TB
    subgraph "Application Layer"
        APP_METRICS[Application Metrics]
        APP_LOGS[Application Logs]
        APP_TRACES[Traces]
    end
    
    subgraph "Infrastructure Layer"
        INFRA_METRICS[System Metrics]
        INFRA_LOGS[System Logs]
        NETWORK[Network Metrics]
    end
    
    subgraph "Business Layer"
        BIZ_METRICS[Business KPIs]
        USER_ANALYTICS[User Analytics]
    end
    
    subgraph "Monitoring Stack"
        PROMETHEUS[Prometheus]
        LOKI[Loki]
        JAEGER[Jaeger]
        GRAFANA[Grafana]
    end
    
    APP_METRICS --> PROMETHEUS
    INFRA_METRICS --> PROMETHEUS
    BIZ_METRICS --> PROMETHEUS
    
    APP_LOGS --> LOKI
    INFRA_LOGS --> LOKI
    
    APP_TRACES --> JAEGER
    
    PROMETHEUS --> GRAFANA
    LOKI --> GRAFANA
    JAEGER --> GRAFANA
    USER_ANALYTICS --> GRAFANA
```

## ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç›£è¦–

### Prometheusãƒ¡ãƒˆãƒªã‚¯ã‚¹

#### ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹

```yaml
# ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å®šç¾©
metrics:
  # HTTPãƒªã‚¯ã‚¨ã‚¹ãƒˆ
  - name: http_requests_total
    type: counter
    help: Total number of HTTP requests
    labels: [method, endpoint, status]
  
  # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¿ã‚¤ãƒ 
  - name: http_request_duration_seconds
    type: histogram
    help: HTTP request latency
    labels: [method, endpoint]
    buckets: [0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2.5, 5, 10]
  
  # AIãƒ¢ãƒ‡ãƒ«ä½¿ç”¨é‡
  - name: ai_model_requests_total
    type: counter
    help: Total AI model requests
    labels: [model, provider, status]
  
  # ãƒ‡ãƒ¼ã‚¿å‡¦ç†
  - name: data_processing_jobs_total
    type: counter
    help: Total data processing jobs
    labels: [job_type, status]
```

#### ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†è¨­å®š

```yaml
# prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  # ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
  - job_name: 'conea-api'
    kubernetes_sd_configs:
      - role: pod
        namespaces:
          names: ['production']
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_label_app]
        action: keep
        regex: conea-api
      - source_labels: [__meta_kubernetes_pod_name]
        target_label: pod
      - source_labels: [__meta_kubernetes_namespace]
        target_label: namespace
  
  # Kubernetes
  - job_name: 'kubernetes-nodes'
    kubernetes_sd_configs:
      - role: node
    relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
  
  # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
  - job_name: 'postgresql'
    static_configs:
      - targets: ['postgres-exporter:9187']
```

### ä¸»è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹

#### ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹

```promql
# CPUä½¿ç”¨ç‡
100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)

# ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡
(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100

# ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨ç‡
(node_filesystem_size_bytes - node_filesystem_free_bytes) / node_filesystem_size_bytes * 100

# ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯
rate(node_network_receive_bytes_total[5m])
rate(node_network_transmit_bytes_total[5m])
```

#### ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¡ãƒˆãƒªã‚¯ã‚¹

```promql
# ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¬ãƒ¼ãƒˆ
sum(rate(http_requests_total[5m])) by (endpoint)

# ã‚¨ãƒ©ãƒ¼ãƒ¬ãƒ¼ãƒˆ
sum(rate(http_requests_total{status=~"5.."}[5m])) / sum(rate(http_requests_total[5m]))

# ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¿ã‚¤ãƒ ï¼ˆp95ï¼‰
histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le, endpoint))

# åŒæ™‚æ¥ç¶šæ•°
sum(tcp_connections{state="established"})
```

## ãƒ­ã‚°ç®¡ç†

### ãƒ­ã‚°åé›†è¨­å®š

```yaml
# fluent-bit-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluent-bit-config
data:
  fluent-bit.conf: |
    [SERVICE]
        Flush         1
        Log_Level     info
        Daemon        off
        Parsers_File  parsers.conf
    
    [INPUT]
        Name              tail
        Path              /var/log/containers/*.log
        Parser            docker
        Tag               kube.*
        Refresh_Interval  5
        Mem_Buf_Limit     50MB
        Skip_Long_Lines   On
    
    [FILTER]
        Name                kubernetes
        Match               kube.*
        Merge_Log           On
        Keep_Log            Off
        K8S-Logging.Parser  On
        K8S-Logging.Exclude On
    
    [OUTPUT]
        Name   loki
        Match  *
        Host   loki.monitoring.svc.cluster.local
        Port   3100
        Labels job=fluent-bit, app=$kubernetes['labels']['app']
```

### æ§‹é€ åŒ–ãƒ­ã‚°ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ

```json
{
  "timestamp": "2025-05-26T10:30:45.123Z",
  "level": "info",
  "service": "conea-api",
  "trace_id": "abc123def456",
  "span_id": "789ghi012",
  "user_id": "user_123",
  "organization_id": "org_456",
  "method": "POST",
  "path": "/api/v1/ai/chat",
  "status": 200,
  "duration_ms": 245,
  "message": "AI chat request completed",
  "metadata": {
    "model": "gpt-4",
    "tokens": 1250,
    "cost": 0.0375
  }
}
```

### ãƒ­ã‚°ã‚¯ã‚¨ãƒªä¾‹

```logql
# ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã®æ¤œç´¢
{app="conea-api"} |= "error" | json | level="error"

# ç‰¹å®šãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ­ã‚°
{app="conea-api"} | json | user_id="user_123"

# ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¿ã‚¤ãƒ ãŒé…ã„ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
{app="conea-api"} | json | duration_ms > 1000

# 5xxã‚¨ãƒ©ãƒ¼ã®é›†è¨ˆ
sum by (status) (
  rate({app="conea-api"} | json | status >= 500 [5m])
)
```

## åˆ†æ•£ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°

### Jaegerè¨­å®š

```yaml
# jaeger-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jaeger
spec:
  template:
    spec:
      containers:
      - name: jaeger
        image: jaegertracing/all-in-one:1.45
        env:
        - name: COLLECTOR_ZIPKIN_HOST_PORT
          value: ":9411"
        - name: SPAN_STORAGE_TYPE
          value: elasticsearch
        - name: ES_SERVER_URLS
          value: http://elasticsearch:9200
        ports:
        - containerPort: 5775
          protocol: UDP
        - containerPort: 6831
          protocol: UDP
        - containerPort: 6832
          protocol: UDP
        - containerPort: 5778
          protocol: TCP
        - containerPort: 16686
          protocol: TCP
        - containerPort: 14268
          protocol: TCP
        - containerPort: 9411
          protocol: TCP
```

### ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°å®Ÿè£…

```javascript
// ãƒˆãƒ¬ãƒ¼ã‚·ãƒ³ã‚°ã®åˆæœŸåŒ–
const { initTracer } = require('jaeger-client');
const opentracing = require('opentracing');

const config = {
  serviceName: 'conea-api',
  sampler: {
    type: 'probabilistic',
    param: 0.1, // 10%ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
  },
  reporter: {
    logSpans: true,
    agentHost: process.env.JAEGER_AGENT_HOST || 'localhost',
    agentPort: process.env.JAEGER_AGENT_PORT || 6831,
  },
};

const tracer = initTracer(config);

// ãƒŸãƒ‰ãƒ«ã‚¦ã‚§ã‚¢ã§ã®ä½¿ç”¨
app.use((req, res, next) => {
  const span = tracer.startSpan('http_request');
  span.setTag('http.method', req.method);
  span.setTag('http.url', req.url);
  
  req.span = span;
  
  res.on('finish', () => {
    span.setTag('http.status_code', res.statusCode);
    span.finish();
  });
  
  next();
});
```

## ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š

### ã‚¢ãƒ©ãƒ¼ãƒˆãƒ«ãƒ¼ãƒ«

```yaml
# alerts.yaml
groups:
  - name: conea_critical
    interval: 30s
    rules:
      # ã‚µãƒ¼ãƒ“ã‚¹ãƒ€ã‚¦ãƒ³
      - alert: ServiceDown
        expr: up{job="conea-api"} == 0
        for: 2m
        labels:
          severity: critical
          team: platform
        annotations:
          summary: "Service {{ $labels.instance }} is down"
          description: "{{ $labels.job }} has been down for more than 2 minutes"
          runbook_url: "https://docs.conea.io/runbooks/service-down"
      
      # é«˜ã‚¨ãƒ©ãƒ¼ãƒ¬ãƒ¼ãƒˆ
      - alert: HighErrorRate
        expr: |
          (
            sum(rate(http_requests_total{status=~"5.."}[5m]))
            /
            sum(rate(http_requests_total[5m]))
          ) > 0.05
        for: 5m
        labels:
          severity: critical
          team: platform
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} for the last 5 minutes"
      
      # é«˜ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¿ã‚¤ãƒ 
      - alert: HighLatency
        expr: |
          histogram_quantile(0.95,
            sum(rate(http_request_duration_seconds_bucket[5m])) by (le)
          ) > 1
        for: 10m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "High latency detected"
          description: "95th percentile latency is {{ $value }}s"
  
  - name: conea_resources
    interval: 60s
    rules:
      # CPUä½¿ç”¨ç‡
      - alert: HighCPUUsage
        expr: |
          (
            100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)
          ) > 80
        for: 15m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "High CPU usage on {{ $labels.instance }}"
          description: "CPU usage is {{ $value }}%"
      
      # ãƒ¡ãƒ¢ãƒªä½¿ç”¨ç‡
      - alert: HighMemoryUsage
        expr: |
          (
            (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100
          ) > 85
        for: 15m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "High memory usage on {{ $labels.instance }}"
          description: "Memory usage is {{ $value }}%"
      
      # ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨ç‡
      - alert: HighDiskUsage
        expr: |
          (
            (node_filesystem_size_bytes - node_filesystem_free_bytes)
            / node_filesystem_size_bytes * 100
          ) > 85
        for: 30m
        labels:
          severity: warning
          team: platform
        annotations:
          summary: "High disk usage on {{ $labels.instance }}"
          description: "Disk usage is {{ $value }}% on {{ $labels.device }}"
```

### ã‚¢ãƒ©ãƒ¼ãƒˆé€šçŸ¥è¨­å®š

```yaml
# alertmanager.yml
global:
  resolve_timeout: 5m
  slack_api_url: 'YOUR_SLACK_WEBHOOK_URL'

route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'default'
  routes:
  - match:
      severity: critical
    receiver: 'critical'
    continue: true
  - match:
      severity: warning
    receiver: 'warning'

receivers:
- name: 'default'
  slack_configs:
  - channel: '#alerts'
    title: 'Conea Alert'
    text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'

- name: 'critical'
  slack_configs:
  - channel: '#alerts-critical'
    title: 'ğŸš¨ CRITICAL: {{ .GroupLabels.alertname }}'
    text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
  pagerduty_configs:
  - service_key: 'YOUR_PAGERDUTY_KEY'

- name: 'warning'
  slack_configs:
  - channel: '#alerts-warning'
    title: 'âš ï¸ WARNING: {{ .GroupLabels.alertname }}'
    text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
```

## ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

### Grafanaãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­å®š

#### ã‚·ã‚¹ãƒ†ãƒ ã‚ªãƒ¼ãƒãƒ¼ãƒ“ãƒ¥ãƒ¼

```json
{
  "dashboard": {
    "title": "Conea System Overview",
    "panels": [
      {
        "title": "Request Rate",
        "targets": [
          {
            "expr": "sum(rate(http_requests_total[5m])) by (endpoint)"
          }
        ],
        "type": "graph"
      },
      {
        "title": "Error Rate",
        "targets": [
          {
            "expr": "sum(rate(http_requests_total{status=~\"5..\"}[5m])) / sum(rate(http_requests_total[5m])) * 100"
          }
        ],
        "type": "singlestat",
        "thresholds": "1,5",
        "colors": ["green", "yellow", "red"]
      },
      {
        "title": "Response Time (p95)",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (le))"
          }
        ],
        "type": "graph"
      },
      {
        "title": "Active Users",
        "targets": [
          {
            "expr": "sum(increase(user_activity_total[5m]))"
          }
        ],
        "type": "singlestat"
      }
    ]
  }
}
```

### ã‚«ã‚¹ã‚¿ãƒ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰

#### AIãƒ¢ãƒ‡ãƒ«ä½¿ç”¨çŠ¶æ³

```json
{
  "panels": [
    {
      "title": "AI Model Requests by Provider",
      "targets": [
        {
          "expr": "sum(rate(ai_model_requests_total[5m])) by (provider, model)"
        }
      ],
      "type": "piechart"
    },
    {
      "title": "AI Processing Time",
      "targets": [
        {
          "expr": "histogram_quantile(0.95, sum(rate(ai_processing_duration_seconds_bucket[5m])) by (le, model))"
        }
      ],
      "type": "graph"
    },
    {
      "title": "Token Usage",
      "targets": [
        {
          "expr": "sum(increase(ai_tokens_total[1h])) by (model)"
        }
      ],
      "type": "bargauge"
    }
  ]
}
```

## SLI/SLOè¨­å®š

### ã‚µãƒ¼ãƒ“ã‚¹ãƒ¬ãƒ™ãƒ«æŒ‡æ¨™ï¼ˆSLIï¼‰

```yaml
slis:
  # å¯ç”¨æ€§
  availability:
    metric: |
      sum(rate(http_requests_total{status!~"5.."}[5m]))
      /
      sum(rate(http_requests_total[5m]))
    
  # ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·
  latency:
    metric: |
      histogram_quantile(0.95,
        sum(rate(http_request_duration_seconds_bucket[5m])) by (le)
      )
    
  # ã‚¨ãƒ©ãƒ¼ç‡
  error_rate:
    metric: |
      sum(rate(http_requests_total{status=~"5.."}[5m]))
      /
      sum(rate(http_requests_total[5m]))
```

### ã‚µãƒ¼ãƒ“ã‚¹ãƒ¬ãƒ™ãƒ«ç›®æ¨™ï¼ˆSLOï¼‰

```yaml
slos:
  - name: API Availability
    sli: availability
    target: 99.9
    window: 30d
    
  - name: API Latency
    sli: latency
    target_value: 0.5  # 500ms
    target_percentile: 95
    window: 30d
    
  - name: Error Budget
    sli: error_rate
    target: 0.1  # 0.1%
    window: 30d
```

### ã‚¨ãƒ©ãƒ¼ãƒã‚¸ã‚§ãƒƒãƒˆç›£è¦–

```promql
# æ®‹ã‚Šã‚¨ãƒ©ãƒ¼ãƒã‚¸ã‚§ãƒƒãƒˆï¼ˆ%ï¼‰
(
  1 - (
    sum(increase(http_requests_total{status=~"5.."}[30d]))
    /
    sum(increase(http_requests_total[30d]))
  )
) * 100 - 99.9

# ã‚¨ãƒ©ãƒ¼ãƒã‚¸ã‚§ãƒƒãƒˆæ¶ˆè²»ç‡
(
  sum(rate(http_requests_total{status=~"5.."}[1h]))
  /
  sum(rate(http_requests_total[1h]))
) / 0.001  # 0.1% ã‚¨ãƒ©ãƒ¼ãƒã‚¸ã‚§ãƒƒãƒˆ
```

## ç›£è¦–ãƒ„ãƒ¼ãƒ«ã®ä½¿ã„æ–¹

### Prometheus ã‚¯ã‚¨ãƒª

```bash
# CLIã§ã®ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
promtool query instant http://prometheus:9090 'up{job="conea-api"}'

# ç¯„å›²ã‚¯ã‚¨ãƒª
curl -G http://prometheus:9090/api/v1/query_range \
  --data-urlencode 'query=rate(http_requests_total[5m])' \
  --data-urlencode 'start=2025-05-26T00:00:00Z' \
  --data-urlencode 'end=2025-05-26T12:00:00Z' \
  --data-urlencode 'step=15s'
```

### Grafana Loki

```bash
# ãƒ­ã‚°ã®æ¤œç´¢
logcli query '{app="conea-api"} |= "error"' --limit=100

# ãƒ­ã‚°ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
logcli query '{app="conea-api"}' --tail

# çµ±è¨ˆæƒ…å ±
logcli series '{app="conea-api"}'
```

### Jaeger ãƒˆãƒ¬ãƒ¼ã‚¹

```bash
# ãƒˆãƒ¬ãƒ¼ã‚¹ã®æ¤œç´¢
curl "http://jaeger:16686/api/traces?service=conea-api&limit=20"

# ç‰¹å®šã®ãƒˆãƒ¬ãƒ¼ã‚¹ID
curl "http://jaeger:16686/api/traces/abc123def456"

# ã‚µãƒ¼ãƒ“ã‚¹ä¾å­˜é–¢ä¿‚
curl "http://jaeger:16686/api/dependencies?endTs=$(date +%s)000"
```

### ç›£è¦–ã®è‡ªå‹•åŒ–

```python
# monitoring_automation.py
import requests
import json
from datetime import datetime, timedelta

class MonitoringAutomation:
    def __init__(self, prometheus_url, alertmanager_url):
        self.prometheus_url = prometheus_url
        self.alertmanager_url = alertmanager_url
    
    def check_slo_compliance(self, slo_name, target, window_days=30):
        """SLOã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ã‚’ãƒã‚§ãƒƒã‚¯"""
        end_time = datetime.now()
        start_time = end_time - timedelta(days=window_days)
        
        query = f'''
            avg_over_time(
                (sum(rate(http_requests_total{{status!~"5.."}}[5m])) 
                / 
                sum(rate(http_requests_total[5m])))[{window_days}d:]
            ) * 100
        '''
        
        response = requests.get(
            f"{self.prometheus_url}/api/v1/query",
            params={'query': query}
        )
        
        result = response.json()
        current_sli = float(result['data']['result'][0]['value'][1])
        
        return {
            'slo_name': slo_name,
            'target': target,
            'current': current_sli,
            'compliant': current_sli >= target,
            'margin': current_sli - target
        }
    
    def generate_capacity_report(self):
        """ã‚­ãƒ£ãƒ‘ã‚·ãƒ†ã‚£ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""
        metrics = [
            ('CPU Usage', 'avg(100 - (rate(node_cpu_seconds_total{mode="idle"}[5m]) * 100))'),
            ('Memory Usage', 'avg((1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100)'),
            ('Disk Usage', 'avg((node_filesystem_size_bytes - node_filesystem_free_bytes) / node_filesystem_size_bytes * 100)')
        ]
        
        report = {}
        for name, query in metrics:
            response = requests.get(
                f"{self.prometheus_url}/api/v1/query",
                params={'query': query}
            )
            result = response.json()
            report[name] = float(result['data']['result'][0]['value'][1])
        
        return report

# ä½¿ç”¨ä¾‹
monitor = MonitoringAutomation(
    prometheus_url="http://prometheus:9090",
    alertmanager_url="http://alertmanager:9093"
)

# SLOãƒã‚§ãƒƒã‚¯
slo_result = monitor.check_slo_compliance("API Availability", 99.9)
print(f"SLO Compliance: {slo_result}")

# ã‚­ãƒ£ãƒ‘ã‚·ãƒ†ã‚£ãƒ¬ãƒãƒ¼ãƒˆ
capacity = monitor.generate_capacity_report()
print(f"Capacity Report: {capacity}")
```

---

æœ€çµ‚æ›´æ–°æ—¥: 2025å¹´5æœˆ26æ—¥