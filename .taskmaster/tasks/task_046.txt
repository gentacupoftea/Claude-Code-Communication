# Task ID: 46
# Title: ローカルLLM高度活用システムの構築 - Phase 2
# Status: pending
# Dependencies: 45
# Priority: medium
# Description: Mac Studioなどの高性能マシンを活用し、CrewAIによるマルチエージェント連携、完全ローカルRAGシステム、フロントエンドでのローカルLLMサポート強化を実装する。
# Details:
このタスクでは、Phase 1で構築したローカルLLM基盤を拡張し、より高度な機能を実装します。

1. **CrewAIによるマルチエージェント連携**
   - CrewAIフレームワークをプロジェクトに統合
   - 複数の特化型エージェントを定義（リサーチャー、コーダー、レビュワーなど）
   - エージェント間の協調作業フローを設計・実装
   - タスク分割と結果統合のメカニズムを構築

2. **完全ローカルRAGシステムの構築**
   - ローカルベクトルデータベース（Chroma、Qdrant等）の設定
   - 効率的なドキュメント処理パイプラインの実装
   - 埋め込みモデルの最適化（ONNX変換、量子化など）
   - クエリ最適化とコンテキスト管理の実装
   - ローカルRAGのパフォーマンス測定と調整

3. **フロントエンドでのローカルLLMサポート強化**
   - WebUIからのローカルモデル選択・切替機能
   - モデルパラメータ（温度、トップK/P等）の動的調整インターフェース
   - ストリーミング出力のUI実装
   - フロントエンドでのコンテキスト管理と会話履歴の保存

4. **パフォーマンス最適化**
   - Mac Studioのハードウェア（M1/M2チップ）に最適化
   - メモリ使用量の監視と制御
   - 並列処理の実装によるスループット向上
   - モデル量子化とキャッシュ戦略の実装

5. **システム統合**
   - 各コンポーネント間のAPIインターフェース設計
   - エラーハンドリングと回復メカニズムの実装
   - ログ記録と診断機能の追加
   - 設定管理システムの構築

# Test Strategy:
1. **機能テスト**
   - CrewAIマルチエージェントシステム
     - 複数エージェントが正しく初期化されることを確認
     - エージェント間の通信が期待通りに機能するか検証
     - 複雑なタスクを分割して処理できることを確認
     - 結果の統合が正確に行われるかテスト

   - ローカルRAGシステム
     - ドキュメント取り込みの正確性を検証
     - 検索精度の評価（関連性スコアの測定）
     - 回答生成の品質評価（人間評価を含む）
     - 大規模データセットでのスケーラビリティテスト

   - フロントエンドサポート
     - 異なるモデル間の切り替えテスト
     - パラメータ調整の効果を検証
     - ストリーミング出力の遅延測定
     - UIの応答性と使いやすさの評価

2. **パフォーマンステスト**
   - 処理速度の測定（トークン/秒）
   - メモリ使用量のプロファイリング
   - 並列処理の効率性評価
   - 長時間実行時の安定性テスト

3. **統合テスト**
   - エンドツーエンドのユースケースシナリオテスト
   - エラー条件下での動作検証
   - 設定変更の反映テスト
   - システム全体のリソース使用状況モニタリング

4. **ユーザー受け入れテスト**
   - 実際のユーザーによる機能評価
   - 使いやすさと直感性の評価
   - パフォーマンスに関するフィードバック収集
   - 改善点の特定と優先順位付け

# Subtasks:
## 1. CrewAI連携Workerの作成 [pending]
### Dependencies: None
### Description: 複数のローカルLLMエージェントを協調させて複雑なタスクを処理するため、`multiLLM_system/workers/crewai_worker.py` を新規作成する。`crewai`と`langchain_community`を利用し、タスク内容に応じて動的にエージェントとタスクを定義し、Crewを実行するロジックを実装する。
### Details:


## 2. ローカル完結型RAGへの拡張 [pending]
### Dependencies: None
### Description: 既存のRAGシステムを、埋め込み生成も含めて完全にローカルで実行できるように拡張する。`multiLLM_system/services/rag_system/embeddings/embedding_service.py` を修正し、HuggingFaceの代わりにOllama経由でローカルの埋め込みモデル（例: `nomic-embed-text`）を呼び出すロジックを追加する。これにより、機密文書のオフラインでの取り扱いを可能にする。
### Details:


## 3. フロントエンドでのローカルLLM対応 [pending]
### Dependencies: None
### Description: UIからローカルLLMの稼働状況を確認し、直接タスクを指示できるようにする。`multiLLM_system/ui/src/pages/Workers.tsx`にローカルWorkerのステータス表示を追加し、`multiLLM_system/ui/src/components/ImprovedChat/ImprovedChatInterface.tsx`に「ローカルモード」を実装する。
### Details:


