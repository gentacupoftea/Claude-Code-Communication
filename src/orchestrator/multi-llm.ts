// ãƒãƒ«ãƒLLMã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼

import { Anthropic } from '@anthropic-ai/sdk';
import OpenAI from 'openai';
import { VertexAI } from '@google-cloud/aiplatform';
import { claudePrompts, selectClaudePrompt } from '../prompts/claude-prompts';
import { geminiPrompts } from '../prompts/gemini-prompts';
import { gpt4Prompts } from '../prompts/gpt4-prompts';

export type LLMProvider = 'claude' | 'gemini' | 'gpt-4';

interface LLMConfig {
  apiKey?: string;
  projectId?: string;
  location?: string;
  model?: string;
}

export class MultiLLMOrchestrator {
  private claude?: Anthropic;
  private openai?: OpenAI;
  private vertexAI?: VertexAI;
  private isTestMode: boolean = false;

  constructor(config?: {
    claude?: LLMConfig;
    openai?: LLMConfig;
    gemini?: LLMConfig;
    testMode?: boolean;
  }) {
    this.isTestMode = config?.testMode || false;

    if (!this.isTestMode) {
      // å®Ÿéš›ã®APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
      if (config?.claude?.apiKey) {
        this.claude = new Anthropic({
          apiKey: config.claude.apiKey
        });
      }

      if (config?.openai?.apiKey) {
        this.openai = new OpenAI({
          apiKey: config.openai.apiKey
        });
      }

      if (config?.gemini?.projectId) {
        // Gemini/Vertex AI ã®åˆæœŸåŒ–ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯é©åˆ‡ãªè¨­å®šãŒå¿…è¦ï¼‰
        // this.vertexAI = new VertexAI({...});
      }
    }
  }

  async query(
    question: string,
    context: any,
    provider: LLMProvider
  ): Promise<string> {
    if (this.isTestMode) {
      return this.generateMockResponse(question, context, provider);
    }

    switch (provider) {
      case 'claude':
        return this.queryAnthropic(question, context);
      case 'gpt-4':
        return this.queryOpenAI(question, context);
      case 'gemini':
        return this.queryGemini(question, context);
      default:
        throw new Error(`Unsupported provider: ${provider}`);
    }
  }

  private async queryAnthropic(question: string, context: any): Promise<string> {
    if (!this.claude) {
      throw new Error('Claude client not initialized');
    }

    const questionType = this.detectQuestionType(question);
    const promptTemplate = selectClaudePrompt(questionType);
    
    const systemPrompt = promptTemplate.systemPrompt;
    const userPrompt = `
${promptTemplate.contextBuilder(context)}

è³ªå•: ${question}

${promptTemplate.responseFormat}
`;

    try {
      const response = await this.claude.messages.create({
        model: 'claude-3-opus-20240229',
        max_tokens: 2000,
        temperature: 0.7,
        system: systemPrompt,
        messages: [
          {
            role: 'user',
            content: userPrompt
          }
        ]
      });

      return response.content[0].text;
    } catch (error) {
      console.error('Claude API error:', error);
      throw error;
    }
  }

  private async queryOpenAI(question: string, context: any): Promise<string> {
    if (!this.openai) {
      throw new Error('OpenAI client not initialized');
    }

    const questionType = this.detectQuestionType(question);
    const promptTemplate = gpt4Prompts[questionType] || gpt4Prompts.complexAnalysis;
    
    const systemMessage = promptTemplate.systemMessage;
    const userMessage = promptTemplate.userMessageBuilder(context);

    try {
      const response = await this.openai.chat.completions.create({
        model: 'gpt-4-turbo-preview',
        temperature: 0.7,
        max_tokens: 2000,
        messages: [
          { role: 'system', content: systemMessage },
          { role: 'user', content: `${userMessage}\\n\\nè³ªå•: ${question}` }
        ],
        functions: promptTemplate.functions
      });

      return response.choices[0].message.content || '';
    } catch (error) {
      console.error('OpenAI API error:', error);
      throw error;
    }
  }

  private async queryGemini(question: string, context: any): Promise<string> {
    // Gemini API ã®å®Ÿè£…ï¼ˆå®Ÿéš›ã®APIãŒåˆ©ç”¨å¯èƒ½ã«ãªã£ãŸã‚‰å®Ÿè£…ï¼‰
    if (!this.vertexAI) {
      throw new Error('Gemini client not initialized');
    }

    const questionType = this.detectQuestionType(question);
    const promptTemplate = geminiPrompts[questionType] || geminiPrompts.creativeProposal;

    // Gemini APIå‘¼ã³å‡ºã—ã®å®Ÿè£…
    // const response = await this.vertexAI.predict({...});
    
    return 'Gemini response placeholder';
  }

  private detectQuestionType(question: string): string {
    // ç°¡æ˜“çš„ãªè³ªå•ã‚¿ã‚¤ãƒ—æ¤œå‡º
    if (question.includes('åˆ†æ')) return 'dataAnalysis';
    if (question.includes('äºˆæ¸¬')) return 'demandForecast';
    if (question.includes('æœ€é©åŒ–')) return 'optimization';
    if (question.includes('æˆ¦ç•¥')) return 'strategicPlanning';
    if (question.includes('ææ¡ˆ') || question.includes('ã‚¢ã‚¤ãƒ‡ã‚¢')) return 'creativeProposal';
    return 'dataAnalysis';
  }

  // ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ¢ãƒƒã‚¯ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç”Ÿæˆ
  private generateMockResponse(
    question: string,
    context: any,
    provider: LLMProvider
  ): string {
    const mockResponses = {
      claude: `## ç¾çŠ¶åˆ†æ
${question}ã«é–¢ã™ã‚‹åˆ†æã‚’å®Ÿæ–½ã—ã¾ã—ãŸã€‚

## ä¸»è¦ãªç™ºè¦‹
1. å£²ä¸Šã¯å‰å¹´æ¯”15%å¢—åŠ ã—ã¦ãŠã‚Šã€ç‰¹ã«ç¬¬3å››åŠæœŸã®æˆé•·ãŒé¡•è‘—ã§ã™
2. é¡§å®¢ç²å¾—ã‚³ã‚¹ãƒˆï¼ˆCACï¼‰ã¯æ¥­ç•Œå¹³å‡ã‚’ä¸‹å›ã‚‹æ°´æº–ã§æ¨ç§»
3. ãƒªãƒ”ãƒ¼ãƒˆç‡ã¯35%ã§ã€æ¥­ç•Œå¹³å‡ï¼ˆ27%ï¼‰ã‚’ä¸Šå›ã£ã¦ã„ã¾ã™

## æ”¹å–„ææ¡ˆ
- ã‚«ãƒ¼ãƒˆæ”¾æ£„ç‡ã‚’ç¾åœ¨ã®70%ã‹ã‚‰60%ã«å‰Šæ¸›ã™ã‚‹ã“ã¨ã§ã€å£²ä¸Šã‚’8-10%å‘ä¸Šå¯èƒ½
- ãƒ‘ãƒ¼ã‚½ãƒŠãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³å¼·åŒ–ã«ã‚ˆã‚Šã€AOVã‚’15%å‘ä¸Šã•ã›ã‚‹ä½™åœ°ãŒã‚ã‚Šã¾ã™
- åœ¨åº«å›è»¢ç‡ã®æœ€é©åŒ–ã«ã‚ˆã‚Šã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼ã‚’20%æ”¹å–„ã§ãã¾ã™

## æœŸå¾…åŠ¹æœ
ã“ã‚Œã‚‰ã®æ–½ç­–ã«ã‚ˆã‚Šã€å¹´é–“å£²ä¸Šã‚’25-30%å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒå¯èƒ½ã¨æ¨å®šã•ã‚Œã¾ã™ã€‚
ROIã¯6ãƒ¶æœˆä»¥å†…ã«é”æˆè¦‹è¾¼ã¿ã§ã™ã€‚`,

      gemini: `ğŸš€ é©æ–°çš„ãªECã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ææ¡ˆ

### 1. AIãƒ‘ãƒ¼ã‚½ãƒŠãƒ«ã‚·ãƒ§ãƒƒãƒ‘ãƒ¼ä½“é¨“
é¡§å®¢ä¸€äººã²ã¨ã‚Šã«å°‚å±ã®AIã‚·ãƒ§ãƒƒãƒ‘ãƒ¼ã‚’æä¾›ã—ã€ã¾ã‚‹ã§é«˜ç´šç™¾è²¨åº—ã®ã‚ˆã†ãªä½“é¨“ã‚’å®Ÿç¾ã—ã¾ã™ã€‚
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒãƒ£ãƒƒãƒˆã§å•†å“ç›¸è«‡
- ARè©¦ç€ã¨ã‚³ãƒ¼ãƒ‡ã‚£ãƒãƒ¼ãƒˆææ¡ˆ
- å‹äººã¸ã®ã‚®ãƒ•ãƒˆé¸ã³ã‚‚AIãŒã‚µãƒãƒ¼ãƒˆ

### 2. ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ã‚³ãƒãƒ¼ã‚¹2.0
- ã‚¤ãƒ³ãƒ•ãƒ«ã‚¨ãƒ³ã‚µãƒ¼ã¨ã®ãƒ©ã‚¤ãƒ–ã‚·ãƒ§ãƒƒãƒ”ãƒ³ã‚°
- è³¼å…¥è€…åŒå£«ã®ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£å½¢æˆ
- UGCã‚’æ´»ç”¨ã—ãŸä¿¡é ¼æ€§å‘ä¸Š

### 3. ã‚µã‚¹ãƒ†ãƒŠãƒ–ãƒ«ãƒã‚¤ãƒ³ãƒˆãƒ—ãƒ­ã‚°ãƒ©ãƒ 
- ã‚¨ã‚³ãªé…é€é¸æŠã§ãƒã‚¤ãƒ³ãƒˆä»˜ä¸
- ãƒªã‚µã‚¤ã‚¯ãƒ«ãƒ»ãƒªãƒ¦ãƒ¼ã‚¹ã§ãƒœãƒ¼ãƒŠã‚¹
- åœ°åŸŸè²¢çŒ®æ´»å‹•ã¨ã®é€£æº

æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœï¼š
- é¡§å®¢ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ300%å‘ä¸Š
- æ–°è¦é¡§å®¢ç²å¾—ã‚³ã‚¹ãƒˆ40%å‰Šæ¸›
- ãƒ–ãƒ©ãƒ³ãƒ‰ãƒ­ã‚¤ãƒ¤ãƒªãƒ†ã‚£ã®å¤§å¹…å‘ä¸Š`,

      'gpt-4': `# éœ€è¦äºˆæ¸¬åˆ†æãƒ¬ãƒãƒ¼ãƒˆ

## äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«
ARIMAãƒ¢ãƒ‡ãƒ«ã¨XGBoostã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚’ä½¿ç”¨ã—ã€ä»¥ä¸‹ã®çµæœã‚’å¾—ã¾ã—ãŸï¼š

### äºˆæ¸¬çµæœï¼ˆæ¥æœˆï¼‰
- å£²ä¸Šäºˆæ¸¬: Â¥125,000,000 (95%ä¿¡é ¼åŒºé–“: Â¥118,000,000 - Â¥132,000,000)
- å‰å¹´åŒæœˆæ¯”: +18.5%
- å‰æœˆæ¯”: +12.3%

### ä¸»è¦ãƒ‰ãƒ©ã‚¤ãƒãƒ¼
1. å­£ç¯€æ€§è¦å› ï¼ˆå¯„ä¸åº¦: 35%ï¼‰
2. ãƒ—ãƒ­ãƒ¢ãƒ¼ã‚·ãƒ§ãƒ³åŠ¹æœï¼ˆå¯„ä¸åº¦: 28%ï¼‰
3. å¸‚å ´ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆå¯„ä¸åº¦: 22%ï¼‰
4. ç«¶åˆå‹•å‘ï¼ˆå¯„ä¸åº¦: 15%ï¼‰

### ãƒ¢ãƒ‡ãƒ«ç²¾åº¦
- MAPE: 7.8%
- RÂ²: 0.92
- éå»6ãƒ¶æœˆã®äºˆæ¸¬ç²¾åº¦: 92.3%

### ãƒªã‚¹ã‚¯è¦å› 
- ä¾›çµ¦åˆ¶ç´„ã«ã‚ˆã‚‹åœ¨åº«ä¸è¶³ãƒªã‚¹ã‚¯ï¼ˆç¢ºç‡: 15%ï¼‰
- ç«¶åˆã®å¤§å‹ã‚­ãƒ£ãƒ³ãƒšãƒ¼ãƒ³ï¼ˆå½±éŸ¿åº¦: -5%ï½-8%ï¼‰
- ç‚ºæ›¿å¤‰å‹•ï¼ˆå½±éŸ¿åº¦: Â±3%ï¼‰`
    };

    return mockResponses[provider] || mockResponses.claude;
  }

  // ãƒãƒƒãƒå‡¦ç†ç”¨ãƒ¡ã‚½ãƒƒãƒ‰
  async batchQuery(
    queries: Array<{
      id: string;
      question: string;
      context: any;
      provider: LLMProvider;
    }>
  ): Promise<Map<string, string>> {
    const results = new Map<string, string>();

    // ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    const groupedQueries = new Map<LLMProvider, typeof queries>();
    
    queries.forEach(query => {
      const group = groupedQueries.get(query.provider) || [];
      group.push(query);
      groupedQueries.set(query.provider, group);
    });

    // ä¸¦åˆ—å‡¦ç†
    const promises: Promise<void>[] = [];
    
    groupedQueries.forEach((providerQueries, provider) => {
      // å„ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’è€ƒæ…®ã—ãŸå‡¦ç†
      const promise = this.processProviderBatch(providerQueries, provider, results);
      promises.push(promise);
    });

    await Promise.all(promises);
    
    return results;
  }

  private async processProviderBatch(
    queries: Array<{
      id: string;
      question: string;
      context: any;
      provider: LLMProvider;
    }>,
    provider: LLMProvider,
    results: Map<string, string>
  ): Promise<void> {
    // ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚’è€ƒæ…®ã—ãŸãƒãƒƒãƒå‡¦ç†
    const batchSize = this.getBatchSize(provider);
    
    for (let i = 0; i < queries.length; i += batchSize) {
      const batch = queries.slice(i, i + batchSize);
      
      await Promise.all(
        batch.map(async (query) => {
          try {
            const response = await this.query(query.question, query.context, provider);
            results.set(query.id, response);
          } catch (error) {
            console.error(`Error processing query ${query.id}:`, error);
            results.set(query.id, `Error: ${error.message}`);
          }
        })
      );
      
      // ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ã®å¾…æ©Ÿ
      if (i + batchSize < queries.length) {
        await this.delay(this.getDelayMs(provider));
      }
    }
  }

  private getBatchSize(provider: LLMProvider): number {
    const batchSizes = {
      claude: 5,
      'gpt-4': 10,
      gemini: 8
    };
    return batchSizes[provider];
  }

  private getDelayMs(provider: LLMProvider): number {
    const delays = {
      claude: 1000,  // 1ç§’
      'gpt-4': 500,  // 0.5ç§’
      gemini: 750    // 0.75ç§’
    };
    return delays[provider];
  }

  private delay(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }
}